{"metadata":{"colab":{"provenance":[{"file_id":"1P-MoDr_-Zxqmm8pb8I30yrM6V2So9VEI","timestamp":1694085888687}],"gpuType":"T4","mount_file_id":"1hY8zVReNuz7uR-XIpVPOYa3qoNljUzMz","authorship_tag":"ABX9TyOVJ5cCikXhMhMYtse1WkCU"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Task 2: \n1. Retrain the model (on the same training and validation dataset) with ‘dice_loss` as the loss function. \n2. The metrics should include at least `dice_score` and `IoU`.\n3. Keep model’s input_size as 320x320.\n","metadata":{}},{"cell_type":"markdown","source":"### To Retrain require the path of the saved model. (.pth , or any checkpoint saved )\n### 1. You can either train over your own model\n### 2. Train over the original weights of model\n<br>\ndownload the model weight from the github its present <br>\nhttps://github.com/xuebinqin/U-2-Net","metadata":{}},{"cell_type":"code","source":"pwd\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"TxOODrrTKZLx","executionInfo":{"status":"ok","timestamp":1694153786211,"user_tz":-330,"elapsed":22,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"outputId":"e4090f3a-04e0-4199-f0e5-77abf829a979","execution":{"iopub.status.busy":"2023-09-08T08:59:38.437608Z","iopub.execute_input":"2023-09-08T08:59:38.437981Z","iopub.status.idle":"2023-09-08T08:59:38.445217Z","shell.execute_reply.started":"2023-09-08T08:59:38.437951Z","shell.execute_reply":"2023-09-08T08:59:38.444102Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"!git clone https://github.com/xuebinqin/U-2-Net.git\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7ISbPkZJ_qe","executionInfo":{"status":"ok","timestamp":1694153788709,"user_tz":-330,"elapsed":2516,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"outputId":"1a03c622-04a6-440a-ffcb-ad41f4edad5a","execution":{"iopub.status.busy":"2023-09-08T08:59:38.904448Z","iopub.execute_input":"2023-09-08T08:59:38.904904Z","iopub.status.idle":"2023-09-08T08:59:39.938280Z","shell.execute_reply.started":"2023-09-08T08:59:38.904870Z","shell.execute_reply":"2023-09-08T08:59:39.936859Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"fatal: destination path 'U-2-Net' already exists and is not an empty directory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd U-2-Net\n%pwd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEn8vCWLKdHd","executionInfo":{"status":"ok","timestamp":1694153788710,"user_tz":-330,"elapsed":11,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"outputId":"9a70dc0b-2fda-4da3-a9f5-bad8a1ea0c64","execution":{"iopub.status.busy":"2023-09-08T09:00:05.299901Z","iopub.execute_input":"2023-09-08T09:00:05.300309Z","iopub.status.idle":"2023-09-08T09:00:05.310545Z","shell.execute_reply.started":"2023-09-08T09:00:05.300278Z","shell.execute_reply":"2023-09-08T09:00:05.309423Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"/kaggle/working/U-2-Net\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/U-2-Net'"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport torch.optim as optim\nimport torchvision.transforms as standard_transforms\n\nimport numpy as np\nimport glob\nimport os\nimport torch.nn.functional as F\n\nfrom data_loader import Rescale\nfrom data_loader import RescaleT\nfrom data_loader import RandomCrop\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET\nfrom model import U2NETP\nfrom sklearn.model_selection import train_test_split","metadata":{"id":"sZeoCG9PKR7c","executionInfo":{"status":"ok","timestamp":1694153996100,"user_tz":-330,"elapsed":2,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:00:06.676784Z","iopub.execute_input":"2023-09-08T09:00:06.677199Z","iopub.status.idle":"2023-09-08T09:00:07.888621Z","shell.execute_reply.started":"2023-09-08T09:00:06.677162Z","shell.execute_reply":"2023-09-08T09:00:07.887274Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Change accordingly ","metadata":{}},{"cell_type":"code","source":"# mount drive with dataset or upload folder\n# change accordingly model path and the image path\nimage_dir = \"/kaggle/input/carmasks/dataset-20230907T065449Z-001/dataset/Image/\"\nlabel_dir = \"/kaggle/input/carmasks/dataset-20230907T065449Z-001/dataset/Mask/\"\nmodel_path = \"/kaggle/input/carmasks/u2net.pth\"","metadata":{"id":"TSS_a9TcKB6A","executionInfo":{"status":"ok","timestamp":1694153996718,"user_tz":-330,"elapsed":2,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:02:02.426510Z","iopub.execute_input":"2023-09-08T09:02:02.427112Z","iopub.status.idle":"2023-09-08T09:02:02.435449Z","shell.execute_reply.started":"2023-09-08T09:02:02.427059Z","shell.execute_reply":"2023-09-08T09:02:02.433886Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#config class\nclass cfg:\n  epoch_num = 50\n  batch_size_train = 4\n  batch_size_val = 1\n  train_num = 0\n  val_num = 0\n  image_ext='.jpg'\n  label_ext ='.png'\n  model_name = 'u2net'\n\ncfg.image_dir = image_dir\ncfg.label_dir = label_dir\ncfg.model_dir  = os.getcwd()\n\n","metadata":{"id":"HMpHxi0kKQql","executionInfo":{"status":"ok","timestamp":1694154062927,"user_tz":-330,"elapsed":2,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:19.903812Z","iopub.execute_input":"2023-09-08T09:09:19.904222Z","iopub.status.idle":"2023-09-08T09:09:19.910637Z","shell.execute_reply.started":"2023-09-08T09:09:19.904189Z","shell.execute_reply":"2023-09-08T09:09:19.909657Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"tra_img_name_list = glob.glob(cfg.image_dir + '*' + cfg.image_ext)\ntra_lbl_name_list = []\nfor img_path in tra_img_name_list:\n\timg_name = img_path.split(os.sep)[-1]\n\n\taaa = img_name.split(\".\")\n\tbbb = aaa[0:-1]\n\timidx = bbb[0]\n\tfor i in range(1,len(bbb)):\n\t\timidx = imidx + \".\" + bbb[i]\n\n\ttra_lbl_name_list.append(cfg.label_dir + imidx + cfg.label_ext)","metadata":{"id":"q2tNl1fWMBCY","executionInfo":{"status":"ok","timestamp":1694153997433,"user_tz":-330,"elapsed":2,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:20.339569Z","iopub.execute_input":"2023-09-08T09:09:20.340693Z","iopub.status.idle":"2023-09-08T09:09:20.352449Z","shell.execute_reply.started":"2023-09-08T09:09:20.340657Z","shell.execute_reply":"2023-09-08T09:09:20.351261Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"tra_img_train_list, tra_img_val_list, tra_lbl_train_list, tra_lbl_val_list = train_test_split(\n    tra_img_name_list, tra_lbl_name_list, test_size=5, random_state=42)","metadata":{"id":"tbOMYdd2LAZa","executionInfo":{"status":"ok","timestamp":1694153998112,"user_tz":-330,"elapsed":2,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:20.814394Z","iopub.execute_input":"2023-09-08T09:09:20.815632Z","iopub.status.idle":"2023-09-08T09:09:20.823547Z","shell.execute_reply.started":"2023-09-08T09:09:20.815586Z","shell.execute_reply":"2023-09-08T09:09:20.822559Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"salobj_train_dataset = SalObjDataset(\n    img_name_list=tra_img_train_list,\n    lbl_name_list=tra_lbl_train_list,\n    transform=transforms.Compose([\n        RescaleT(320),\n        RandomCrop(288),\n        ToTensorLab(flag=0)]))\n\nsalobj_val_dataset = SalObjDataset(\n    img_name_list=tra_img_val_list,\n    lbl_name_list=tra_lbl_val_list,\n    transform=transforms.Compose([\n        RescaleT(320),\n        RandomCrop(288),\n        ToTensorLab(flag=0)]))\n\nsalobj_train_dataloader = DataLoader(salobj_train_dataset, batch_size=cfg.batch_size_train, shuffle=True, num_workers=1)\nsalobj_val_dataloader = DataLoader(salobj_val_dataset, batch_size=cfg.batch_size_val, shuffle=False, num_workers=1)","metadata":{"id":"S3uFvTU4LwaL","executionInfo":{"status":"ok","timestamp":1694153998761,"user_tz":-330,"elapsed":1,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:21.165565Z","iopub.execute_input":"2023-09-08T09:09:21.166006Z","iopub.status.idle":"2023-09-08T09:09:21.174325Z","shell.execute_reply.started":"2023-09-08T09:09:21.165971Z","shell.execute_reply":"2023-09-08T09:09:21.173281Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Loss functions ","metadata":{}},{"cell_type":"code","source":"bce_loss = nn.BCELoss(size_average=True)\n\ndef muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n\n\tloss0 = bce_loss(d0,labels_v)\n\tloss1 = bce_loss(d1,labels_v)\n\tloss2 = bce_loss(d2,labels_v)\n\tloss3 = bce_loss(d3,labels_v)\n\tloss4 = bce_loss(d4,labels_v)\n\tloss5 = bce_loss(d5,labels_v)\n\tloss6 = bce_loss(d6,labels_v)\n\n\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n\tprint(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data.item(),loss1.data.item(),loss2.data.item(),loss3.data.item(),loss4.data.item(),loss5.data.item(),loss6.data.item()))\n\n\treturn loss0, loss","metadata":{"execution":{"iopub.status.busy":"2023-09-08T09:09:21.696712Z","iopub.execute_input":"2023-09-08T09:09:21.697110Z","iopub.status.idle":"2023-09-08T09:09:21.708546Z","shell.execute_reply.started":"2023-09-08T09:09:21.697082Z","shell.execute_reply":"2023-09-08T09:09:21.705524Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class DiceLoss(torch.nn.Module):\n    def __init__(self, smooth=1.0):\n        super(DiceLoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, prediction, target):\n        intersection = torch.sum(prediction * target)\n        union = torch.sum(prediction) + torch.sum(target)\n        dice_coeff = (2.0 * intersection + self.smooth) / (union + self.smooth)\n        loss = 1.0 - dice_coeff\n        return loss","metadata":{"id":"ZpINSiMrMcx9","executionInfo":{"status":"ok","timestamp":1694154000467,"user_tz":-330,"elapsed":955,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:21.988249Z","iopub.execute_input":"2023-09-08T09:09:21.988710Z","iopub.status.idle":"2023-09-08T09:09:21.996702Z","shell.execute_reply.started":"2023-09-08T09:09:21.988656Z","shell.execute_reply":"2023-09-08T09:09:21.995537Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def muti_Dice_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n\n\tloss0 = DiceLoss(smooth=1e-5)(d0,labels_v)\n\tloss1 = DiceLoss(smooth=1e-5)(d1,labels_v)\n\tloss2 = DiceLoss(smooth=1e-5)(d2,labels_v)\n\tloss3 = DiceLoss(smooth=1e-5)(d3,labels_v)\n\tloss4 = DiceLoss(smooth=1e-5)(d4,labels_v)\n\tloss5 = DiceLoss(smooth=1e-5)(d5,labels_v)\n\tloss6 = DiceLoss(smooth=1e-5)(d6,labels_v)\n\n\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n\tprint(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data.item(),loss1.data.item(),loss2.data.item(),loss3.data.item(),loss4.data.item(),loss5.data.item(),loss6.data.item()))\n\n\treturn loss0, loss","metadata":{"id":"IuVyXjGyP9Mm","executionInfo":{"status":"ok","timestamp":1694154087471,"user_tz":-330,"elapsed":2,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:22.320606Z","iopub.execute_input":"2023-09-08T09:09:22.320980Z","iopub.status.idle":"2023-09-08T09:09:22.330899Z","shell.execute_reply.started":"2023-09-08T09:09:22.320951Z","shell.execute_reply":"2023-09-08T09:09:22.329715Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#IOU loss for the item .....\nclass IoULoss(torch.nn.Module):\n    def __init__(self, smooth=1.0):\n        super(IoULoss, self).__init__()\n        self.smooth = smooth\n\n    def forward(self, prediction, target):\n        intersection = torch.sum(prediction * target)\n        union = torch.sum(prediction) + torch.sum(target) - intersection\n        iou = (intersection + self.smooth) / (union + self.smooth)\n        loss = 1.0 - iou\n        return loss\n\ndef multi_IoU_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n    loss0 = IoULoss(smooth=1e-5)(d0, labels_v)\n    loss1 = IoULoss(smooth=1e-5)(d1, labels_v)\n    loss2 = IoULoss(smooth=1e-5)(d2, labels_v)\n    loss3 = IoULoss(smooth=1e-5)(d3, labels_v)\n    loss4 = IoULoss(smooth=1e-5)(d4, labels_v)\n    loss5 = IoULoss(smooth=1e-5)(d5, labels_v)\n    loss6 = IoULoss(smooth=1e-5)(d6, labels_v)\n\n    loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n    print(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\" % (\n        loss0.data.item(), loss1.data.item(), loss2.data.item(), loss3.data.item(), loss4.data.item(), loss5.data.item(),\n        loss6.data.item()))\n","metadata":{"id":"7kakwuPMcqk3","executionInfo":{"status":"ok","timestamp":1694154094780,"user_tz":-330,"elapsed":1,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:22.722850Z","iopub.execute_input":"2023-09-08T09:09:22.723233Z","iopub.status.idle":"2023-09-08T09:09:22.736788Z","shell.execute_reply.started":"2023-09-08T09:09:22.723196Z","shell.execute_reply":"2023-09-08T09:09:22.735783Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Retrain with the best model with least validation losss.....","metadata":{"id":"5Kbl3zz6aAce"}},{"cell_type":"code","source":"if(cfg.model_name=='u2net'):\n    net = U2NET(3, 1)\nelif(cfg.model_name=='u2netp'):\n    net = U2NETP(3,1)\nif torch.cuda.is_available():\n    net.cuda()\n","metadata":{"id":"GfoCEH9EPfPs","executionInfo":{"status":"ok","timestamp":1694154004312,"user_tz":-330,"elapsed":708,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:23.774587Z","iopub.execute_input":"2023-09-08T09:09:23.774984Z","iopub.status.idle":"2023-09-08T09:09:24.303970Z","shell.execute_reply.started":"2023-09-08T09:09:23.774953Z","shell.execute_reply":"2023-09-08T09:09:24.302757Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"1. You can either train over your own model\n2. Train over the original weights of model","metadata":{"id":"uVGcXJT4cDLK"}},{"cell_type":"code","source":"## 1. Own Model ..\n# checkpoint_path = '/content/drive/MyDrive/BestModel/U-2-Netu2net_best.pth'\n\n## 2. Original weights\n# checkpoint_path = '/content/drive/MyDrive/BestModel/u2net.pth'\nnet.load_state_dict(torch.load(model_path))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"rs8FuGkQb_-C","executionInfo":{"status":"error","timestamp":1694154101349,"user_tz":-330,"elapsed":434,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"outputId":"ac5d5f55-9807-4aac-8558-906381309600","execution":{"iopub.status.busy":"2023-09-08T09:09:24.571873Z","iopub.execute_input":"2023-09-08T09:09:24.572728Z","iopub.status.idle":"2023-09-08T09:09:24.869302Z","shell.execute_reply.started":"2023-09-08T09:09:24.572675Z","shell.execute_reply":"2023-09-08T09:09:24.868163Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{"id":"tk0taIKPb9qz"}},{"cell_type":"code","source":"print(\"---define optimizer...\")\noptimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1C0qxIeRPpvj","executionInfo":{"status":"ok","timestamp":1694086134393,"user_tz":-330,"elapsed":6,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"outputId":"52f726af-4fe2-4544-aae9-9865de3e45ed","execution":{"iopub.status.busy":"2023-09-08T09:09:25.308969Z","iopub.execute_input":"2023-09-08T09:09:25.310341Z","iopub.status.idle":"2023-09-08T09:09:25.331536Z","shell.execute_reply.started":"2023-09-08T09:09:25.310274Z","shell.execute_reply":"2023-09-08T09:09:25.330693Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"---define optimizer...\n","output_type":"stream"}]},{"cell_type":"code","source":"# ------- 5. training process --------\nite_num = 0\nrunning_loss = 0.0\nrunning_tar_loss = 0.0\nite_num4val = 0\nsave_frq = 10\nbest_val_loss=999999 #largeno\nprint(\"---start training...\")\nfor epoch in range(0, cfg.epoch_num):\n    net.train()\n\n    for i, data in enumerate(salobj_train_dataloader):\n        ite_num = ite_num + 1\n        ite_num4val = ite_num4val + 1\n\n        inputs, labels = data['image'], data['label']\n\n        inputs = inputs.type(torch.FloatTensor)\n        labels = labels.type(torch.FloatTensor)\n\n        # wrap them in Variable\n        if torch.cuda.is_available():\n            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n                                                                                        requires_grad=False)\n        else:\n            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n\n        # y zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)\n        loss2, loss = muti_Dice_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n\n        loss.backward()\n        optimizer.step()\n\n        # # print statistics\n        running_loss += loss.data.item()\n        running_tar_loss += loss2.data.item()\n\n        # del temporary outputs and loss\n        del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n\n        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n        epoch + 1, cfg.epoch_num, (i + 1) * cfg.batch_size_train, cfg.train_num, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n\n        if ite_num % save_frq == 0:\n\n            torch.save(net.state_dict(), cfg.model_dir + cfg.model_name+\"_bce_itr_%d_train_%3f_tar_%3fRetrain.pth\" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n            running_loss = 0.0\n            running_tar_loss = 0.0\n            net.train()  # resume train\n            ite_num4val = 0\n\n        ## validation\n        # Validation loop\n        net.eval()  # Set the model to evaluation mode\n        val_loss = 0.0\n\n        with torch.no_grad():\n            for i, data in enumerate(salobj_val_dataloader):\n                inputs, labels = data['image'], data['label']\n\n                inputs = inputs.type(torch.FloatTensor)\n                labels = labels.type(torch.FloatTensor)\n\n                if torch.cuda.is_available():\n                    inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(), requires_grad=False)\n                else:\n                    inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n\n                # Forward pass\n                d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)\n                loss2, loss = muti_Dice_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n\n                val_loss += loss.data.item()\n\n                # Clean up\n                del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n\n        average_val_loss = val_loss / len(salobj_val_dataloader)\n        print(\"[epoch: %3d/%3d] Validation Loss: %3f\" % (epoch + 1, cfg.epoch_num, average_val_loss))\n\n        # Save the model checkpoint if validation loss is lower than previous best\n        if average_val_loss < best_val_loss:\n            best_val_loss = average_val_loss\n            torch.save(net.state_dict(), cfg.model_dir + cfg.model_name + \"Retrain_best.pth\")\n\n        net.train()  # S\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LZbKnlGKOyci","outputId":"887a59bb-df7f-47e7-ad64-5a822ae07651","executionInfo":{"status":"ok","timestamp":1694086328781,"user_tz":-330,"elapsed":162144,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"execution":{"iopub.status.busy":"2023-09-08T09:09:25.808127Z","iopub.execute_input":"2023-09-08T09:09:25.808566Z","iopub.status.idle":"2023-09-08T09:15:07.415554Z","shell.execute_reply.started":"2023-09-08T09:09:25.808525Z","shell.execute_reply":"2023-09-08T09:15:07.414252Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"---start training...\nl0: 0.031414, l1: 0.031429, l2: 0.032272, l3: 0.032020, l4: 0.035848, l5: 0.037846, l6: 0.067728\n\n[epoch:   1/ 50, batch:     4/    0, ite: 1] train loss: 0.268557, tar: 0.031414 \nl0: 0.068745, l1: 0.069223, l2: 0.070167, l3: 0.072453, l4: 0.056569, l5: 0.072215, l6: 0.109578\n\nl0: 0.007252, l1: 0.007254, l2: 0.007659, l3: 0.008265, l4: 0.010099, l5: 0.013139, l6: 0.021433\n\nl0: 0.044859, l1: 0.044799, l2: 0.046144, l3: 0.046454, l4: 0.047927, l5: 0.050784, l6: 0.061271\n\nl0: 0.011065, l1: 0.011036, l2: 0.011905, l3: 0.012704, l4: 0.016321, l5: 0.021500, l6: 0.031684\n\nl0: 0.168640, l1: 0.168754, l2: 0.166885, l3: 0.170490, l4: 0.155342, l5: 0.142701, l6: 0.116723\n\n[epoch:   1/ 50] Validation Loss: 0.428407\nl0: 0.014854, l1: 0.014840, l2: 0.015413, l3: 0.016729, l4: 0.019884, l5: 0.027462, l6: 0.048061\n\n[epoch:   1/ 50, batch:     8/    0, ite: 2] train loss: 0.212901, tar: 0.023134 \nl0: 0.088174, l1: 0.088138, l2: 0.088885, l3: 0.089835, l4: 0.087048, l5: 0.086689, l6: 0.119313\n\nl0: 0.007685, l1: 0.007692, l2: 0.008341, l3: 0.008983, l4: 0.012811, l5: 0.015490, l6: 0.021317\n\nl0: 0.044577, l1: 0.044533, l2: 0.045819, l3: 0.046392, l4: 0.046206, l5: 0.050987, l6: 0.059637\n\nl0: 0.009711, l1: 0.009700, l2: 0.010691, l3: 0.011523, l4: 0.014222, l5: 0.017261, l6: 0.028841\n\nl0: 0.139081, l1: 0.139222, l2: 0.138549, l3: 0.138857, l4: 0.133348, l5: 0.126355, l6: 0.129375\n\n[epoch:   1/ 50] Validation Loss: 0.423057\nl0: 0.012415, l1: 0.012431, l2: 0.013070, l3: 0.014307, l4: 0.017366, l5: 0.022861, l6: 0.038736\n\n[epoch:   1/ 50, batch:    12/    0, ite: 3] train loss: 0.185663, tar: 0.019561 \nl0: 0.063597, l1: 0.063632, l2: 0.064064, l3: 0.064028, l4: 0.067481, l5: 0.080755, l6: 0.124665\n\nl0: 0.008090, l1: 0.008131, l2: 0.008959, l3: 0.009574, l4: 0.011105, l5: 0.015752, l6: 0.027657\n\nl0: 0.045864, l1: 0.045757, l2: 0.047774, l3: 0.046928, l4: 0.049531, l5: 0.050693, l6: 0.067802\n\nl0: 0.010640, l1: 0.010593, l2: 0.011794, l3: 0.012398, l4: 0.015848, l5: 0.021775, l6: 0.033205\n\nl0: 0.115573, l1: 0.115682, l2: 0.114834, l3: 0.114974, l4: 0.112985, l5: 0.116064, l6: 0.109363\n\n[epoch:   1/ 50] Validation Loss: 0.377514\nl0: 0.031441, l1: 0.032011, l2: 0.031045, l3: 0.030995, l4: 0.030445, l5: 0.035298, l6: 0.044904\n\n[epoch:   1/ 50, batch:    16/    0, ite: 4] train loss: 0.198282, tar: 0.022531 \nl0: 0.078383, l1: 0.078490, l2: 0.079010, l3: 0.079501, l4: 0.077547, l5: 0.081540, l6: 0.119994\n\nl0: 0.007851, l1: 0.007901, l2: 0.008741, l3: 0.009038, l4: 0.011006, l5: 0.015447, l6: 0.027848\n\nl0: 0.045373, l1: 0.045355, l2: 0.046751, l3: 0.046442, l4: 0.048892, l5: 0.051525, l6: 0.056658\n\nl0: 0.009986, l1: 0.009960, l2: 0.011390, l3: 0.011895, l4: 0.014497, l5: 0.019364, l6: 0.033737\n\nl0: 0.035588, l1: 0.035110, l2: 0.036915, l3: 0.040699, l4: 0.064880, l5: 0.110177, l6: 0.186701\n\n[epoch:   1/ 50] Validation Loss: 0.328838\nl0: 0.025492, l1: 0.025538, l2: 0.026527, l3: 0.027916, l4: 0.030445, l5: 0.035279, l6: 0.049488\n\n[epoch:   2/ 50, batch:     4/    0, ite: 5] train loss: 0.202762, tar: 0.023123 \nl0: 0.053777, l1: 0.053836, l2: 0.054307, l3: 0.055317, l4: 0.058437, l5: 0.078280, l6: 0.101955\n\nl0: 0.006671, l1: 0.006744, l2: 0.007266, l3: 0.007708, l4: 0.009696, l5: 0.014979, l6: 0.022850\n\nl0: 0.043704, l1: 0.043681, l2: 0.045083, l3: 0.045532, l4: 0.045366, l5: 0.050026, l6: 0.058179\n\nl0: 0.009571, l1: 0.009574, l2: 0.011063, l3: 0.011775, l4: 0.014851, l5: 0.021590, l6: 0.033789\n\nl0: 0.116590, l1: 0.116526, l2: 0.115570, l3: 0.117613, l4: 0.126749, l5: 0.129481, l6: 0.128725\n\n[epoch:   2/ 50] Validation Loss: 0.365372\nl0: 0.014555, l1: 0.014587, l2: 0.015397, l3: 0.016683, l4: 0.020635, l5: 0.028854, l6: 0.045905\n\n[epoch:   2/ 50, batch:     8/    0, ite: 6] train loss: 0.195071, tar: 0.021695 \nl0: 0.088863, l1: 0.088724, l2: 0.089180, l3: 0.090773, l4: 0.088898, l5: 0.084570, l6: 0.126626\n\nl0: 0.006551, l1: 0.006610, l2: 0.007316, l3: 0.007695, l4: 0.010279, l5: 0.013339, l6: 0.019977\n\nl0: 0.040271, l1: 0.040309, l2: 0.040932, l3: 0.041401, l4: 0.041355, l5: 0.045677, l6: 0.054539\n\nl0: 0.010312, l1: 0.010335, l2: 0.011662, l3: 0.012512, l4: 0.014919, l5: 0.020132, l6: 0.031954\n\nl0: 0.046650, l1: 0.046641, l2: 0.047686, l3: 0.048696, l4: 0.057513, l5: 0.079248, l6: 0.095201\n\n[epoch:   2/ 50] Validation Loss: 0.313470\nl0: 0.023614, l1: 0.023669, l2: 0.024435, l3: 0.025360, l4: 0.028182, l5: 0.033149, l6: 0.050941\n\n[epoch:   2/ 50, batch:    12/    0, ite: 7] train loss: 0.197111, tar: 0.021969 \nl0: 0.088383, l1: 0.088601, l2: 0.089647, l3: 0.092465, l4: 0.076596, l5: 0.078900, l6: 0.116433\n\nl0: 0.007138, l1: 0.007211, l2: 0.007863, l3: 0.008397, l4: 0.011027, l5: 0.014710, l6: 0.023270\n\nl0: 0.043731, l1: 0.043756, l2: 0.045104, l3: 0.044437, l4: 0.047304, l5: 0.049129, l6: 0.055788\n\nl0: 0.010425, l1: 0.010494, l2: 0.011770, l3: 0.012068, l4: 0.015920, l5: 0.022241, l6: 0.034125\n\nl0: 0.066036, l1: 0.066602, l2: 0.064624, l3: 0.066248, l4: 0.079152, l5: 0.089469, l6: 0.096560\n\n[epoch:   2/ 50] Validation Loss: 0.337125\nl0: 0.015068, l1: 0.015124, l2: 0.015616, l3: 0.016738, l4: 0.019639, l5: 0.023430, l6: 0.033325\n\n[epoch:   2/ 50, batch:    16/    0, ite: 8] train loss: 0.189840, tar: 0.021107 \nl0: 0.058164, l1: 0.058102, l2: 0.058073, l3: 0.056518, l4: 0.057207, l5: 0.070765, l6: 0.099793\n\nl0: 0.006323, l1: 0.006397, l2: 0.007169, l3: 0.007856, l4: 0.010024, l5: 0.014235, l6: 0.020936\n\nl0: 0.043805, l1: 0.043836, l2: 0.045201, l3: 0.044647, l4: 0.045435, l5: 0.047423, l6: 0.051715\n\nl0: 0.009099, l1: 0.009120, l2: 0.010462, l3: 0.011330, l4: 0.014160, l5: 0.019288, l6: 0.033819\n\nl0: 0.132125, l1: 0.132149, l2: 0.131734, l3: 0.131065, l4: 0.129427, l5: 0.123412, l6: 0.109226\n\n[epoch:   2/ 50] Validation Loss: 0.370008\nl0: 0.025870, l1: 0.026008, l2: 0.026678, l3: 0.027356, l4: 0.028753, l5: 0.032777, l6: 0.044498\n\n[epoch:   3/ 50, batch:     4/    0, ite: 9] train loss: 0.192295, tar: 0.021636 \nl0: 0.063663, l1: 0.063669, l2: 0.064256, l3: 0.064972, l4: 0.069459, l5: 0.069000, l6: 0.090222\n\nl0: 0.006377, l1: 0.006459, l2: 0.007244, l3: 0.008124, l4: 0.010176, l5: 0.015847, l6: 0.027555\n\nl0: 0.043674, l1: 0.043690, l2: 0.044908, l3: 0.044993, l4: 0.044985, l5: 0.048459, l6: 0.053798\n\nl0: 0.009210, l1: 0.009220, l2: 0.010792, l3: 0.011258, l4: 0.013824, l5: 0.018077, l6: 0.032116\n\nl0: 0.113393, l1: 0.113686, l2: 0.113320, l3: 0.114487, l4: 0.110149, l5: 0.100691, l6: 0.103050\n\n[epoch:   3/ 50] Validation Loss: 0.352960\nl0: 0.011556, l1: 0.011708, l2: 0.012242, l3: 0.013459, l4: 0.016660, l5: 0.023352, l6: 0.039315\n\n[epoch:   3/ 50, batch:     8/    0, ite: 10] train loss: 0.185895, tar: 0.020628 \nl0: 0.075419, l1: 0.075318, l2: 0.076622, l3: 0.078356, l4: 0.073119, l5: 0.079058, l6: 0.111327\n\nl0: 0.007076, l1: 0.007131, l2: 0.008266, l3: 0.008861, l4: 0.011275, l5: 0.015371, l6: 0.025513\n\nl0: 0.040356, l1: 0.040336, l2: 0.042145, l3: 0.042419, l4: 0.044541, l5: 0.046094, l6: 0.053320\n\nl0: 0.008806, l1: 0.008871, l2: 0.010497, l3: 0.011568, l4: 0.013117, l5: 0.015860, l6: 0.028547\n\nl0: 0.036173, l1: 0.036351, l2: 0.037483, l3: 0.039519, l4: 0.050073, l5: 0.071180, l6: 0.102793\n\n[epoch:   3/ 50] Validation Loss: 0.286553\nl0: 0.010132, l1: 0.010216, l2: 0.011043, l3: 0.012333, l4: 0.015825, l5: 0.022321, l6: 0.039862\n\n[epoch:   3/ 50, batch:    12/    0, ite: 11] train loss: 0.121731, tar: 0.010132 \nl0: 0.064467, l1: 0.064475, l2: 0.065068, l3: 0.065611, l4: 0.071536, l5: 0.068372, l6: 0.091416\n\nl0: 0.006214, l1: 0.006279, l2: 0.007211, l3: 0.008075, l4: 0.010251, l5: 0.015085, l6: 0.022786\n\nl0: 0.045896, l1: 0.046001, l2: 0.047226, l3: 0.047153, l4: 0.047147, l5: 0.046963, l6: 0.053978\n\nl0: 0.009306, l1: 0.009334, l2: 0.011091, l3: 0.011818, l4: 0.014129, l5: 0.017504, l6: 0.033329\n\nl0: 0.100076, l1: 0.099567, l2: 0.101818, l3: 0.101695, l4: 0.104257, l5: 0.112795, l6: 0.107737\n\n[epoch:   3/ 50] Validation Loss: 0.347133\nl0: 0.011676, l1: 0.011769, l2: 0.012303, l3: 0.013550, l4: 0.016343, l5: 0.022563, l6: 0.036569\n\n[epoch:   3/ 50, batch:    16/    0, ite: 12] train loss: 0.123252, tar: 0.010904 \nl0: 0.061688, l1: 0.061831, l2: 0.061998, l3: 0.062311, l4: 0.063279, l5: 0.066830, l6: 0.089771\n\nl0: 0.006962, l1: 0.007043, l2: 0.007934, l3: 0.008406, l4: 0.011273, l5: 0.015393, l6: 0.023594\n\nl0: 0.034931, l1: 0.034927, l2: 0.035590, l3: 0.037115, l4: 0.039854, l5: 0.041721, l6: 0.056242\n\nl0: 0.009392, l1: 0.009374, l2: 0.011009, l3: 0.011514, l4: 0.013946, l5: 0.020632, l6: 0.033993\n\nl0: 0.086626, l1: 0.085859, l2: 0.085715, l3: 0.091848, l4: 0.101876, l5: 0.113267, l6: 0.130322\n\n[epoch:   3/ 50] Validation Loss: 0.326813\nl0: 0.018556, l1: 0.018585, l2: 0.019394, l3: 0.020455, l4: 0.023050, l5: 0.032378, l6: 0.049399\n\n[epoch:   4/ 50, batch:     4/    0, ite: 13] train loss: 0.142774, tar: 0.013454 \nl0: 0.063407, l1: 0.063519, l2: 0.064998, l3: 0.065282, l4: 0.066343, l5: 0.074580, l6: 0.103010\n\nl0: 0.005943, l1: 0.006058, l2: 0.006818, l3: 0.007379, l4: 0.009798, l5: 0.013814, l6: 0.021195\n\nl0: 0.036468, l1: 0.036451, l2: 0.037473, l3: 0.038933, l4: 0.039853, l5: 0.043678, l6: 0.050544\n\nl0: 0.009145, l1: 0.009209, l2: 0.010932, l3: 0.011810, l4: 0.013900, l5: 0.017234, l6: 0.030071\n\nl0: 0.043137, l1: 0.042776, l2: 0.045000, l3: 0.048437, l4: 0.059733, l5: 0.077679, l6: 0.096513\n\n[epoch:   4/ 50] Validation Loss: 0.274224\nl0: 0.013013, l1: 0.013129, l2: 0.014084, l3: 0.015940, l4: 0.020877, l5: 0.027034, l6: 0.042138\n\n[epoch:   4/ 50, batch:     8/    0, ite: 14] train loss: 0.143634, tar: 0.013344 \nl0: 0.083432, l1: 0.083389, l2: 0.084404, l3: 0.085593, l4: 0.084067, l5: 0.085382, l6: 0.117646\n\nl0: 0.006648, l1: 0.006730, l2: 0.007699, l3: 0.008225, l4: 0.011009, l5: 0.014182, l6: 0.020794\n\nl0: 0.037942, l1: 0.037809, l2: 0.039229, l3: 0.041460, l4: 0.044314, l5: 0.044303, l6: 0.051851\n\nl0: 0.009015, l1: 0.009068, l2: 0.010911, l3: 0.011778, l4: 0.013953, l5: 0.019369, l6: 0.030549\n\nl0: 0.093762, l1: 0.093599, l2: 0.094532, l3: 0.096452, l4: 0.111369, l5: 0.118833, l6: 0.137342\n\n[epoch:   4/ 50] Validation Loss: 0.369327\nl0: 0.009952, l1: 0.010027, l2: 0.010902, l3: 0.012099, l4: 0.015163, l5: 0.020587, l6: 0.033533\n\n[epoch:   4/ 50, batch:    12/    0, ite: 15] train loss: 0.137360, tar: 0.012666 \nl0: 0.054666, l1: 0.054943, l2: 0.055253, l3: 0.054014, l4: 0.052430, l5: 0.054587, l6: 0.088838\n\nl0: 0.005924, l1: 0.006016, l2: 0.006849, l3: 0.007842, l4: 0.009979, l5: 0.014787, l6: 0.023480\n\nl0: 0.031088, l1: 0.030907, l2: 0.033095, l3: 0.035248, l4: 0.041192, l5: 0.043563, l6: 0.050588\n\nl0: 0.009128, l1: 0.009149, l2: 0.010839, l3: 0.011471, l4: 0.013920, l5: 0.017616, l6: 0.028364\n\nl0: 0.066616, l1: 0.066394, l2: 0.065933, l3: 0.073850, l4: 0.082305, l5: 0.102283, l6: 0.136100\n\n[epoch:   4/ 50] Validation Loss: 0.289851\nl0: 0.022861, l1: 0.022966, l2: 0.023376, l3: 0.024312, l4: 0.027110, l5: 0.032251, l6: 0.041187\n\n[epoch:   4/ 50, batch:    16/    0, ite: 16] train loss: 0.146811, tar: 0.014365 \nl0: 0.068163, l1: 0.068135, l2: 0.068974, l3: 0.069411, l4: 0.073575, l5: 0.072420, l6: 0.083667\n\nl0: 0.006155, l1: 0.006242, l2: 0.007131, l3: 0.007906, l4: 0.010354, l5: 0.014677, l6: 0.023720\n\nl0: 0.037149, l1: 0.036921, l2: 0.038749, l3: 0.041511, l4: 0.044808, l5: 0.046144, l6: 0.050144\n\nl0: 0.008979, l1: 0.008974, l2: 0.010935, l3: 0.011658, l4: 0.013996, l5: 0.018795, l6: 0.031543\n\nl0: 0.091843, l1: 0.091617, l2: 0.092408, l3: 0.094165, l4: 0.108381, l5: 0.123010, l6: 0.143952\n\n[epoch:   4/ 50] Validation Loss: 0.345242\nl0: 0.013512, l1: 0.013649, l2: 0.014330, l3: 0.015378, l4: 0.018330, l5: 0.022518, l6: 0.033096\n\n[epoch:   5/ 50, batch:     4/    0, ite: 17] train loss: 0.144525, tar: 0.014243 \nl0: 0.065375, l1: 0.065446, l2: 0.065523, l3: 0.066099, l4: 0.064521, l5: 0.068404, l6: 0.098494\n\nl0: 0.005787, l1: 0.005908, l2: 0.006815, l3: 0.007547, l4: 0.009684, l5: 0.013365, l6: 0.019101\n\nl0: 0.037480, l1: 0.037451, l2: 0.038406, l3: 0.039743, l4: 0.039353, l5: 0.043347, l6: 0.056714\n\nl0: 0.008990, l1: 0.009007, l2: 0.010492, l3: 0.011108, l4: 0.013651, l5: 0.017531, l6: 0.028948\n\nl0: 0.064475, l1: 0.064926, l2: 0.064848, l3: 0.067020, l4: 0.081393, l5: 0.097471, l6: 0.102545\n\n[epoch:   5/ 50] Validation Loss: 0.299393\nl0: 0.009122, l1: 0.009176, l2: 0.010178, l3: 0.011727, l4: 0.014969, l5: 0.020886, l6: 0.037258\n\n[epoch:   5/ 50, batch:     8/    0, ite: 18] train loss: 0.140624, tar: 0.013603 \nl0: 0.082168, l1: 0.082240, l2: 0.082800, l3: 0.082762, l4: 0.081973, l5: 0.080290, l6: 0.099858\n\nl0: 0.005692, l1: 0.005810, l2: 0.006588, l3: 0.007542, l4: 0.009869, l5: 0.017168, l6: 0.026421\n\nl0: 0.039477, l1: 0.039505, l2: 0.039959, l3: 0.041425, l4: 0.041852, l5: 0.043408, l6: 0.052702\n\nl0: 0.007622, l1: 0.007732, l2: 0.009387, l3: 0.010424, l4: 0.012249, l5: 0.015067, l6: 0.029585\n\nl0: 0.076236, l1: 0.076666, l2: 0.076506, l3: 0.076481, l4: 0.077814, l5: 0.087330, l6: 0.109349\n\n[epoch:   5/ 50] Validation Loss: 0.328392\nl0: 0.011572, l1: 0.011622, l2: 0.012766, l3: 0.013864, l4: 0.018170, l5: 0.025720, l6: 0.037051\n\n[epoch:   5/ 50, batch:    12/    0, ite: 19] train loss: 0.139529, tar: 0.013377 \nl0: 0.062465, l1: 0.062762, l2: 0.065009, l3: 0.065625, l4: 0.067027, l5: 0.075733, l6: 0.141437\n\nl0: 0.006447, l1: 0.006554, l2: 0.007762, l3: 0.008614, l4: 0.010135, l5: 0.015979, l6: 0.028702\n\nl0: 0.027652, l1: 0.027412, l2: 0.029986, l3: 0.032172, l4: 0.035692, l5: 0.041444, l6: 0.048345\n\nl0: 0.008887, l1: 0.008882, l2: 0.010579, l3: 0.011331, l4: 0.013448, l5: 0.018597, l6: 0.032632\n\nl0: 0.093098, l1: 0.093055, l2: 0.093716, l3: 0.095010, l4: 0.104465, l5: 0.114944, l6: 0.136396\n\n[epoch:   5/ 50] Validation Loss: 0.340399\nl0: 0.008190, l1: 0.008288, l2: 0.009366, l3: 0.011162, l4: 0.014309, l5: 0.019996, l6: 0.035944\n\n[epoch:   5/ 50, batch:    16/    0, ite: 20] train loss: 0.136301, tar: 0.012858 \nl0: 0.056176, l1: 0.056503, l2: 0.056461, l3: 0.054832, l4: 0.050583, l5: 0.051668, l6: 0.081250\n\nl0: 0.011095, l1: 0.011190, l2: 0.011915, l3: 0.012761, l4: 0.014813, l5: 0.015549, l6: 0.022657\n\nl0: 0.035903, l1: 0.035834, l2: 0.037108, l3: 0.038944, l4: 0.039931, l5: 0.045089, l6: 0.050923\n\nl0: 0.008218, l1: 0.008264, l2: 0.010116, l3: 0.011130, l4: 0.013747, l5: 0.016436, l6: 0.029344\n\nl0: 0.072702, l1: 0.072026, l2: 0.074150, l3: 0.079869, l4: 0.085398, l5: 0.094383, l6: 0.108187\n\n[epoch:   5/ 50] Validation Loss: 0.295031\nl0: 0.009251, l1: 0.009380, l2: 0.010239, l3: 0.011716, l4: 0.014555, l5: 0.020069, l6: 0.035168\n\n[epoch:   6/ 50, batch:     4/    0, ite: 21] train loss: 0.110379, tar: 0.009251 \nl0: 0.054838, l1: 0.054997, l2: 0.054865, l3: 0.054501, l4: 0.056223, l5: 0.070517, l6: 0.094212\n\nl0: 0.005383, l1: 0.005528, l2: 0.006254, l3: 0.007045, l4: 0.009259, l5: 0.013842, l6: 0.020165\n\nl0: 0.028468, l1: 0.028251, l2: 0.030399, l3: 0.032445, l4: 0.038932, l5: 0.042346, l6: 0.050546\n\nl0: 0.008093, l1: 0.008164, l2: 0.010131, l3: 0.011004, l4: 0.012517, l5: 0.017160, l6: 0.031015\n\nl0: 0.067134, l1: 0.067305, l2: 0.068093, l3: 0.072775, l4: 0.085402, l5: 0.091467, l6: 0.110777\n\n[epoch:   6/ 50] Validation Loss: 0.284010\nl0: 0.008471, l1: 0.008605, l2: 0.009366, l3: 0.010637, l4: 0.013640, l5: 0.019165, l6: 0.033133\n\n[epoch:   6/ 50, batch:     8/    0, ite: 22] train loss: 0.106698, tar: 0.008861 \nl0: 0.316929, l1: 0.334672, l2: 0.319891, l3: 0.324097, l4: 0.178607, l5: 0.271410, l6: 0.248729\n\nl0: 0.005343, l1: 0.005480, l2: 0.006162, l3: 0.007211, l4: 0.009618, l5: 0.015319, l6: 0.023239\n\nl0: 0.029766, l1: 0.029515, l2: 0.032020, l3: 0.034768, l4: 0.041663, l5: 0.043090, l6: 0.045062\n\nl0: 0.008089, l1: 0.008159, l2: 0.009983, l3: 0.010990, l4: 0.012462, l5: 0.016986, l6: 0.030902\n\nl0: 0.026350, l1: 0.026262, l2: 0.026827, l3: 0.028795, l4: 0.038076, l5: 0.042636, l6: 0.086297\n\n[epoch:   6/ 50] Validation Loss: 0.539081\nl0: 0.011379, l1: 0.011464, l2: 0.012424, l3: 0.013628, l4: 0.016664, l5: 0.021457, l6: 0.034786\n\n[epoch:   6/ 50, batch:    12/    0, ite: 23] train loss: 0.111733, tar: 0.009700 \nl0: 0.067889, l1: 0.067842, l2: 0.068310, l3: 0.068809, l4: 0.072297, l5: 0.077105, l6: 0.107989\n\nl0: 0.006001, l1: 0.006082, l2: 0.007121, l3: 0.008099, l4: 0.010635, l5: 0.013690, l6: 0.021130\n\nl0: 0.031333, l1: 0.031279, l2: 0.032482, l3: 0.034576, l4: 0.035806, l5: 0.042132, l6: 0.048144\n\nl0: 0.008327, l1: 0.008372, l2: 0.010509, l3: 0.011546, l4: 0.013186, l5: 0.018121, l6: 0.029183\n\nl0: 0.044873, l1: 0.043977, l2: 0.044306, l3: 0.045078, l4: 0.068744, l5: 0.082357, l6: 0.104425\n\n[epoch:   6/ 50] Validation Loss: 0.278351\nl0: 0.011748, l1: 0.011847, l2: 0.012456, l3: 0.013564, l4: 0.015892, l5: 0.024063, l6: 0.036254\n\n[epoch:   6/ 50, batch:    16/    0, ite: 24] train loss: 0.115256, tar: 0.010212 \nl0: 0.053163, l1: 0.053253, l2: 0.055159, l3: 0.055989, l4: 0.063356, l5: 0.065039, l6: 0.081718\n\nl0: 0.005382, l1: 0.005510, l2: 0.006260, l3: 0.007178, l4: 0.009499, l5: 0.014738, l6: 0.021263\n\nl0: 0.030997, l1: 0.030809, l2: 0.031906, l3: 0.034316, l4: 0.039442, l5: 0.043332, l6: 0.048715\n\nl0: 0.008632, l1: 0.008625, l2: 0.010222, l3: 0.011204, l4: 0.013458, l5: 0.018678, l6: 0.029880\n\nl0: 0.086535, l1: 0.086370, l2: 0.087157, l3: 0.089622, l4: 0.095495, l5: 0.109728, l6: 0.133690\n\n[epoch:   6/ 50] Validation Loss: 0.309264\nl0: 0.008027, l1: 0.008115, l2: 0.009133, l3: 0.010513, l4: 0.013112, l5: 0.019193, l6: 0.033625\n\n[epoch:   7/ 50, batch:     4/    0, ite: 25] train loss: 0.112549, tar: 0.009775 \nl0: 0.061333, l1: 0.061651, l2: 0.061755, l3: 0.061429, l4: 0.060090, l5: 0.062016, l6: 0.080232\n\nl0: 0.005546, l1: 0.005615, l2: 0.006685, l3: 0.007764, l4: 0.010971, l5: 0.013981, l6: 0.022018\n\nl0: 0.033470, l1: 0.033340, l2: 0.034806, l3: 0.037224, l4: 0.040436, l5: 0.044452, l6: 0.047132\n\nl0: 0.007474, l1: 0.007587, l2: 0.009255, l3: 0.010246, l4: 0.012433, l5: 0.014839, l6: 0.028974\n\nl0: 0.039764, l1: 0.039514, l2: 0.042854, l3: 0.047668, l4: 0.056605, l5: 0.079267, l6: 0.091900\n\n[epoch:   7/ 50] Validation Loss: 0.256065\nl0: 0.010244, l1: 0.010321, l2: 0.011230, l3: 0.012504, l4: 0.015425, l5: 0.023565, l6: 0.038735\n\n[epoch:   7/ 50, batch:     8/    0, ite: 26] train loss: 0.114128, tar: 0.009853 \nl0: 0.078792, l1: 0.078571, l2: 0.078665, l3: 0.079141, l4: 0.073957, l5: 0.082508, l6: 0.093083\n\nl0: 0.005198, l1: 0.005291, l2: 0.006166, l3: 0.006925, l4: 0.009593, l5: 0.012642, l6: 0.020324\n\nl0: 0.028534, l1: 0.028176, l2: 0.030731, l3: 0.034118, l4: 0.042079, l5: 0.045803, l6: 0.051556\n\nl0: 0.007543, l1: 0.007612, l2: 0.009625, l3: 0.010909, l4: 0.012709, l5: 0.016322, l6: 0.026977\n\nl0: 0.069314, l1: 0.069298, l2: 0.070540, l3: 0.073961, l4: 0.081245, l5: 0.086441, l6: 0.109732\n\n[epoch:   7/ 50] Validation Loss: 0.308816\nl0: 0.010266, l1: 0.010372, l2: 0.011192, l3: 0.012663, l4: 0.016261, l5: 0.021785, l6: 0.036313\n\n[epoch:   7/ 50, batch:    12/    0, ite: 27] train loss: 0.114803, tar: 0.009912 \nl0: 0.081985, l1: 0.082102, l2: 0.083315, l3: 0.083261, l4: 0.078550, l5: 0.071539, l6: 0.092461\n\nl0: 0.005214, l1: 0.005294, l2: 0.006151, l3: 0.007064, l4: 0.009694, l5: 0.013275, l6: 0.021929\n\nl0: 0.028297, l1: 0.028096, l2: 0.029764, l3: 0.031894, l4: 0.039467, l5: 0.044433, l6: 0.056528\n\nl0: 0.007543, l1: 0.007586, l2: 0.009866, l3: 0.010962, l4: 0.012381, l5: 0.016621, l6: 0.028997\n\nl0: 0.082177, l1: 0.081968, l2: 0.083255, l3: 0.089353, l4: 0.098037, l5: 0.103507, l6: 0.115313\n\n[epoch:   7/ 50] Validation Loss: 0.329576\nl0: 0.011839, l1: 0.011898, l2: 0.012674, l3: 0.013876, l4: 0.017321, l5: 0.023714, l6: 0.035979\n\n[epoch:   7/ 50, batch:    16/    0, ite: 28] train loss: 0.116365, tar: 0.010153 \nl0: 0.056919, l1: 0.057189, l2: 0.058152, l3: 0.058639, l4: 0.063808, l5: 0.064716, l6: 0.086210\n\nl0: 0.005258, l1: 0.005368, l2: 0.006169, l3: 0.007215, l4: 0.009372, l5: 0.013828, l6: 0.019525\n\nl0: 0.027463, l1: 0.027287, l2: 0.029272, l3: 0.031475, l4: 0.036307, l5: 0.042088, l6: 0.045011\n\nl0: 0.008486, l1: 0.008441, l2: 0.010203, l3: 0.011352, l4: 0.013469, l5: 0.017618, l6: 0.029854\n\nl0: 0.083238, l1: 0.082681, l2: 0.080450, l3: 0.086494, l4: 0.082053, l5: 0.091076, l6: 0.132849\n\n[epoch:   7/ 50] Validation Loss: 0.297907\nl0: 0.006664, l1: 0.006767, l2: 0.007794, l3: 0.009163, l4: 0.011900, l5: 0.017509, l6: 0.032439\n\n[epoch:   8/ 50, batch:     4/    0, ite: 29] train loss: 0.113684, tar: 0.009766 \nl0: 0.062671, l1: 0.062627, l2: 0.063401, l3: 0.063221, l4: 0.066122, l5: 0.066649, l6: 0.085500\n\nl0: 0.005211, l1: 0.005269, l2: 0.006318, l3: 0.007314, l4: 0.010236, l5: 0.013623, l6: 0.018873\n\nl0: 0.033675, l1: 0.033572, l2: 0.034836, l3: 0.037273, l4: 0.039001, l5: 0.043531, l6: 0.048713\n\nl0: 0.007686, l1: 0.007713, l2: 0.009823, l3: 0.010988, l4: 0.013114, l5: 0.015107, l6: 0.031257\n\nl0: 0.020165, l1: 0.020283, l2: 0.020688, l3: 0.022180, l4: 0.029884, l5: 0.047624, l6: 0.077170\n\n[epoch:   8/ 50] Validation Loss: 0.228263\nl0: 0.012816, l1: 0.012978, l2: 0.013619, l3: 0.014736, l4: 0.017016, l5: 0.023333, l6: 0.039325\n\n[epoch:   8/ 50, batch:     8/    0, ite: 30] train loss: 0.115698, tar: 0.010071 \nl0: 0.021185, l1: 0.021305, l2: 0.022696, l3: 0.023744, l4: 0.033061, l5: 0.055812, l6: 0.080076\n\nl0: 0.006376, l1: 0.006552, l2: 0.007427, l3: 0.008481, l4: 0.010056, l5: 0.015538, l6: 0.028224\n\nl0: 0.027479, l1: 0.027249, l2: 0.029653, l3: 0.031860, l4: 0.037897, l5: 0.042416, l6: 0.049636\n\nl0: 0.007592, l1: 0.007567, l2: 0.010133, l3: 0.011588, l4: 0.013390, l5: 0.017388, l6: 0.026284\n\nl0: 0.078293, l1: 0.077259, l2: 0.076028, l3: 0.084331, l4: 0.081559, l5: 0.101133, l6: 0.134138\n\n[epoch:   8/ 50] Validation Loss: 0.262680\nl0: 0.015619, l1: 0.015724, l2: 0.016734, l3: 0.018055, l4: 0.020910, l5: 0.027869, l6: 0.042715\n\n[epoch:   8/ 50, batch:    12/    0, ite: 31] train loss: 0.157626, tar: 0.015619 \nl0: 0.032643, l1: 0.033301, l2: 0.032852, l3: 0.030250, l4: 0.035613, l5: 0.040411, l6: 0.075807\n\nl0: 0.005660, l1: 0.005718, l2: 0.006798, l3: 0.007914, l4: 0.010158, l5: 0.015289, l6: 0.022855\n\nl0: 0.037597, l1: 0.037590, l2: 0.038501, l3: 0.040463, l4: 0.039548, l5: 0.044530, l6: 0.050844\n\nl0: 0.009194, l1: 0.009156, l2: 0.011089, l3: 0.012126, l4: 0.015827, l5: 0.022126, l6: 0.035995\n\nl0: 0.035793, l1: 0.035263, l2: 0.037335, l3: 0.044394, l4: 0.054377, l5: 0.077344, l6: 0.107642\n\n[epoch:   8/ 50] Validation Loss: 0.230401\nl0: 0.012021, l1: 0.012093, l2: 0.013219, l3: 0.014422, l4: 0.016158, l5: 0.020027, l6: 0.030891\n\n[epoch:   8/ 50, batch:    16/    0, ite: 32] train loss: 0.138229, tar: 0.013820 \nl0: 0.059233, l1: 0.062327, l2: 0.067424, l3: 0.067355, l4: 0.074608, l5: 0.165823, l6: 0.248041\n\nl0: 0.005791, l1: 0.005897, l2: 0.007017, l3: 0.008227, l4: 0.010514, l5: 0.014505, l6: 0.027211\n\nl0: 0.029083, l1: 0.028824, l2: 0.030471, l3: 0.032942, l4: 0.038327, l5: 0.045133, l6: 0.056511\n\nl0: 0.007707, l1: 0.007691, l2: 0.010028, l3: 0.011338, l4: 0.013519, l5: 0.015982, l6: 0.028171\n\nl0: 0.086144, l1: 0.084680, l2: 0.085357, l3: 0.086476, l4: 0.096262, l5: 0.115745, l6: 0.139759\n\n[epoch:   8/ 50] Validation Loss: 0.374824\nl0: 0.007097, l1: 0.007269, l2: 0.008095, l3: 0.009536, l4: 0.012199, l5: 0.016648, l6: 0.032729\n\n[epoch:   9/ 50, batch:     4/    0, ite: 33] train loss: 0.123344, tar: 0.011579 \nl0: 0.051890, l1: 0.052203, l2: 0.052180, l3: 0.052443, l4: 0.052483, l5: 0.055905, l6: 0.081084\n\nl0: 0.005797, l1: 0.005939, l2: 0.007033, l3: 0.008202, l4: 0.010841, l5: 0.014945, l6: 0.025118\n\nl0: 0.029842, l1: 0.029626, l2: 0.031537, l3: 0.033957, l4: 0.035714, l5: 0.040975, l6: 0.050144\n\nl0: 0.008059, l1: 0.008001, l2: 0.009816, l3: 0.010948, l4: 0.013136, l5: 0.016991, l6: 0.028501\n\nl0: 0.082983, l1: 0.082684, l2: 0.083081, l3: 0.095972, l4: 0.100768, l5: 0.102095, l6: 0.111526\n\n[epoch:   9/ 50] Validation Loss: 0.296484\nl0: 0.010977, l1: 0.011085, l2: 0.011916, l3: 0.013165, l4: 0.015702, l5: 0.021843, l6: 0.034792\n\n[epoch:   9/ 50, batch:     8/    0, ite: 34] train loss: 0.122378, tar: 0.011429 \nl0: 0.049627, l1: 0.049882, l2: 0.049662, l3: 0.049814, l4: 0.049480, l5: 0.051301, l6: 0.076487\n\nl0: 0.004973, l1: 0.005086, l2: 0.005878, l3: 0.007145, l4: 0.009608, l5: 0.014247, l6: 0.021824\n\nl0: 0.026845, l1: 0.026556, l2: 0.029580, l3: 0.032647, l4: 0.038422, l5: 0.042480, l6: 0.050060\n\nl0: 0.008304, l1: 0.008288, l2: 0.010210, l3: 0.011621, l4: 0.014839, l5: 0.022024, l6: 0.032741\n\nl0: 0.020656, l1: 0.020600, l2: 0.022020, l3: 0.024693, l4: 0.040127, l5: 0.046398, l6: 0.087603\n\n[epoch:   9/ 50] Validation Loss: 0.212346\nl0: 0.008406, l1: 0.008449, l2: 0.009421, l3: 0.011055, l4: 0.015001, l5: 0.023628, l6: 0.033541\n\n[epoch:   9/ 50, batch:    12/    0, ite: 35] train loss: 0.119803, tar: 0.010824 \nl0: 0.092204, l1: 0.092208, l2: 0.092055, l3: 0.091929, l4: 0.082814, l5: 0.076924, l6: 0.114775\n\nl0: 0.006050, l1: 0.006101, l2: 0.007425, l3: 0.008722, l4: 0.011282, l5: 0.015679, l6: 0.024829\n\nl0: 0.024130, l1: 0.024005, l2: 0.026577, l3: 0.028954, l4: 0.035164, l5: 0.041269, l6: 0.046430\n\nl0: 0.009546, l1: 0.009513, l2: 0.011214, l3: 0.012264, l4: 0.016054, l5: 0.023020, l6: 0.033414\n\nl0: 0.021599, l1: 0.021645, l2: 0.022327, l3: 0.024711, l4: 0.035598, l5: 0.048275, l6: 0.079743\n\n[epoch:   9/ 50] Validation Loss: 0.263690\nl0: 0.007580, l1: 0.007663, l2: 0.008547, l3: 0.009674, l4: 0.012658, l5: 0.017287, l6: 0.027789\n\n[epoch:   9/ 50, batch:    16/    0, ite: 36] train loss: 0.115035, tar: 0.010283 \nl0: 0.032211, l1: 0.032426, l2: 0.035349, l3: 0.037351, l4: 0.048192, l5: 0.060173, l6: 0.087598\n\nl0: 0.005116, l1: 0.005182, l2: 0.006008, l3: 0.006871, l4: 0.009794, l5: 0.013943, l6: 0.021887\n\nl0: 0.025293, l1: 0.025106, l2: 0.027751, l3: 0.029854, l4: 0.037172, l5: 0.040287, l6: 0.050576\n\nl0: 0.007675, l1: 0.007647, l2: 0.010041, l3: 0.011282, l4: 0.013654, l5: 0.015612, l6: 0.031296\n\nl0: 0.074308, l1: 0.073602, l2: 0.073663, l3: 0.078210, l4: 0.079774, l5: 0.089793, l6: 0.118777\n\n[epoch:   9/ 50] Validation Loss: 0.264695\nl0: 0.006874, l1: 0.007017, l2: 0.007555, l3: 0.008732, l4: 0.011259, l5: 0.017428, l6: 0.028725\n\n[epoch:  10/ 50, batch:     4/    0, ite: 37] train loss: 0.111114, tar: 0.009796 \nl0: 0.062238, l1: 0.062666, l2: 0.063938, l3: 0.064161, l4: 0.068076, l5: 0.066250, l6: 0.083481\n\nl0: 0.005354, l1: 0.005412, l2: 0.006294, l3: 0.007387, l4: 0.011150, l5: 0.014383, l6: 0.021501\n\nl0: 0.032923, l1: 0.032756, l2: 0.034638, l3: 0.036978, l4: 0.039171, l5: 0.046127, l6: 0.058610\n\nl0: 0.008713, l1: 0.008640, l2: 0.010795, l3: 0.011674, l4: 0.014495, l5: 0.020065, l6: 0.029153\n\nl0: 0.085324, l1: 0.084061, l2: 0.083055, l3: 0.086372, l4: 0.084331, l5: 0.100801, l6: 0.130313\n\n[epoch:  10/ 50] Validation Loss: 0.316257\nl0: 0.012794, l1: 0.012926, l2: 0.013604, l3: 0.014879, l4: 0.016550, l5: 0.021517, l6: 0.034181\n\n[epoch:  10/ 50, batch:     8/    0, ite: 38] train loss: 0.113031, tar: 0.010171 \nl0: 0.056081, l1: 0.058498, l2: 0.061635, l3: 0.062760, l4: 0.064571, l5: 0.114027, l6: 0.207247\n\nl0: 0.005707, l1: 0.005827, l2: 0.006853, l3: 0.007936, l4: 0.010363, l5: 0.016074, l6: 0.026544\n\nl0: 0.024399, l1: 0.024211, l2: 0.026719, l3: 0.028575, l4: 0.035695, l5: 0.039242, l6: 0.045305\n\nl0: 0.008137, l1: 0.008079, l2: 0.009829, l3: 0.010792, l4: 0.013105, l5: 0.016726, l6: 0.029480\n\nl0: 0.107868, l1: 0.107200, l2: 0.107281, l3: 0.112007, l4: 0.094116, l5: 0.108565, l6: 0.119276\n\n[epoch:  10/ 50] Validation Loss: 0.356146\nl0: 0.008166, l1: 0.008242, l2: 0.009503, l3: 0.011497, l4: 0.014984, l5: 0.021662, l6: 0.036719\n\n[epoch:  10/ 50, batch:    12/    0, ite: 39] train loss: 0.112780, tar: 0.009948 \nl0: 0.081367, l1: 0.081524, l2: 0.082046, l3: 0.082510, l4: 0.078854, l5: 0.080710, l6: 0.103153\n\nl0: 0.005558, l1: 0.005627, l2: 0.006997, l3: 0.008264, l4: 0.010690, l5: 0.014124, l6: 0.025109\n\nl0: 0.031929, l1: 0.031824, l2: 0.032921, l3: 0.035192, l4: 0.035790, l5: 0.043147, l6: 0.060030\n\nl0: 0.007721, l1: 0.007657, l2: 0.009641, l3: 0.010730, l4: 0.012959, l5: 0.016685, l6: 0.028918\n\nl0: 0.035598, l1: 0.034901, l2: 0.038205, l3: 0.044960, l4: 0.060159, l5: 0.081137, l6: 0.093898\n\n[epoch:  10/ 50] Validation Loss: 0.284106\nl0: 0.007614, l1: 0.007740, l2: 0.008554, l3: 0.009962, l4: 0.013199, l5: 0.020123, l6: 0.035934\n\n[epoch:  10/ 50, batch:    16/    0, ite: 40] train loss: 0.111815, tar: 0.009715 \nl0: 0.082472, l1: 0.082546, l2: 0.083055, l3: 0.083121, l4: 0.081580, l5: 0.082295, l6: 0.104218\n\nl0: 0.004878, l1: 0.004929, l2: 0.006125, l3: 0.007505, l4: 0.009985, l5: 0.013922, l6: 0.020762\n\nl0: 0.032621, l1: 0.032515, l2: 0.034195, l3: 0.036637, l4: 0.037868, l5: 0.043182, l6: 0.049817\n\nl0: 0.008479, l1: 0.008449, l2: 0.010243, l3: 0.011236, l4: 0.013900, l5: 0.018523, l6: 0.028061\n\nl0: 0.101536, l1: 0.101243, l2: 0.099447, l3: 0.102370, l4: 0.093946, l5: 0.105874, l6: 0.153867\n\n[epoch:  10/ 50] Validation Loss: 0.358280\nl0: 0.007357, l1: 0.007458, l2: 0.008856, l3: 0.010714, l4: 0.014070, l5: 0.019131, l6: 0.035627\n\n[epoch:  11/ 50, batch:     4/    0, ite: 41] train loss: 0.103213, tar: 0.007357 \nl0: 0.078499, l1: 0.078483, l2: 0.078084, l3: 0.077724, l4: 0.077424, l5: 0.073516, l6: 0.098079\n\nl0: 0.004951, l1: 0.005015, l2: 0.006091, l3: 0.007514, l4: 0.009963, l5: 0.014637, l6: 0.021083\n\nl0: 0.023920, l1: 0.023772, l2: 0.026101, l3: 0.028521, l4: 0.036110, l5: 0.041157, l6: 0.048395\n\nl0: 0.008899, l1: 0.008835, l2: 0.010779, l3: 0.011625, l4: 0.014219, l5: 0.020295, l6: 0.028651\n\nl0: 0.078986, l1: 0.079167, l2: 0.077716, l3: 0.085710, l4: 0.082734, l5: 0.102095, l6: 0.113141\n\n[epoch:  11/ 50] Validation Loss: 0.316378\nl0: 0.006448, l1: 0.006536, l2: 0.007475, l3: 0.008996, l4: 0.011963, l5: 0.016835, l6: 0.028427\n\n[epoch:  11/ 50, batch:     8/    0, ite: 42] train loss: 0.094947, tar: 0.006902 \nl0: 0.053344, l1: 0.053411, l2: 0.053646, l3: 0.055310, l4: 0.057193, l5: 0.058004, l6: 0.080340\n\nl0: 0.005004, l1: 0.005120, l2: 0.005740, l3: 0.006764, l4: 0.008924, l5: 0.014434, l6: 0.019482\n\nl0: 0.025173, l1: 0.025007, l2: 0.027250, l3: 0.029729, l4: 0.035056, l5: 0.039464, l6: 0.045770\n\nl0: 0.009024, l1: 0.008994, l2: 0.011042, l3: 0.012405, l4: 0.016429, l5: 0.021729, l6: 0.034960\n\nl0: 0.086298, l1: 0.085994, l2: 0.086874, l3: 0.090017, l4: 0.096042, l5: 0.108657, l6: 0.134340\n\n[epoch:  11/ 50] Validation Loss: 0.301395\nl0: 0.008846, l1: 0.008994, l2: 0.009492, l3: 0.010303, l4: 0.012608, l5: 0.018094, l6: 0.030448\n\n[epoch:  11/ 50, batch:    12/    0, ite: 43] train loss: 0.096226, tar: 0.007550 \nl0: 0.064216, l1: 0.064199, l2: 0.065305, l3: 0.065711, l4: 0.065044, l5: 0.062477, l6: 0.071007\n\nl0: 0.006107, l1: 0.006261, l2: 0.007939, l3: 0.008937, l4: 0.013464, l5: 0.016410, l6: 0.023577\n\nl0: 0.030729, l1: 0.030527, l2: 0.031758, l3: 0.034046, l4: 0.035891, l5: 0.042456, l6: 0.051066\n\nl0: 0.008016, l1: 0.007964, l2: 0.010394, l3: 0.011665, l4: 0.013574, l5: 0.016177, l6: 0.032093\n\nl0: 0.064521, l1: 0.064264, l2: 0.066355, l3: 0.070976, l4: 0.083561, l5: 0.089934, l6: 0.110604\n\n[epoch:  11/ 50] Validation Loss: 0.289445\nl0: 0.007876, l1: 0.008027, l2: 0.009028, l3: 0.010784, l4: 0.014044, l5: 0.019653, l6: 0.033123\n\n[epoch:  11/ 50, batch:    16/    0, ite: 44] train loss: 0.097803, tar: 0.007632 \nl0: 0.056713, l1: 0.056202, l2: 0.057414, l3: 0.059023, l4: 0.062346, l5: 0.071821, l6: 0.093442\n\nl0: 0.006601, l1: 0.006819, l2: 0.008404, l3: 0.009578, l4: 0.013471, l5: 0.014965, l6: 0.019593\n\nl0: 0.023764, l1: 0.023548, l2: 0.026550, l3: 0.028896, l4: 0.034250, l5: 0.041805, l6: 0.053582\n\nl0: 0.008461, l1: 0.008517, l2: 0.010382, l3: 0.011228, l4: 0.013154, l5: 0.016760, l6: 0.025790\n\nl0: 0.085502, l1: 0.085049, l2: 0.084707, l3: 0.087510, l4: 0.077900, l5: 0.093030, l6: 0.137236\n\n[epoch:  11/ 50] Validation Loss: 0.302802\nl0: 0.008248, l1: 0.008366, l2: 0.009219, l3: 0.010380, l4: 0.013995, l5: 0.021320, l6: 0.038211\n\n[epoch:  12/ 50, batch:     4/    0, ite: 45] train loss: 0.100191, tar: 0.007755 \nl0: 0.056688, l1: 0.055739, l2: 0.055730, l3: 0.055636, l4: 0.066129, l5: 0.070024, l6: 0.095728\n\nl0: 0.009782, l1: 0.009913, l2: 0.010554, l3: 0.011474, l4: 0.013776, l5: 0.015997, l6: 0.023381\n\nl0: 0.031736, l1: 0.031622, l2: 0.033239, l3: 0.035463, l4: 0.036824, l5: 0.043522, l6: 0.046941\n\nl0: 0.008302, l1: 0.008269, l2: 0.010162, l3: 0.011323, l4: 0.013430, l5: 0.018572, l6: 0.028152\n\nl0: 0.062511, l1: 0.061433, l2: 0.062088, l3: 0.068824, l4: 0.062410, l5: 0.080881, l6: 0.127616\n\n[epoch:  12/ 50] Validation Loss: 0.286775\nl0: 0.006562, l1: 0.006645, l2: 0.007711, l3: 0.009429, l4: 0.012750, l5: 0.018465, l6: 0.033537\n\n[epoch:  12/ 50, batch:     8/    0, ite: 46] train loss: 0.099342, tar: 0.007556 \nl0: 0.024927, l1: 0.024892, l2: 0.027052, l3: 0.029597, l4: 0.038406, l5: 0.057172, l6: 0.093728\n\nl0: 0.009949, l1: 0.010019, l2: 0.010813, l3: 0.011645, l4: 0.014905, l5: 0.015203, l6: 0.022447\n\nl0: 0.023830, l1: 0.023630, l2: 0.026440, l3: 0.029305, l4: 0.035819, l5: 0.042318, l6: 0.058112\n\nl0: 0.008359, l1: 0.008300, l2: 0.010257, l3: 0.011651, l4: 0.013904, l5: 0.020119, l6: 0.029571\n\nl0: 0.065952, l1: 0.065691, l2: 0.066814, l3: 0.069344, l4: 0.055214, l5: 0.071593, l6: 0.103132\n\n[epoch:  12/ 50] Validation Loss: 0.246022\nl0: 0.008336, l1: 0.008407, l2: 0.009273, l3: 0.010391, l4: 0.013215, l5: 0.017621, l6: 0.028560\n\n[epoch:  12/ 50, batch:    12/    0, ite: 47] train loss: 0.098837, tar: 0.007668 \nl0: 0.050462, l1: 0.049704, l2: 0.052077, l3: 0.051839, l4: 0.070532, l5: 0.068554, l6: 0.101140\n\nl0: 0.004916, l1: 0.004991, l2: 0.005701, l3: 0.006540, l4: 0.009482, l5: 0.012544, l6: 0.019899\n\nl0: 0.032418, l1: 0.032120, l2: 0.033134, l3: 0.035689, l4: 0.039746, l5: 0.042860, l6: 0.048117\n\nl0: 0.008336, l1: 0.008269, l2: 0.010353, l3: 0.011388, l4: 0.013614, l5: 0.018614, l6: 0.029054\n\nl0: 0.068690, l1: 0.068007, l2: 0.068162, l3: 0.071952, l4: 0.059112, l5: 0.078460, l6: 0.111292\n\n[epoch:  12/ 50] Validation Loss: 0.279554\nl0: 0.006563, l1: 0.006663, l2: 0.007475, l3: 0.008749, l4: 0.011062, l5: 0.015150, l6: 0.025919\n\n[epoch:  12/ 50, batch:    16/    0, ite: 48] train loss: 0.096680, tar: 0.007530 \nl0: 0.060244, l1: 0.060048, l2: 0.061191, l3: 0.058709, l4: 0.064431, l5: 0.061682, l6: 0.089685\n\nl0: 0.005391, l1: 0.005467, l2: 0.006487, l3: 0.007369, l4: 0.010042, l5: 0.013524, l6: 0.023488\n\nl0: 0.027406, l1: 0.027110, l2: 0.029073, l3: 0.032193, l4: 0.038093, l5: 0.042363, l6: 0.047099\n\nl0: 0.008199, l1: 0.008155, l2: 0.010259, l3: 0.011524, l4: 0.013035, l5: 0.015640, l6: 0.027575\n\nl0: 0.021215, l1: 0.021091, l2: 0.021355, l3: 0.023496, l4: 0.027731, l5: 0.048508, l6: 0.157150\n\n[epoch:  12/ 50] Validation Loss: 0.237205\nl0: 0.008974, l1: 0.009100, l2: 0.009774, l3: 0.010701, l4: 0.012523, l5: 0.017983, l6: 0.031950\n\n[epoch:  13/ 50, batch:     4/    0, ite: 49] train loss: 0.097160, tar: 0.007690 \nl0: 0.026773, l1: 0.027050, l2: 0.030238, l3: 0.033135, l4: 0.043317, l5: 0.065122, l6: 0.097597\n\nl0: 0.004877, l1: 0.005000, l2: 0.005566, l3: 0.006533, l4: 0.008669, l5: 0.013923, l6: 0.018463\n\nl0: 0.025350, l1: 0.025176, l2: 0.027502, l3: 0.030652, l4: 0.034538, l5: 0.040950, l6: 0.056445\n\nl0: 0.007624, l1: 0.007604, l2: 0.009417, l3: 0.010458, l4: 0.012452, l5: 0.016238, l6: 0.029665\n\nl0: 0.063637, l1: 0.063441, l2: 0.064678, l3: 0.066856, l4: 0.069053, l5: 0.083422, l6: 0.095229\n\n[epoch:  13/ 50] Validation Loss: 0.245330\nl0: 0.007589, l1: 0.007658, l2: 0.008988, l3: 0.010319, l4: 0.013761, l5: 0.020470, l6: 0.033748\n\n[epoch:  13/ 50, batch:     8/    0, ite: 50] train loss: 0.097698, tar: 0.007680 \nl0: 0.041429, l1: 0.041130, l2: 0.042788, l3: 0.043484, l4: 0.051345, l5: 0.067493, l6: 0.082047\n\nl0: 0.004354, l1: 0.004497, l2: 0.005072, l3: 0.006415, l4: 0.009159, l5: 0.015255, l6: 0.022804\n\nl0: 0.025796, l1: 0.025510, l2: 0.027763, l3: 0.030827, l4: 0.036450, l5: 0.042769, l6: 0.055968\n\nl0: 0.008840, l1: 0.008767, l2: 0.010969, l3: 0.011978, l4: 0.013223, l5: 0.017820, l6: 0.029053\n\nl0: 0.080226, l1: 0.079880, l2: 0.078829, l3: 0.086260, l4: 0.077767, l5: 0.086365, l6: 0.157075\n\n[epoch:  13/ 50] Validation Loss: 0.285881\nl0: 0.007771, l1: 0.007892, l2: 0.008792, l3: 0.010201, l4: 0.012770, l5: 0.017605, l6: 0.033457\n\n[epoch:  13/ 50, batch:    12/    0, ite: 51] train loss: 0.098488, tar: 0.007771 \nl0: 0.036229, l1: 0.036221, l2: 0.035788, l3: 0.035405, l4: 0.041786, l5: 0.039811, l6: 0.083829\n\nl0: 0.006005, l1: 0.006093, l2: 0.007102, l3: 0.007994, l4: 0.010899, l5: 0.014096, l6: 0.024550\n\nl0: 0.027158, l1: 0.026778, l2: 0.029085, l3: 0.032411, l4: 0.037714, l5: 0.043424, l6: 0.044881\n\nl0: 0.007767, l1: 0.007679, l2: 0.009794, l3: 0.010966, l4: 0.012833, l5: 0.016849, l6: 0.031599\n\nl0: 0.019103, l1: 0.019067, l2: 0.022087, l3: 0.024347, l4: 0.037816, l5: 0.063095, l6: 0.110520\n\n[epoch:  13/ 50] Validation Loss: 0.204156\nl0: 0.006285, l1: 0.006371, l2: 0.007247, l3: 0.008530, l4: 0.011336, l5: 0.016303, l6: 0.029528\n\n[epoch:  13/ 50, batch:    16/    0, ite: 52] train loss: 0.092044, tar: 0.007028 \nl0: 0.074114, l1: 0.074026, l2: 0.072248, l3: 0.072625, l4: 0.070133, l5: 0.062860, l6: 0.091206\n\nl0: 0.005004, l1: 0.005083, l2: 0.005795, l3: 0.006607, l4: 0.009209, l5: 0.012337, l6: 0.018900\n\nl0: 0.023299, l1: 0.023155, l2: 0.025609, l3: 0.027709, l4: 0.033188, l5: 0.042734, l6: 0.046011\n\nl0: 0.008035, l1: 0.008072, l2: 0.010139, l3: 0.011017, l4: 0.012249, l5: 0.016944, l6: 0.025793\n\nl0: 0.020112, l1: 0.020001, l2: 0.020406, l3: 0.022529, l4: 0.026861, l5: 0.045140, l6: 0.128470\n\n[epoch:  13/ 50] Validation Loss: 0.235524\nl0: 0.005873, l1: 0.005982, l2: 0.006906, l3: 0.008107, l4: 0.010233, l5: 0.016634, l6: 0.033430\n\n[epoch:  14/ 50, batch:     4/    0, ite: 53] train loss: 0.090418, tar: 0.006643 \nl0: 0.059855, l1: 0.059902, l2: 0.061665, l3: 0.061629, l4: 0.068370, l5: 0.066950, l6: 0.101874\n\nl0: 0.004544, l1: 0.004685, l2: 0.005239, l3: 0.006398, l4: 0.008637, l5: 0.014015, l6: 0.022341\n\nl0: 0.024358, l1: 0.024006, l2: 0.026762, l3: 0.029767, l4: 0.037111, l5: 0.041343, l6: 0.044675\n\nl0: 0.009072, l1: 0.009042, l2: 0.011090, l3: 0.012243, l4: 0.013277, l5: 0.018246, l6: 0.028895\n\nl0: 0.086847, l1: 0.086232, l2: 0.086640, l3: 0.091391, l4: 0.093153, l5: 0.096875, l6: 0.113337\n\n[epoch:  14/ 50] Validation Loss: 0.306093\nl0: 0.009647, l1: 0.009801, l2: 0.010429, l3: 0.011461, l4: 0.013664, l5: 0.019913, l6: 0.034695\n\n[epoch:  14/ 50, batch:     8/    0, ite: 54] train loss: 0.095216, tar: 0.007394 \nl0: 0.059677, l1: 0.059172, l2: 0.058942, l3: 0.060078, l4: 0.059539, l5: 0.066344, l6: 0.094804\n\nl0: 0.009542, l1: 0.009652, l2: 0.010025, l3: 0.011023, l4: 0.013986, l5: 0.016204, l6: 0.024239\n\nl0: 0.032240, l1: 0.032080, l2: 0.033506, l3: 0.035570, l4: 0.037762, l5: 0.043572, l6: 0.049716\n\nl0: 0.008719, l1: 0.008705, l2: 0.010473, l3: 0.011535, l4: 0.012578, l5: 0.015744, l6: 0.029760\n\nl0: 0.046921, l1: 0.047077, l2: 0.047660, l3: 0.052997, l4: 0.052450, l5: 0.053386, l6: 0.097534\n\n[epoch:  14/ 50] Validation Loss: 0.262641\nl0: 0.006427, l1: 0.006521, l2: 0.007357, l3: 0.008596, l4: 0.011660, l5: 0.017468, l6: 0.028006\n\n[epoch:  14/ 50, batch:    12/    0, ite: 55] train loss: 0.093380, tar: 0.007200 \nl0: 0.051556, l1: 0.050064, l2: 0.050408, l3: 0.051218, l4: 0.061910, l5: 0.075959, l6: 0.081927\n\nl0: 0.004418, l1: 0.004520, l2: 0.005308, l3: 0.006707, l4: 0.009250, l5: 0.013677, l6: 0.020448\n\nl0: 0.027445, l1: 0.027042, l2: 0.029346, l3: 0.033315, l4: 0.039055, l5: 0.042951, l6: 0.045351\n\nl0: 0.006765, l1: 0.006804, l2: 0.008813, l3: 0.009898, l4: 0.011849, l5: 0.016663, l6: 0.027941\n\nl0: 0.019371, l1: 0.019088, l2: 0.020152, l3: 0.022691, l4: 0.034314, l5: 0.059371, l6: 0.097074\n\n[epoch:  14/ 50] Validation Loss: 0.218534\nl0: 0.008184, l1: 0.008284, l2: 0.009306, l3: 0.010828, l4: 0.014257, l5: 0.019831, l6: 0.038834\n\n[epoch:  14/ 50, batch:    16/    0, ite: 56] train loss: 0.096070, tar: 0.007364 \nl0: 0.062274, l1: 0.062265, l2: 0.063004, l3: 0.063305, l4: 0.065242, l5: 0.065420, l6: 0.086824\n\nl0: 0.004485, l1: 0.004582, l2: 0.005457, l3: 0.006814, l4: 0.009376, l5: 0.014391, l6: 0.021982\n\nl0: 0.024081, l1: 0.023819, l2: 0.025945, l3: 0.028535, l4: 0.035498, l5: 0.041574, l6: 0.045643\n\nl0: 0.007797, l1: 0.007842, l2: 0.009879, l3: 0.011180, l4: 0.012674, l5: 0.015305, l6: 0.027690\n\nl0: 0.019464, l1: 0.019301, l2: 0.019644, l3: 0.022138, l4: 0.028815, l5: 0.046916, l6: 0.127729\n\n[epoch:  14/ 50] Validation Loss: 0.227378\nl0: 0.007309, l1: 0.007446, l2: 0.008051, l3: 0.009337, l4: 0.011372, l5: 0.016623, l6: 0.028531\n\n[epoch:  15/ 50, batch:     4/    0, ite: 57] train loss: 0.095013, tar: 0.007356 \nl0: 0.062738, l1: 0.062714, l2: 0.063570, l3: 0.064525, l4: 0.067522, l5: 0.062803, l6: 0.078493\n\nl0: 0.005353, l1: 0.005438, l2: 0.006568, l3: 0.007574, l4: 0.010266, l5: 0.013991, l6: 0.025205\n\nl0: 0.027151, l1: 0.026989, l2: 0.028939, l3: 0.031163, l4: 0.033491, l5: 0.039261, l6: 0.043560\n\nl0: 0.009192, l1: 0.009155, l2: 0.011072, l3: 0.012141, l4: 0.012807, l5: 0.015949, l6: 0.028456\n\nl0: 0.015762, l1: 0.015565, l2: 0.016403, l3: 0.018580, l4: 0.024423, l5: 0.030820, l6: 0.078702\n\n[epoch:  15/ 50] Validation Loss: 0.213268\nl0: 0.006285, l1: 0.006337, l2: 0.007346, l3: 0.008854, l4: 0.012460, l5: 0.017965, l6: 0.031828\n\n[epoch:  15/ 50, batch:     8/    0, ite: 58] train loss: 0.094521, tar: 0.007223 \nl0: 0.059710, l1: 0.059545, l2: 0.061699, l3: 0.062418, l4: 0.066838, l5: 0.065155, l6: 0.099750\n\nl0: 0.009882, l1: 0.010005, l2: 0.010163, l3: 0.010970, l4: 0.013461, l5: 0.014636, l6: 0.024046\n\nl0: 0.026599, l1: 0.026360, l2: 0.028534, l3: 0.031840, l4: 0.036187, l5: 0.042136, l6: 0.046407\n\nl0: 0.007012, l1: 0.007057, l2: 0.008814, l3: 0.010073, l4: 0.011525, l5: 0.013904, l6: 0.024406\n\nl0: 0.079431, l1: 0.078042, l2: 0.076635, l3: 0.081282, l4: 0.068842, l5: 0.091439, l6: 0.126773\n\n[epoch:  15/ 50] Validation Loss: 0.298315\nl0: 0.006009, l1: 0.006108, l2: 0.006920, l3: 0.008368, l4: 0.010643, l5: 0.014971, l6: 0.024111\n\n[epoch:  15/ 50, batch:    12/    0, ite: 59] train loss: 0.092589, tar: 0.007088 \nl0: 0.078969, l1: 0.079118, l2: 0.078827, l3: 0.078614, l4: 0.075094, l5: 0.069179, l6: 0.085552\n\nl0: 0.005573, l1: 0.005583, l2: 0.006680, l3: 0.007627, l4: 0.010654, l5: 0.013875, l6: 0.021277\n\nl0: 0.023787, l1: 0.023663, l2: 0.025542, l3: 0.027763, l4: 0.033270, l5: 0.038861, l6: 0.045732\n\nl0: 0.007416, l1: 0.007361, l2: 0.009588, l3: 0.010844, l4: 0.011771, l5: 0.013948, l6: 0.026435\n\nl0: 0.043708, l1: 0.041733, l2: 0.042612, l3: 0.043987, l4: 0.050611, l5: 0.071635, l6: 0.105212\n\n[epoch:  15/ 50] Validation Loss: 0.264420\nl0: 0.008795, l1: 0.008962, l2: 0.009599, l3: 0.010464, l4: 0.012964, l5: 0.021063, l6: 0.038820\n\n[epoch:  15/ 50, batch:    16/    0, ite: 60] train loss: 0.094396, tar: 0.007258 \nl0: 0.045571, l1: 0.045706, l2: 0.046470, l3: 0.047825, l4: 0.047733, l5: 0.055517, l6: 0.090156\n\nl0: 0.006628, l1: 0.006780, l2: 0.007232, l3: 0.008234, l4: 0.009802, l5: 0.013730, l6: 0.025107\n\nl0: 0.028960, l1: 0.028652, l2: 0.031104, l3: 0.035558, l4: 0.039277, l5: 0.042063, l6: 0.046513\n\nl0: 0.008783, l1: 0.008772, l2: 0.010487, l3: 0.011360, l4: 0.012513, l5: 0.015424, l6: 0.028341\n\nl0: 0.019234, l1: 0.019193, l2: 0.019067, l3: 0.020410, l4: 0.023333, l5: 0.043326, l6: 0.120179\n\n[epoch:  15/ 50] Validation Loss: 0.213808\nl0: 0.007165, l1: 0.007203, l2: 0.008273, l3: 0.009782, l4: 0.013424, l5: 0.019577, l6: 0.034027\n\n[epoch:  16/ 50, batch:     4/    0, ite: 61] train loss: 0.099452, tar: 0.007165 \nl0: 0.041409, l1: 0.040820, l2: 0.043660, l3: 0.043488, l4: 0.057316, l5: 0.070866, l6: 0.098792\n\nl0: 0.005909, l1: 0.005963, l2: 0.007000, l3: 0.007926, l4: 0.010427, l5: 0.013459, l6: 0.022439\n\nl0: 0.025492, l1: 0.025247, l2: 0.027817, l3: 0.031434, l4: 0.036138, l5: 0.039178, l6: 0.043133\n\nl0: 0.007713, l1: 0.007734, l2: 0.009589, l3: 0.010693, l4: 0.011505, l5: 0.013629, l6: 0.025727\n\nl0: 0.042890, l1: 0.041999, l2: 0.043908, l3: 0.049669, l4: 0.060540, l5: 0.081687, l6: 0.107019\n\n[epoch:  16/ 50] Validation Loss: 0.242443\nl0: 0.008470, l1: 0.008574, l2: 0.009520, l3: 0.010809, l4: 0.012700, l5: 0.016463, l6: 0.026836\n\n[epoch:  16/ 50, batch:     8/    0, ite: 62] train loss: 0.096412, tar: 0.007817 \nl0: 0.023229, l1: 0.023226, l2: 0.025704, l3: 0.029001, l4: 0.036121, l5: 0.049205, l6: 0.091486\n\nl0: 0.005559, l1: 0.005591, l2: 0.006882, l3: 0.007909, l4: 0.010206, l5: 0.013199, l6: 0.023820\n\nl0: 0.033291, l1: 0.033176, l2: 0.034593, l3: 0.036407, l4: 0.036659, l5: 0.042671, l6: 0.052067\n\nl0: 0.008040, l1: 0.007982, l2: 0.010246, l3: 0.011828, l4: 0.015095, l5: 0.018993, l6: 0.030878\n\nl0: 0.020637, l1: 0.020510, l2: 0.020958, l3: 0.023186, l4: 0.030394, l5: 0.041668, l6: 0.094224\n\n[epoch:  16/ 50] Validation Loss: 0.194929\nl0: 0.008444, l1: 0.008577, l2: 0.009147, l3: 0.010287, l4: 0.012634, l5: 0.018269, l6: 0.031288\n\n[epoch:  16/ 50, batch:    12/    0, ite: 63] train loss: 0.097157, tar: 0.008026 \nl0: 0.048598, l1: 0.048757, l2: 0.048843, l3: 0.049120, l4: 0.048637, l5: 0.051871, l6: 0.071689\n\nl0: 0.004545, l1: 0.004632, l2: 0.005343, l3: 0.006651, l4: 0.009003, l5: 0.013240, l6: 0.018862\n\nl0: 0.027347, l1: 0.026984, l2: 0.029725, l3: 0.033519, l4: 0.036040, l5: 0.041337, l6: 0.045557\n\nl0: 0.008403, l1: 0.008311, l2: 0.010182, l3: 0.011369, l4: 0.014059, l5: 0.020022, l6: 0.029703\n\nl0: 0.018692, l1: 0.018518, l2: 0.020675, l3: 0.027598, l4: 0.049475, l5: 0.081412, l6: 0.104600\n\n[epoch:  16/ 50] Validation Loss: 0.218664\nl0: 0.006960, l1: 0.007078, l2: 0.007737, l3: 0.008958, l4: 0.011489, l5: 0.018358, l6: 0.034824\n\n[epoch:  16/ 50, batch:    16/    0, ite: 64] train loss: 0.096719, tar: 0.007760 \nl0: 0.045255, l1: 0.045350, l2: 0.047208, l3: 0.049402, l4: 0.056442, l5: 0.068275, l6: 0.095047\n\nl0: 0.004589, l1: 0.004688, l2: 0.005362, l3: 0.006694, l4: 0.009046, l5: 0.012884, l6: 0.020151\n\nl0: 0.023229, l1: 0.023023, l2: 0.025931, l3: 0.028293, l4: 0.035278, l5: 0.040388, l6: 0.047513\n\nl0: 0.006962, l1: 0.006969, l2: 0.009105, l3: 0.010607, l4: 0.012930, l5: 0.016362, l6: 0.027579\n\nl0: 0.024298, l1: 0.024122, l2: 0.030026, l3: 0.038588, l4: 0.065923, l5: 0.077260, l6: 0.104720\n\n[epoch:  16/ 50] Validation Loss: 0.229900\nl0: 0.006373, l1: 0.006501, l2: 0.007178, l3: 0.008192, l4: 0.011116, l5: 0.014823, l6: 0.028848\n\n[epoch:  17/ 50, batch:     4/    0, ite: 65] train loss: 0.093981, tar: 0.007482 \nl0: 0.058964, l1: 0.062218, l2: 0.067291, l3: 0.065082, l4: 0.065218, l5: 0.118248, l6: 0.226427\n\nl0: 0.009104, l1: 0.009197, l2: 0.009681, l3: 0.010553, l4: 0.013965, l5: 0.016049, l6: 0.024805\n\nl0: 0.030368, l1: 0.029990, l2: 0.031957, l3: 0.035261, l4: 0.036692, l5: 0.040586, l6: 0.046399\n\nl0: 0.008011, l1: 0.008083, l2: 0.009677, l3: 0.010613, l4: 0.012152, l5: 0.013779, l6: 0.025501\n\nl0: 0.065524, l1: 0.065693, l2: 0.065607, l3: 0.065729, l4: 0.064809, l5: 0.064126, l6: 0.084653\n\n[epoch:  17/ 50] Validation Loss: 0.314403\nl0: 0.005306, l1: 0.005392, l2: 0.006445, l3: 0.007777, l4: 0.010869, l5: 0.016050, l6: 0.027959\n\n[epoch:  17/ 50, batch:     8/    0, ite: 66] train loss: 0.091618, tar: 0.007120 \nl0: 0.055511, l1: 0.055075, l2: 0.057214, l3: 0.058794, l4: 0.062761, l5: 0.077513, l6: 0.086971\n\nl0: 0.004765, l1: 0.004851, l2: 0.005668, l3: 0.006986, l4: 0.009301, l5: 0.014295, l6: 0.022902\n\nl0: 0.027211, l1: 0.026898, l2: 0.029300, l3: 0.032720, l4: 0.036581, l5: 0.039486, l6: 0.042808\n\nl0: 0.007692, l1: 0.007603, l2: 0.009600, l3: 0.011014, l4: 0.012986, l5: 0.016709, l6: 0.029953\n\nl0: 0.085730, l1: 0.085499, l2: 0.085523, l3: 0.087810, l4: 0.082475, l5: 0.091347, l6: 0.121006\n\n[epoch:  17/ 50] Validation Loss: 0.298512\nl0: 0.008531, l1: 0.008625, l2: 0.009744, l3: 0.011183, l4: 0.013932, l5: 0.020098, l6: 0.033566\n\n[epoch:  17/ 50, batch:    12/    0, ite: 67] train loss: 0.093626, tar: 0.007321 \nl0: 0.076640, l1: 0.076761, l2: 0.075511, l3: 0.074300, l4: 0.065734, l5: 0.062136, l6: 0.119078\n\nl0: 0.004595, l1: 0.004728, l2: 0.005201, l3: 0.006273, l4: 0.009245, l5: 0.013223, l6: 0.021157\n\nl0: 0.026534, l1: 0.026242, l2: 0.028641, l3: 0.031000, l4: 0.033623, l5: 0.041150, l6: 0.046101\n\nl0: 0.008340, l1: 0.008224, l2: 0.010113, l3: 0.011398, l4: 0.013696, l5: 0.017400, l6: 0.026817\n\nl0: 0.050760, l1: 0.051230, l2: 0.050825, l3: 0.054009, l4: 0.053939, l5: 0.053388, l6: 0.089450\n\n[epoch:  17/ 50] Validation Loss: 0.269492\nl0: 0.008489, l1: 0.008667, l2: 0.009216, l3: 0.010348, l4: 0.013158, l5: 0.021294, l6: 0.036173\n\n[epoch:  17/ 50, batch:    16/    0, ite: 68] train loss: 0.095341, tar: 0.007467 \nl0: 0.047662, l1: 0.047896, l2: 0.048117, l3: 0.048655, l4: 0.056086, l5: 0.060142, l6: 0.091555\n\nl0: 0.005234, l1: 0.005299, l2: 0.006087, l3: 0.006894, l4: 0.009946, l5: 0.012509, l6: 0.020000\n\nl0: 0.027087, l1: 0.026797, l2: 0.029120, l3: 0.032507, l4: 0.037039, l5: 0.041936, l6: 0.047337\n\nl0: 0.007829, l1: 0.007700, l2: 0.009928, l3: 0.010965, l4: 0.014031, l5: 0.017898, l6: 0.032374\n\nl0: 0.024454, l1: 0.024900, l2: 0.028816, l3: 0.039544, l4: 0.047167, l5: 0.072314, l6: 0.110494\n\n[epoch:  17/ 50] Validation Loss: 0.231264\nl0: 0.007824, l1: 0.007893, l2: 0.008917, l3: 0.010540, l4: 0.013666, l5: 0.018552, l6: 0.031158\n\n[epoch:  18/ 50, batch:     4/    0, ite: 69] train loss: 0.095698, tar: 0.007507 \nl0: 0.052095, l1: 0.052687, l2: 0.051848, l3: 0.052340, l4: 0.048791, l5: 0.049410, l6: 0.069449\n\nl0: 0.004789, l1: 0.004926, l2: 0.005350, l3: 0.006226, l4: 0.008172, l5: 0.012767, l6: 0.019023\n\nl0: 0.023516, l1: 0.023307, l2: 0.025799, l3: 0.028553, l4: 0.033011, l5: 0.041744, l6: 0.055966\n\nl0: 0.007181, l1: 0.007057, l2: 0.009210, l3: 0.010568, l4: 0.012919, l5: 0.016292, l6: 0.029447\n\nl0: 0.100379, l1: 0.100177, l2: 0.099890, l3: 0.100936, l4: 0.095066, l5: 0.103217, l6: 0.129428\n\n[epoch:  18/ 50] Validation Loss: 0.298308\nl0: 0.008900, l1: 0.009041, l2: 0.009528, l3: 0.010478, l4: 0.012919, l5: 0.017229, l6: 0.028072\n\n[epoch:  18/ 50, batch:     8/    0, ite: 70] train loss: 0.095745, tar: 0.007646 \nl0: 0.056495, l1: 0.056976, l2: 0.057634, l3: 0.059069, l4: 0.061971, l5: 0.070686, l6: 0.153623\n\nl0: 0.004574, l1: 0.004697, l2: 0.005065, l3: 0.005923, l4: 0.008033, l5: 0.012336, l6: 0.018596\n\nl0: 0.033259, l1: 0.033032, l2: 0.034602, l3: 0.036481, l4: 0.036682, l5: 0.042771, l6: 0.047829\n\nl0: 0.007818, l1: 0.007748, l2: 0.009844, l3: 0.010821, l4: 0.012582, l5: 0.015976, l6: 0.028721\n\nl0: 0.015987, l1: 0.015815, l2: 0.016755, l3: 0.019536, l4: 0.028153, l5: 0.037703, l6: 0.076973\n\n[epoch:  18/ 50] Validation Loss: 0.228954\nl0: 0.006205, l1: 0.006277, l2: 0.007222, l3: 0.008559, l4: 0.011415, l5: 0.017603, l6: 0.030775\n\n[epoch:  18/ 50, batch:    12/    0, ite: 71] train loss: 0.088058, tar: 0.006205 \nl0: 0.060722, l1: 0.060634, l2: 0.061957, l3: 0.064192, l4: 0.062746, l5: 0.068249, l6: 0.100004\n\nl0: 0.006208, l1: 0.006306, l2: 0.007160, l3: 0.008081, l4: 0.009687, l5: 0.013027, l6: 0.025697\n\nl0: 0.032567, l1: 0.032248, l2: 0.033758, l3: 0.037253, l4: 0.038192, l5: 0.043149, l6: 0.047294\n\nl0: 0.007532, l1: 0.007436, l2: 0.009550, l3: 0.010944, l4: 0.013136, l5: 0.018088, l6: 0.028247\n\nl0: 0.017515, l1: 0.017352, l2: 0.018362, l3: 0.020559, l4: 0.026120, l5: 0.037538, l6: 0.069817\n\n[epoch:  18/ 50] Validation Loss: 0.224265\nl0: 0.006069, l1: 0.006252, l2: 0.006722, l3: 0.007993, l4: 0.011544, l5: 0.018738, l6: 0.033263\n\n[epoch:  18/ 50, batch:    16/    0, ite: 72] train loss: 0.089320, tar: 0.006137 \nl0: 0.075744, l1: 0.075613, l2: 0.075592, l3: 0.077513, l4: 0.072038, l5: 0.066149, l6: 0.088019\n\nl0: 0.004944, l1: 0.005012, l2: 0.005760, l3: 0.006590, l4: 0.008923, l5: 0.011330, l6: 0.018676\n\nl0: 0.026208, l1: 0.025860, l2: 0.028871, l3: 0.032820, l4: 0.037481, l5: 0.040105, l6: 0.049578\n\nl0: 0.008354, l1: 0.008342, l2: 0.010188, l3: 0.011108, l4: 0.012721, l5: 0.016994, l6: 0.025853\n\nl0: 0.014722, l1: 0.014498, l2: 0.015241, l3: 0.018051, l4: 0.024186, l5: 0.038404, l6: 0.095252\n\n[epoch:  18/ 50] Validation Loss: 0.229349\nl0: 0.006006, l1: 0.006118, l2: 0.007031, l3: 0.008490, l4: 0.011711, l5: 0.018619, l6: 0.032202\n\n[epoch:  19/ 50, batch:     4/    0, ite: 73] train loss: 0.089605, tar: 0.006094 \nl0: 0.009851, l1: 0.009884, l2: 0.011124, l3: 0.013711, l4: 0.020872, l5: 0.031490, l6: 0.072761\n\nl0: 0.009734, l1: 0.009859, l2: 0.010004, l3: 0.010731, l4: 0.012898, l5: 0.013686, l6: 0.022408\n\nl0: 0.024060, l1: 0.023835, l2: 0.026198, l3: 0.028603, l4: 0.036076, l5: 0.041025, l6: 0.051543\n\nl0: 0.008331, l1: 0.008185, l2: 0.010647, l3: 0.012666, l4: 0.014491, l5: 0.018375, l6: 0.028170\n\nl0: 0.067503, l1: 0.066128, l2: 0.065760, l3: 0.068577, l4: 0.066648, l5: 0.082666, l6: 0.104513\n\n[epoch:  19/ 50] Validation Loss: 0.222603\nl0: 0.005156, l1: 0.005232, l2: 0.006125, l3: 0.007468, l4: 0.010046, l5: 0.014878, l6: 0.027983\n\n[epoch:  19/ 50, batch:     8/    0, ite: 74] train loss: 0.086426, tar: 0.005859 \nl0: 0.057493, l1: 0.057239, l2: 0.058629, l3: 0.058422, l4: 0.068902, l5: 0.068359, l6: 0.104365\n\nl0: 0.007899, l1: 0.008074, l2: 0.009209, l3: 0.010308, l4: 0.013334, l5: 0.013207, l6: 0.020437\n\nl0: 0.029590, l1: 0.029368, l2: 0.030912, l3: 0.032725, l4: 0.035716, l5: 0.040982, l6: 0.046820\n\nl0: 0.007159, l1: 0.007045, l2: 0.009105, l3: 0.010518, l4: 0.012245, l5: 0.015511, l6: 0.028883\n\nl0: 0.015319, l1: 0.015093, l2: 0.016717, l3: 0.020092, l4: 0.030190, l5: 0.041502, l6: 0.084332\n\n[epoch:  19/ 50] Validation Loss: 0.223140\nl0: 0.006547, l1: 0.006627, l2: 0.007495, l3: 0.008951, l4: 0.011446, l5: 0.017731, l6: 0.035339\n\n[epoch:  19/ 50, batch:    12/    0, ite: 75] train loss: 0.087968, tar: 0.005997 \nl0: 0.013006, l1: 0.013219, l2: 0.016401, l3: 0.019652, l4: 0.033664, l5: 0.057767, l6: 0.078783\n\nl0: 0.005125, l1: 0.005288, l2: 0.005788, l3: 0.006564, l4: 0.008705, l5: 0.011432, l6: 0.018933\n\nl0: 0.029393, l1: 0.028996, l2: 0.031527, l3: 0.034760, l4: 0.038043, l5: 0.042653, l6: 0.048057\n\nl0: 0.008124, l1: 0.007999, l2: 0.010151, l3: 0.011369, l4: 0.013522, l5: 0.018438, l6: 0.027131\n\nl0: 0.019172, l1: 0.019015, l2: 0.021101, l3: 0.024116, l4: 0.040224, l5: 0.065652, l6: 0.125224\n\n[epoch:  19/ 50] Validation Loss: 0.191798\nl0: 0.006408, l1: 0.006489, l2: 0.007201, l3: 0.008557, l4: 0.010986, l5: 0.014333, l6: 0.025736\n\n[epoch:  19/ 50, batch:    16/    0, ite: 76] train loss: 0.086592, tar: 0.006065 \nl0: 0.052898, l1: 0.052906, l2: 0.054909, l3: 0.057289, l4: 0.063690, l5: 0.060553, l6: 0.079435\n\nl0: 0.004777, l1: 0.004830, l2: 0.005471, l3: 0.006505, l4: 0.008739, l5: 0.012508, l6: 0.020651\n\nl0: 0.031218, l1: 0.030959, l2: 0.033058, l3: 0.035876, l4: 0.037328, l5: 0.041522, l6: 0.044986\n\nl0: 0.007339, l1: 0.007182, l2: 0.009188, l3: 0.010649, l4: 0.012448, l5: 0.017984, l6: 0.029411\n\nl0: 0.016512, l1: 0.016363, l2: 0.016727, l3: 0.018955, l4: 0.024309, l5: 0.034736, l6: 0.078165\n\n[epoch:  19/ 50] Validation Loss: 0.208015\nl0: 0.005327, l1: 0.005405, l2: 0.006137, l3: 0.007285, l4: 0.009627, l5: 0.015640, l6: 0.028807\n\n[epoch:  20/ 50, batch:     4/    0, ite: 77] train loss: 0.085397, tar: 0.005960 \nl0: 0.061405, l1: 0.061466, l2: 0.064332, l3: 0.067476, l4: 0.072443, l5: 0.074795, l6: 0.139895\n\nl0: 0.004783, l1: 0.004858, l2: 0.005507, l3: 0.006799, l4: 0.008970, l5: 0.013212, l6: 0.018378\n\nl0: 0.025655, l1: 0.025242, l2: 0.027584, l3: 0.030218, l4: 0.035305, l5: 0.040329, l6: 0.047160\n\nl0: 0.008165, l1: 0.008135, l2: 0.009781, l3: 0.011601, l4: 0.014077, l5: 0.020344, l6: 0.032537\n\nl0: 0.018321, l1: 0.018645, l2: 0.020875, l3: 0.027177, l4: 0.038451, l5: 0.067478, l6: 0.124875\n\n[epoch:  20/ 50] Validation Loss: 0.251255\nl0: 0.006013, l1: 0.006060, l2: 0.006945, l3: 0.008064, l4: 0.010904, l5: 0.016349, l6: 0.028839\n\n[epoch:  20/ 50, batch:     8/    0, ite: 78] train loss: 0.085119, tar: 0.005966 \nl0: 0.055245, l1: 0.054249, l2: 0.053894, l3: 0.053739, l4: 0.064847, l5: 0.070585, l6: 0.097564\n\nl0: 0.005625, l1: 0.005811, l2: 0.006146, l3: 0.007089, l4: 0.008558, l5: 0.013199, l6: 0.025117\n\nl0: 0.025098, l1: 0.024812, l2: 0.028063, l3: 0.030788, l4: 0.035666, l5: 0.041645, l6: 0.046907\n\nl0: 0.007847, l1: 0.007770, l2: 0.009676, l3: 0.011376, l4: 0.014069, l5: 0.020572, l6: 0.032134\n\nl0: 0.053441, l1: 0.054039, l2: 0.053706, l3: 0.055511, l4: 0.054573, l5: 0.055198, l6: 0.067710\n\n[epoch:  20/ 50] Validation Loss: 0.250453\nl0: 0.006307, l1: 0.006375, l2: 0.007127, l3: 0.008605, l4: 0.012239, l5: 0.017407, l6: 0.030504\n\n[epoch:  20/ 50, batch:    12/    0, ite: 79] train loss: 0.085502, tar: 0.006004 \nl0: 0.053122, l1: 0.052760, l2: 0.052093, l3: 0.051337, l4: 0.055530, l5: 0.056408, l6: 0.093893\n\nl0: 0.005606, l1: 0.005646, l2: 0.006763, l3: 0.007736, l4: 0.009978, l5: 0.012173, l6: 0.022700\n\nl0: 0.031214, l1: 0.031109, l2: 0.032256, l3: 0.033637, l4: 0.033993, l5: 0.040059, l6: 0.050129\n\nl0: 0.008489, l1: 0.008373, l2: 0.010531, l3: 0.011959, l4: 0.013634, l5: 0.017542, l6: 0.026721\n\nl0: 0.015908, l1: 0.015722, l2: 0.016544, l3: 0.018240, l4: 0.024963, l5: 0.034633, l6: 0.067023\n\n[epoch:  20/ 50] Validation Loss: 0.205685\nl0: 0.005602, l1: 0.005715, l2: 0.006641, l3: 0.007803, l4: 0.009839, l5: 0.013697, l6: 0.025334\n\n[epoch:  20/ 50, batch:    16/    0, ite: 80] train loss: 0.084415, tar: 0.005964 \nl0: 0.026020, l1: 0.026173, l2: 0.026679, l3: 0.027452, l4: 0.038692, l5: 0.040176, l6: 0.072870\n\nl0: 0.005610, l1: 0.005644, l2: 0.006542, l3: 0.007263, l4: 0.009458, l5: 0.012178, l6: 0.018052\n\nl0: 0.033660, l1: 0.033509, l2: 0.034440, l3: 0.036449, l4: 0.037370, l5: 0.041456, l6: 0.047867\n\nl0: 0.007953, l1: 0.007818, l2: 0.009702, l3: 0.011045, l4: 0.012783, l5: 0.016391, l6: 0.025930\n\nl0: 0.034257, l1: 0.032703, l2: 0.038175, l3: 0.040649, l4: 0.052545, l5: 0.074022, l6: 0.096000\n\n[epoch:  20/ 50] Validation Loss: 0.209507\nl0: 0.007335, l1: 0.007437, l2: 0.008220, l3: 0.009236, l4: 0.011733, l5: 0.017134, l6: 0.029281\n\n[epoch:  21/ 50, batch:     4/    0, ite: 81] train loss: 0.090377, tar: 0.007335 \nl0: 0.045378, l1: 0.045425, l2: 0.046394, l3: 0.048283, l4: 0.049870, l5: 0.059363, l6: 0.077422\n\nl0: 0.006300, l1: 0.006438, l2: 0.006586, l3: 0.007394, l4: 0.009063, l5: 0.014114, l6: 0.024716\n\nl0: 0.029650, l1: 0.029371, l2: 0.031360, l3: 0.034906, l4: 0.037351, l5: 0.042603, l6: 0.045364\n\nl0: 0.007927, l1: 0.007793, l2: 0.009635, l3: 0.011009, l4: 0.012794, l5: 0.019406, l6: 0.029722\n\nl0: 0.017233, l1: 0.017087, l2: 0.018046, l3: 0.020377, l4: 0.029043, l5: 0.056081, l6: 0.091241\n\n[epoch:  21/ 50] Validation Loss: 0.208950\nl0: 0.004834, l1: 0.004925, l2: 0.005694, l3: 0.006811, l4: 0.009110, l5: 0.013870, l6: 0.023793\n\n[epoch:  21/ 50, batch:     8/    0, ite: 82] train loss: 0.079707, tar: 0.006085 \nl0: 0.058491, l1: 0.058522, l2: 0.058977, l3: 0.058303, l4: 0.063132, l5: 0.060661, l6: 0.096008\n\nl0: 0.005690, l1: 0.005678, l2: 0.006873, l3: 0.007817, l4: 0.010248, l5: 0.012801, l6: 0.021409\n\nl0: 0.036348, l1: 0.036195, l2: 0.035901, l3: 0.037549, l4: 0.038749, l5: 0.040334, l6: 0.043688\n\nl0: 0.007743, l1: 0.007695, l2: 0.009681, l3: 0.010925, l4: 0.011998, l5: 0.014859, l6: 0.029012\n\nl0: 0.016510, l1: 0.016354, l2: 0.017313, l3: 0.019103, l4: 0.026053, l5: 0.060361, l6: 0.073791\n\n[epoch:  21/ 50] Validation Loss: 0.222955\nl0: 0.005742, l1: 0.005849, l2: 0.006704, l3: 0.007912, l4: 0.010910, l5: 0.017224, l6: 0.031951\n\n[epoch:  21/ 50, batch:    12/    0, ite: 83] train loss: 0.081902, tar: 0.005970 \nl0: 0.060184, l1: 0.060053, l2: 0.062253, l3: 0.062957, l4: 0.064833, l5: 0.060987, l6: 0.091406\n\nl0: 0.004403, l1: 0.004508, l2: 0.005114, l3: 0.006392, l4: 0.009272, l5: 0.013451, l6: 0.021840\n\nl0: 0.023924, l1: 0.023440, l2: 0.026967, l3: 0.031220, l4: 0.035701, l5: 0.040436, l6: 0.041631\n\nl0: 0.007184, l1: 0.007205, l2: 0.008633, l3: 0.010084, l4: 0.012166, l5: 0.015849, l6: 0.024536\n\nl0: 0.013418, l1: 0.013281, l2: 0.014128, l3: 0.015639, l4: 0.022130, l5: 0.030591, l6: 0.059557\n\n[epoch:  21/ 50] Validation Loss: 0.201074\nl0: 0.009119, l1: 0.009209, l2: 0.010156, l3: 0.011606, l4: 0.013760, l5: 0.018910, l6: 0.033747\n\n[epoch:  21/ 50, batch:    16/    0, ite: 84] train loss: 0.088053, tar: 0.006757 \nl0: 0.014805, l1: 0.015160, l2: 0.020636, l3: 0.021986, l4: 0.035386, l5: 0.054334, l6: 0.086509\n\nl0: 0.005077, l1: 0.005147, l2: 0.005827, l3: 0.006565, l4: 0.009347, l5: 0.011829, l6: 0.019650\n\nl0: 0.031956, l1: 0.031753, l2: 0.033106, l3: 0.034772, l4: 0.038318, l5: 0.041428, l6: 0.050015\n\nl0: 0.008212, l1: 0.008065, l2: 0.010142, l3: 0.011835, l4: 0.014230, l5: 0.019817, l6: 0.028787\n\nl0: 0.017745, l1: 0.017503, l2: 0.021331, l3: 0.027899, l4: 0.046134, l5: 0.072511, l6: 0.101454\n\n[epoch:  21/ 50] Validation Loss: 0.195853\nl0: 0.005318, l1: 0.005385, l2: 0.006289, l3: 0.007520, l4: 0.009892, l5: 0.014732, l6: 0.025979\n\n[epoch:  22/ 50, batch:     4/    0, ite: 85] train loss: 0.085466, tar: 0.006470 \nl0: 0.066447, l1: 0.065891, l2: 0.064713, l3: 0.065133, l4: 0.063639, l5: 0.063624, l6: 0.085348\n\nl0: 0.004977, l1: 0.005029, l2: 0.005623, l3: 0.006501, l4: 0.009227, l5: 0.011674, l6: 0.018995\n\nl0: 0.026207, l1: 0.025817, l2: 0.028816, l3: 0.032076, l4: 0.037605, l5: 0.040965, l6: 0.048096\n\nl0: 0.008163, l1: 0.008105, l2: 0.009818, l3: 0.011122, l4: 0.012698, l5: 0.015103, l6: 0.027249\n\nl0: 0.072994, l1: 0.071879, l2: 0.070754, l3: 0.072693, l4: 0.067452, l5: 0.073498, l6: 0.104253\n\n[epoch:  22/ 50] Validation Loss: 0.280437\nl0: 0.005286, l1: 0.005383, l2: 0.006180, l3: 0.007441, l4: 0.010135, l5: 0.015811, l6: 0.028313\n\n[epoch:  22/ 50, batch:     8/    0, ite: 86] train loss: 0.084313, tar: 0.006272 \nl0: 0.061844, l1: 0.061855, l2: 0.062760, l3: 0.063821, l4: 0.066213, l5: 0.065027, l6: 0.098408\n\nl0: 0.009871, l1: 0.009962, l2: 0.010542, l3: 0.011178, l4: 0.013373, l5: 0.015664, l6: 0.024957\n\nl0: 0.025448, l1: 0.025118, l2: 0.027156, l3: 0.029005, l4: 0.034480, l5: 0.042092, l6: 0.043477\n\nl0: 0.007643, l1: 0.007513, l2: 0.009565, l3: 0.010988, l4: 0.012868, l5: 0.017589, l6: 0.027554\n\nl0: 0.015221, l1: 0.015009, l2: 0.015968, l3: 0.018627, l4: 0.024225, l5: 0.044043, l6: 0.090990\n\n[epoch:  22/ 50] Validation Loss: 0.224011\nl0: 0.006875, l1: 0.006948, l2: 0.007716, l3: 0.009010, l4: 0.011529, l5: 0.017563, l6: 0.029356\n\n[epoch:  22/ 50, batch:    12/    0, ite: 87] train loss: 0.084982, tar: 0.006358 \nl0: 0.052698, l1: 0.052645, l2: 0.053692, l3: 0.055007, l4: 0.059536, l5: 0.059091, l6: 0.073697\n\nl0: 0.005013, l1: 0.005083, l2: 0.005662, l3: 0.006542, l4: 0.009094, l5: 0.011607, l6: 0.019120\n\nl0: 0.025135, l1: 0.024716, l2: 0.027518, l3: 0.031499, l4: 0.037185, l5: 0.039883, l6: 0.046975\n\nl0: 0.007708, l1: 0.007533, l2: 0.009605, l3: 0.011071, l4: 0.013122, l5: 0.017338, l6: 0.029020\n\nl0: 0.025155, l1: 0.024903, l2: 0.026136, l3: 0.030147, l4: 0.035735, l5: 0.054300, l6: 0.087689\n\n[epoch:  22/ 50] Validation Loss: 0.216173\nl0: 0.005444, l1: 0.005514, l2: 0.006446, l3: 0.008141, l4: 0.011509, l5: 0.016673, l6: 0.029450\n\n[epoch:  22/ 50, batch:    16/    0, ite: 88] train loss: 0.084756, tar: 0.006244 \nl0: 0.076807, l1: 0.076687, l2: 0.076601, l3: 0.078826, l4: 0.072526, l5: 0.077538, l6: 0.092655\n\nl0: 0.004668, l1: 0.004755, l2: 0.005380, l3: 0.006307, l4: 0.008705, l5: 0.012934, l6: 0.016783\n\nl0: 0.023589, l1: 0.023373, l2: 0.025885, l3: 0.028263, l4: 0.032787, l5: 0.039916, l6: 0.046690\n\nl0: 0.009360, l1: 0.009338, l2: 0.010823, l3: 0.012002, l4: 0.014339, l5: 0.017507, l6: 0.025949\n\nl0: 0.029864, l1: 0.029289, l2: 0.032817, l3: 0.039620, l4: 0.045579, l5: 0.047852, l6: 0.072941\n\n[epoch:  22/ 50] Validation Loss: 0.245791\nl0: 0.004760, l1: 0.004845, l2: 0.005468, l3: 0.006541, l4: 0.009413, l5: 0.015274, l6: 0.026928\n\n[epoch:  23/ 50, batch:     4/    0, ite: 89] train loss: 0.083476, tar: 0.006079 \nl0: 0.059690, l1: 0.059817, l2: 0.060687, l3: 0.059614, l4: 0.061743, l5: 0.057630, l6: 0.085039\n\nl0: 0.005749, l1: 0.006288, l2: 0.006481, l3: 0.007427, l4: 0.009721, l5: 0.013263, l6: 0.024031\n\nl0: 0.030732, l1: 0.030688, l2: 0.031666, l3: 0.033079, l4: 0.035004, l5: 0.039649, l6: 0.042590\n\nl0: 0.007263, l1: 0.007249, l2: 0.009218, l3: 0.010648, l4: 0.012738, l5: 0.016150, l6: 0.026279\n\nl0: 0.016293, l1: 0.016266, l2: 0.016572, l3: 0.018101, l4: 0.024414, l5: 0.038674, l6: 0.059803\n\n[epoch:  23/ 50] Validation Loss: 0.208052\nl0: 0.005493, l1: 0.005558, l2: 0.006179, l3: 0.007193, l4: 0.009458, l5: 0.015115, l6: 0.028833\n\n[epoch:  23/ 50, batch:     8/    0, ite: 90] train loss: 0.082911, tar: 0.006021 \nl0: 0.057561, l1: 0.056606, l2: 0.058499, l3: 0.058926, l4: 0.067261, l5: 0.081878, l6: 0.095297\n\nl0: 0.006125, l1: 0.006160, l2: 0.007149, l3: 0.008049, l4: 0.010480, l5: 0.013723, l6: 0.021132\n\nl0: 0.031666, l1: 0.031408, l2: 0.032780, l3: 0.034017, l4: 0.035653, l5: 0.042387, l6: 0.047614\n\nl0: 0.007330, l1: 0.007191, l2: 0.009470, l3: 0.011034, l4: 0.013470, l5: 0.016466, l6: 0.026040\n\nl0: 0.014339, l1: 0.014205, l2: 0.014506, l3: 0.016662, l4: 0.023669, l5: 0.039523, l6: 0.096091\n\n[epoch:  23/ 50] Validation Loss: 0.222873\nl0: 0.006034, l1: 0.006085, l2: 0.007076, l3: 0.008622, l4: 0.011334, l5: 0.016070, l6: 0.029387\n\n[epoch:  23/ 50, batch:    12/    0, ite: 91] train loss: 0.084608, tar: 0.006034 \nl0: 0.045862, l1: 0.045773, l2: 0.046892, l3: 0.049642, l4: 0.057512, l5: 0.066372, l6: 0.086086\n\nl0: 0.005516, l1: 0.005532, l2: 0.006353, l3: 0.007291, l4: 0.009369, l5: 0.013609, l6: 0.020077\n\nl0: 0.032845, l1: 0.032683, l2: 0.033257, l3: 0.036537, l4: 0.039390, l5: 0.041495, l6: 0.045260\n\nl0: 0.008593, l1: 0.008439, l2: 0.010488, l3: 0.011873, l4: 0.014000, l5: 0.020169, l6: 0.027417\n\nl0: 0.016721, l1: 0.016317, l2: 0.018698, l3: 0.021693, l4: 0.042054, l5: 0.066238, l6: 0.098166\n\n[epoch:  23/ 50] Validation Loss: 0.221644\nl0: 0.007992, l1: 0.008065, l2: 0.008488, l3: 0.009617, l4: 0.012953, l5: 0.017753, l6: 0.029488\n\n[epoch:  23/ 50, batch:    16/    0, ite: 92] train loss: 0.089482, tar: 0.007013 \nl0: 0.056758, l1: 0.056481, l2: 0.057367, l3: 0.058860, l4: 0.061813, l5: 0.064884, l6: 0.082451\n\nl0: 0.004777, l1: 0.004874, l2: 0.005375, l3: 0.006585, l4: 0.008667, l5: 0.013323, l6: 0.022401\n\nl0: 0.025667, l1: 0.025389, l2: 0.028364, l3: 0.031096, l4: 0.036071, l5: 0.043270, l6: 0.045288\n\nl0: 0.007932, l1: 0.007918, l2: 0.009571, l3: 0.010980, l4: 0.012697, l5: 0.015625, l6: 0.023605\n\nl0: 0.014838, l1: 0.014650, l2: 0.015163, l3: 0.017639, l4: 0.025579, l5: 0.040476, l6: 0.097883\n\n[epoch:  23/ 50] Validation Loss: 0.210863\nl0: 0.005408, l1: 0.005512, l2: 0.006332, l3: 0.007444, l4: 0.009648, l5: 0.014311, l6: 0.029282\n\n[epoch:  24/ 50, batch:     4/    0, ite: 93] train loss: 0.085634, tar: 0.006478 \nl0: 0.062790, l1: 0.062875, l2: 0.062897, l3: 0.061939, l4: 0.060135, l5: 0.055741, l6: 0.075144\n\nl0: 0.005796, l1: 0.005825, l2: 0.006504, l3: 0.007165, l4: 0.009524, l5: 0.011899, l6: 0.020614\n\nl0: 0.032799, l1: 0.032535, l2: 0.034165, l3: 0.036437, l4: 0.037378, l5: 0.041107, l6: 0.047977\n\nl0: 0.007721, l1: 0.007702, l2: 0.009688, l3: 0.010516, l4: 0.012259, l5: 0.015004, l6: 0.028524\n\nl0: 0.072229, l1: 0.071065, l2: 0.071989, l3: 0.074654, l4: 0.078942, l5: 0.084371, l6: 0.094818\n\n[epoch:  24/ 50] Validation Loss: 0.282146\nl0: 0.005585, l1: 0.005657, l2: 0.006516, l3: 0.007829, l4: 0.011078, l5: 0.016212, l6: 0.028588\n\n[epoch:  24/ 50, batch:     8/    0, ite: 94] train loss: 0.084592, tar: 0.006255 \nl0: 0.078361, l1: 0.078408, l2: 0.079727, l3: 0.080718, l4: 0.076722, l5: 0.077167, l6: 0.090017\n\nl0: 0.006482, l1: 0.006673, l2: 0.006773, l3: 0.007493, l4: 0.009098, l5: 0.012866, l6: 0.024238\n\nl0: 0.024152, l1: 0.023991, l2: 0.025804, l3: 0.028168, l4: 0.033616, l5: 0.038964, l6: 0.044895\n\nl0: 0.007330, l1: 0.007175, l2: 0.009356, l3: 0.010916, l4: 0.012469, l5: 0.015580, l6: 0.027073\n\nl0: 0.016087, l1: 0.015989, l2: 0.016511, l3: 0.018234, l4: 0.024076, l5: 0.036022, l6: 0.059280\n\n[epoch:  24/ 50] Validation Loss: 0.226086\nl0: 0.006633, l1: 0.006750, l2: 0.007483, l3: 0.008865, l4: 0.011859, l5: 0.018280, l6: 0.034382\n\n[epoch:  24/ 50, batch:    12/    0, ite: 95] train loss: 0.086523, tar: 0.006330 \nl0: 0.026134, l1: 0.025509, l2: 0.026910, l3: 0.028777, l4: 0.032243, l5: 0.045534, l6: 0.082410\n\nl0: 0.005042, l1: 0.005093, l2: 0.005734, l3: 0.006969, l4: 0.009071, l5: 0.012602, l6: 0.018238\n\nl0: 0.026237, l1: 0.025840, l2: 0.028144, l3: 0.030899, l4: 0.035895, l5: 0.041051, l6: 0.042990\n\nl0: 0.008022, l1: 0.007859, l2: 0.009918, l3: 0.011271, l4: 0.013554, l5: 0.018222, l6: 0.022979\n\nl0: 0.014925, l1: 0.014779, l2: 0.014790, l3: 0.017625, l4: 0.024181, l5: 0.039163, l6: 0.105064\n\n[epoch:  24/ 50] Validation Loss: 0.176734\nl0: 0.006840, l1: 0.006883, l2: 0.007602, l3: 0.008787, l4: 0.012281, l5: 0.016505, l6: 0.027382\n\n[epoch:  24/ 50, batch:    16/    0, ite: 96] train loss: 0.086483, tar: 0.006415 \nl0: 0.012775, l1: 0.012333, l2: 0.016505, l3: 0.018115, l4: 0.027149, l5: 0.049817, l6: 0.082794\n\nl0: 0.005731, l1: 0.005762, l2: 0.006688, l3: 0.007670, l4: 0.009573, l5: 0.012243, l6: 0.022814\n\nl0: 0.028842, l1: 0.028569, l2: 0.030112, l3: 0.031680, l4: 0.033783, l5: 0.041070, l6: 0.045385\n\nl0: 0.007546, l1: 0.007389, l2: 0.009581, l3: 0.010973, l4: 0.012759, l5: 0.015893, l6: 0.025335\n\nl0: 0.044201, l1: 0.042596, l2: 0.046042, l3: 0.051212, l4: 0.051936, l5: 0.064405, l6: 0.093984\n\n[epoch:  24/ 50] Validation Loss: 0.202652\nl0: 0.005778, l1: 0.005875, l2: 0.006866, l3: 0.008251, l4: 0.010722, l5: 0.015610, l6: 0.026219\n\n[epoch:  25/ 50, batch:     4/    0, ite: 97] train loss: 0.085459, tar: 0.006324 \nl0: 0.056623, l1: 0.056986, l2: 0.059085, l3: 0.061146, l4: 0.063411, l5: 0.059293, l6: 0.079523\n\nl0: 0.005489, l1: 0.005515, l2: 0.006435, l3: 0.007164, l4: 0.009824, l5: 0.013113, l6: 0.021276\n\nl0: 0.037829, l1: 0.037888, l2: 0.036946, l3: 0.039009, l4: 0.039260, l5: 0.041775, l6: 0.047298\n\nl0: 0.006871, l1: 0.006825, l2: 0.008970, l3: 0.010153, l4: 0.012198, l5: 0.014157, l6: 0.029285\n\nl0: 0.080502, l1: 0.079765, l2: 0.080603, l3: 0.083596, l4: 0.080639, l5: 0.089669, l6: 0.095104\n\n[epoch:  25/ 50] Validation Loss: 0.292645\nl0: 0.004502, l1: 0.004589, l2: 0.005231, l3: 0.006160, l4: 0.008494, l5: 0.012699, l6: 0.021131\n\n[epoch:  25/ 50, batch:     8/    0, ite: 98] train loss: 0.082628, tar: 0.006097 \nl0: 0.075674, l1: 0.075523, l2: 0.075140, l3: 0.076060, l4: 0.072633, l5: 0.077957, l6: 0.094698\n\nl0: 0.004833, l1: 0.004863, l2: 0.005636, l3: 0.007034, l4: 0.008837, l5: 0.013181, l6: 0.018096\n\nl0: 0.025660, l1: 0.025278, l2: 0.027762, l3: 0.030951, l4: 0.034524, l5: 0.041516, l6: 0.043533\n\nl0: 0.007452, l1: 0.007417, l2: 0.009556, l3: 0.010557, l4: 0.012236, l5: 0.016301, l6: 0.026440\n\nl0: 0.018992, l1: 0.018782, l2: 0.022207, l3: 0.027049, l4: 0.050348, l5: 0.070132, l6: 0.106347\n\n[epoch:  25/ 50] Validation Loss: 0.248641\nl0: 0.006074, l1: 0.006121, l2: 0.006963, l3: 0.008258, l4: 0.011187, l5: 0.016649, l6: 0.027791\n\n[epoch:  25/ 50, batch:    12/    0, ite: 99] train loss: 0.082674, tar: 0.006094 \nl0: 0.027250, l1: 0.027249, l2: 0.028323, l3: 0.030727, l4: 0.034029, l5: 0.051137, l6: 0.087479\n\nl0: 0.005519, l1: 0.005559, l2: 0.006083, l3: 0.006838, l4: 0.008873, l5: 0.012321, l6: 0.016750\n\nl0: 0.025018, l1: 0.024747, l2: 0.027144, l3: 0.030987, l4: 0.035035, l5: 0.041043, l6: 0.046462\n\nl0: 0.007320, l1: 0.007250, l2: 0.009197, l3: 0.010089, l4: 0.011867, l5: 0.014294, l6: 0.027758\n\nl0: 0.013085, l1: 0.012822, l2: 0.013752, l3: 0.016328, l4: 0.022794, l5: 0.031874, l6: 0.056540\n\n[epoch:  25/ 50] Validation Loss: 0.166709\nl0: 0.006667, l1: 0.006764, l2: 0.007511, l3: 0.008480, l4: 0.011249, l5: 0.016607, l6: 0.029115\n\n[epoch:  25/ 50, batch:    16/    0, ite: 100] train loss: 0.083046, tar: 0.006151 \nl0: 0.044209, l1: 0.044264, l2: 0.046348, l3: 0.049402, l4: 0.050094, l5: 0.069074, l6: 0.085585\n\nl0: 0.004949, l1: 0.004997, l2: 0.005776, l3: 0.007209, l4: 0.008934, l5: 0.012995, l6: 0.018804\n\nl0: 0.024922, l1: 0.024706, l2: 0.026315, l3: 0.028933, l4: 0.033925, l5: 0.042327, l6: 0.044136\n\nl0: 0.006276, l1: 0.006321, l2: 0.007952, l3: 0.009414, l4: 0.011002, l5: 0.013549, l6: 0.024811\n\nl0: 0.015406, l1: 0.015326, l2: 0.019881, l3: 0.022641, l4: 0.046384, l5: 0.070723, l6: 0.097069\n\n[epoch:  25/ 50] Validation Loss: 0.208932\nl0: 0.005987, l1: 0.006078, l2: 0.006651, l3: 0.007744, l4: 0.010214, l5: 0.013756, l6: 0.027619\n\n[epoch:  26/ 50, batch:     4/    0, ite: 101] train loss: 0.078048, tar: 0.005987 \nl0: 0.047803, l1: 0.048902, l2: 0.047904, l3: 0.049273, l4: 0.053660, l5: 0.054251, l6: 0.087552\n\nl0: 0.005237, l1: 0.005279, l2: 0.005908, l3: 0.007076, l4: 0.009021, l5: 0.012033, l6: 0.021100\n\nl0: 0.034700, l1: 0.034551, l2: 0.035216, l3: 0.036778, l4: 0.037305, l5: 0.042071, l6: 0.057322\n\nl0: 0.007332, l1: 0.007296, l2: 0.009166, l3: 0.010306, l4: 0.011997, l5: 0.014596, l6: 0.026251\n\nl0: 0.019553, l1: 0.019229, l2: 0.023214, l3: 0.029748, l4: 0.042542, l5: 0.056537, l6: 0.083022\n\n[epoch:  26/ 50] Validation Loss: 0.218746\nl0: 0.006346, l1: 0.006400, l2: 0.007284, l3: 0.008645, l4: 0.011071, l5: 0.016852, l6: 0.030020\n\n[epoch:  26/ 50, batch:     8/    0, ite: 102] train loss: 0.082333, tar: 0.006166 \nl0: 0.046787, l1: 0.047811, l2: 0.047771, l3: 0.049435, l4: 0.056373, l5: 0.057916, l6: 0.090348\n\nl0: 0.004616, l1: 0.004700, l2: 0.005095, l3: 0.006310, l4: 0.008353, l5: 0.012981, l6: 0.019752\n\nl0: 0.033257, l1: 0.033122, l2: 0.034247, l3: 0.036043, l4: 0.034960, l5: 0.042160, l6: 0.054275\n\nl0: 0.007510, l1: 0.007411, l2: 0.009489, l3: 0.011204, l4: 0.013863, l5: 0.017938, l6: 0.028033\n\nl0: 0.014765, l1: 0.014592, l2: 0.015184, l3: 0.016985, l4: 0.021127, l5: 0.036866, l6: 0.110726\n\n[epoch:  26/ 50] Validation Loss: 0.210401\nl0: 0.005129, l1: 0.005213, l2: 0.006058, l3: 0.007389, l4: 0.009905, l5: 0.015477, l6: 0.028091\n\n[epoch:  26/ 50, batch:    12/    0, ite: 103] train loss: 0.080643, tar: 0.005821 \nl0: 0.010117, l1: 0.010121, l2: 0.011058, l3: 0.013552, l4: 0.017025, l5: 0.029440, l6: 0.081738\n\nl0: 0.005823, l1: 0.005922, l2: 0.006745, l3: 0.007356, l4: 0.009001, l5: 0.012971, l6: 0.024268\n\nl0: 0.034101, l1: 0.033894, l2: 0.035104, l3: 0.037419, l4: 0.037583, l5: 0.041564, l6: 0.047834\n\nl0: 0.007705, l1: 0.007656, l2: 0.009656, l3: 0.011222, l4: 0.013100, l5: 0.016792, l6: 0.028467\n\nl0: 0.012414, l1: 0.012211, l2: 0.012831, l3: 0.015506, l4: 0.021451, l5: 0.037638, l6: 0.103029\n\n[epoch:  26/ 50] Validation Loss: 0.164463\nl0: 0.006150, l1: 0.006238, l2: 0.007016, l3: 0.008329, l4: 0.011299, l5: 0.016707, l6: 0.026735\n\n[epoch:  26/ 50, batch:    16/    0, ite: 104] train loss: 0.081101, tar: 0.005903 \nl0: 0.050825, l1: 0.051552, l2: 0.050916, l3: 0.055769, l4: 0.051518, l5: 0.052893, l6: 0.085228\n\nl0: 0.005068, l1: 0.005132, l2: 0.005637, l3: 0.006415, l4: 0.009631, l5: 0.012410, l6: 0.017205\n\nl0: 0.033094, l1: 0.032877, l2: 0.033735, l3: 0.035094, l4: 0.036234, l5: 0.042354, l6: 0.047297\n\nl0: 0.007380, l1: 0.007270, l2: 0.009390, l3: 0.010824, l4: 0.012268, l5: 0.014613, l6: 0.027111\n\nl0: 0.076654, l1: 0.075822, l2: 0.076179, l3: 0.078995, l4: 0.079276, l5: 0.081029, l6: 0.102353\n\n[epoch:  26/ 50] Validation Loss: 0.276010\nl0: 0.005315, l1: 0.005391, l2: 0.006662, l3: 0.008088, l4: 0.010929, l5: 0.015289, l6: 0.031683\n\n[epoch:  27/ 50, batch:     4/    0, ite: 105] train loss: 0.081552, tar: 0.005785 \nl0: 0.054626, l1: 0.055764, l2: 0.056742, l3: 0.055018, l4: 0.048482, l5: 0.072336, l6: 0.173962\n\nl0: 0.004713, l1: 0.004792, l2: 0.005355, l3: 0.006511, l4: 0.008580, l5: 0.012295, l6: 0.021048\n\nl0: 0.031972, l1: 0.031652, l2: 0.032337, l3: 0.035906, l4: 0.037976, l5: 0.041396, l6: 0.043555\n\nl0: 0.008014, l1: 0.007838, l2: 0.010018, l3: 0.011243, l4: 0.013545, l5: 0.020320, l6: 0.028959\n\nl0: 0.018797, l1: 0.018536, l2: 0.018836, l3: 0.021184, l4: 0.029186, l5: 0.053709, l6: 0.125212\n\n[epoch:  27/ 50] Validation Loss: 0.244083\nl0: 0.004852, l1: 0.004942, l2: 0.005594, l3: 0.006885, l4: 0.010142, l5: 0.014731, l6: 0.028321\n\n[epoch:  27/ 50, batch:     8/    0, ite: 106] train loss: 0.080538, tar: 0.005630 \nl0: 0.073726, l1: 0.073614, l2: 0.072158, l3: 0.071857, l4: 0.068027, l5: 0.067409, l6: 0.084839\n\nl0: 0.005800, l1: 0.005833, l2: 0.006345, l3: 0.007099, l4: 0.008896, l5: 0.011473, l6: 0.019957\n\nl0: 0.034858, l1: 0.034815, l2: 0.035374, l3: 0.037730, l4: 0.037060, l5: 0.041287, l6: 0.046591\n\nl0: 0.007720, l1: 0.007600, l2: 0.009307, l3: 0.010297, l4: 0.012077, l5: 0.015997, l6: 0.022910\n\nl0: 0.016718, l1: 0.016403, l2: 0.017507, l3: 0.019539, l4: 0.032642, l5: 0.059019, l6: 0.094616\n\n[epoch:  27/ 50] Validation Loss: 0.237420\nl0: 0.006183, l1: 0.006231, l2: 0.007021, l3: 0.007744, l4: 0.010324, l5: 0.015459, l6: 0.025753\n\n[epoch:  27/ 50, batch:    12/    0, ite: 107] train loss: 0.080277, tar: 0.005709 \nl0: 0.074421, l1: 0.074505, l2: 0.074527, l3: 0.075618, l4: 0.071800, l5: 0.069034, l6: 0.080158\n\nl0: 0.005336, l1: 0.005323, l2: 0.006442, l3: 0.007264, l4: 0.009282, l5: 0.012699, l6: 0.021116\n\nl0: 0.032662, l1: 0.032273, l2: 0.033498, l3: 0.035343, l4: 0.037379, l5: 0.043900, l6: 0.049026\n\nl0: 0.007658, l1: 0.007743, l2: 0.009236, l3: 0.010441, l4: 0.011894, l5: 0.015685, l6: 0.025247\n\nl0: 0.045873, l1: 0.046493, l2: 0.046752, l3: 0.049566, l4: 0.049285, l5: 0.047197, l6: 0.071517\n\n[epoch:  27/ 50] Validation Loss: 0.259239\nl0: 0.005542, l1: 0.005648, l2: 0.006222, l3: 0.007106, l4: 0.008821, l5: 0.013178, l6: 0.020824\n\n[epoch:  27/ 50, batch:    16/    0, ite: 108] train loss: 0.078660, tar: 0.005688 \nl0: 0.051651, l1: 0.051589, l2: 0.052473, l3: 0.054284, l4: 0.057478, l5: 0.057671, l6: 0.078907\n\nl0: 0.007210, l1: 0.007488, l2: 0.008305, l3: 0.008579, l4: 0.011591, l5: 0.013457, l6: 0.020730\n\nl0: 0.032238, l1: 0.032047, l2: 0.033873, l3: 0.035782, l4: 0.035477, l5: 0.042085, l6: 0.049184\n\nl0: 0.007148, l1: 0.007021, l2: 0.009022, l3: 0.010522, l4: 0.012712, l5: 0.017644, l6: 0.027789\n\nl0: 0.013177, l1: 0.013056, l2: 0.013345, l3: 0.015186, l4: 0.020426, l5: 0.028038, l6: 0.062873\n\n[epoch:  27/ 50] Validation Loss: 0.200012\nl0: 0.005753, l1: 0.005821, l2: 0.006478, l3: 0.007467, l4: 0.010096, l5: 0.014827, l6: 0.024982\n\n[epoch:  28/ 50, batch:     4/    0, ite: 109] train loss: 0.078300, tar: 0.005695 \nl0: 0.056399, l1: 0.056091, l2: 0.055308, l3: 0.055125, l4: 0.056055, l5: 0.062344, l6: 0.090033\n\nl0: 0.005530, l1: 0.005634, l2: 0.006016, l3: 0.006916, l4: 0.008476, l5: 0.013034, l6: 0.022182\n\nl0: 0.029548, l1: 0.029090, l2: 0.030909, l3: 0.033438, l4: 0.036520, l5: 0.041407, l6: 0.051567\n\nl0: 0.006626, l1: 0.006620, l2: 0.008472, l3: 0.009700, l4: 0.010546, l5: 0.012474, l6: 0.024222\n\nl0: 0.016429, l1: 0.016035, l2: 0.017459, l3: 0.020114, l4: 0.030614, l5: 0.055733, l6: 0.091511\n\n[epoch:  28/ 50] Validation Loss: 0.215635\nl0: 0.006055, l1: 0.006118, l2: 0.006919, l3: 0.008356, l4: 0.010930, l5: 0.015628, l6: 0.030036\n\n[epoch:  28/ 50, batch:     8/    0, ite: 110] train loss: 0.078875, tar: 0.005731 \nl0: 0.059443, l1: 0.059439, l2: 0.061001, l3: 0.060264, l4: 0.065075, l5: 0.059529, l6: 0.092958\n\nl0: 0.006733, l1: 0.006828, l2: 0.007297, l3: 0.007684, l4: 0.010122, l5: 0.013707, l6: 0.023723\n\nl0: 0.032596, l1: 0.032400, l2: 0.033925, l3: 0.036159, l4: 0.036241, l5: 0.041161, l6: 0.042585\n\nl0: 0.008191, l1: 0.008165, l2: 0.009935, l3: 0.010878, l4: 0.012467, l5: 0.016711, l6: 0.027794\n\nl0: 0.080698, l1: 0.079274, l2: 0.079985, l3: 0.081019, l4: 0.076036, l5: 0.082872, l6: 0.096430\n\n[epoch:  28/ 50] Validation Loss: 0.291865\nl0: 0.005674, l1: 0.005737, l2: 0.006305, l3: 0.007187, l4: 0.009354, l5: 0.013663, l6: 0.023814\n\n[epoch:  28/ 50, batch:    12/    0, ite: 111] train loss: 0.071733, tar: 0.005674 \nl0: 0.054847, l1: 0.054188, l2: 0.052795, l3: 0.051685, l4: 0.054831, l5: 0.060530, l6: 0.081963\n\nl0: 0.004490, l1: 0.004567, l2: 0.005158, l3: 0.006390, l4: 0.008299, l5: 0.013429, l6: 0.022007\n\nl0: 0.037056, l1: 0.036969, l2: 0.038236, l3: 0.040711, l4: 0.040664, l5: 0.044570, l6: 0.051359\n\nl0: 0.006604, l1: 0.006581, l2: 0.008488, l3: 0.009650, l4: 0.011665, l5: 0.015903, l6: 0.029127\n\nl0: 0.012267, l1: 0.012109, l2: 0.012806, l3: 0.014470, l4: 0.020702, l5: 0.029028, l6: 0.055457\n\n[epoch:  28/ 50] Validation Loss: 0.201920\nl0: 0.005911, l1: 0.005938, l2: 0.007182, l3: 0.008691, l4: 0.011247, l5: 0.015775, l6: 0.025108\n\n[epoch:  28/ 50, batch:    16/    0, ite: 112] train loss: 0.075793, tar: 0.005792 \nl0: 0.052301, l1: 0.052073, l2: 0.051280, l3: 0.050822, l4: 0.056129, l5: 0.056609, l6: 0.069848\n\nl0: 0.005482, l1: 0.005516, l2: 0.006218, l3: 0.007163, l4: 0.008917, l5: 0.011301, l6: 0.019895\n\nl0: 0.025510, l1: 0.025023, l2: 0.027782, l3: 0.031809, l4: 0.035248, l5: 0.042308, l6: 0.051596\n\nl0: 0.006951, l1: 0.006839, l2: 0.009185, l3: 0.010645, l4: 0.012715, l5: 0.016333, l6: 0.027842\n\nl0: 0.012990, l1: 0.012828, l2: 0.013255, l3: 0.015209, l4: 0.020501, l5: 0.029503, l6: 0.079933\n\n[epoch:  28/ 50] Validation Loss: 0.193512\nl0: 0.006005, l1: 0.006066, l2: 0.006930, l3: 0.007902, l4: 0.009994, l5: 0.014965, l6: 0.029048\n\n[epoch:  29/ 50, batch:     4/    0, ite: 113] train loss: 0.077498, tar: 0.005863 \nl0: 0.057538, l1: 0.057703, l2: 0.058615, l3: 0.058941, l4: 0.046358, l5: 0.057056, l6: 0.169970\n\nl0: 0.004446, l1: 0.004561, l2: 0.004824, l3: 0.005655, l4: 0.007695, l5: 0.010789, l6: 0.016479\n\nl0: 0.030370, l1: 0.030141, l2: 0.031911, l3: 0.036780, l4: 0.036763, l5: 0.041669, l6: 0.042237\n\nl0: 0.007997, l1: 0.007820, l2: 0.009957, l3: 0.010825, l4: 0.013196, l5: 0.019626, l6: 0.025543\n\nl0: 0.028257, l1: 0.027714, l2: 0.028944, l3: 0.034743, l4: 0.038169, l5: 0.042415, l6: 0.084794\n\n[epoch:  29/ 50] Validation Loss: 0.238100\nl0: 0.004999, l1: 0.005057, l2: 0.005938, l3: 0.007332, l4: 0.010035, l5: 0.014732, l6: 0.028504\n\n[epoch:  29/ 50, batch:     8/    0, ite: 114] train loss: 0.077273, tar: 0.005647 \nl0: 0.040228, l1: 0.040404, l2: 0.042921, l3: 0.045218, l4: 0.054716, l5: 0.056747, l6: 0.078127\n\nl0: 0.005311, l1: 0.005357, l2: 0.005965, l3: 0.007048, l4: 0.008940, l5: 0.012676, l6: 0.020833\n\nl0: 0.022972, l1: 0.022537, l2: 0.025950, l3: 0.029690, l4: 0.033800, l5: 0.041065, l6: 0.045381\n\nl0: 0.007806, l1: 0.007778, l2: 0.009360, l3: 0.010741, l4: 0.012810, l5: 0.015581, l6: 0.024065\n\nl0: 0.014283, l1: 0.014190, l2: 0.014430, l3: 0.016304, l4: 0.022475, l5: 0.032331, l6: 0.053511\n\n[epoch:  29/ 50] Validation Loss: 0.180310\nl0: 0.005836, l1: 0.005967, l2: 0.006113, l3: 0.007219, l4: 0.010250, l5: 0.014660, l6: 0.026456\n\n[epoch:  29/ 50, batch:    12/    0, ite: 115] train loss: 0.077119, tar: 0.005685 \nl0: 0.064949, l1: 0.065418, l2: 0.065563, l3: 0.067606, l4: 0.066005, l5: 0.060455, l6: 0.081009\n\nl0: 0.005541, l1: 0.005586, l2: 0.006234, l3: 0.007201, l4: 0.008888, l5: 0.013027, l6: 0.019824\n\nl0: 0.035005, l1: 0.034929, l2: 0.035083, l3: 0.037549, l4: 0.036966, l5: 0.041175, l6: 0.049985\n\nl0: 0.007600, l1: 0.007379, l2: 0.009478, l3: 0.010653, l4: 0.012697, l5: 0.017719, l6: 0.025700\n\nl0: 0.016592, l1: 0.016258, l2: 0.017453, l3: 0.019582, l4: 0.028702, l5: 0.055976, l6: 0.089287\n\n[epoch:  29/ 50] Validation Loss: 0.228615\nl0: 0.004824, l1: 0.004899, l2: 0.005818, l3: 0.006998, l4: 0.009343, l5: 0.013925, l6: 0.031409\n\n[epoch:  29/ 50, batch:    16/    0, ite: 116] train loss: 0.077135, tar: 0.005541 \nl0: 0.073592, l1: 0.073509, l2: 0.072444, l3: 0.073346, l4: 0.064392, l5: 0.065849, l6: 0.080351\n\nl0: 0.004843, l1: 0.004909, l2: 0.005323, l3: 0.006028, l4: 0.009086, l5: 0.011308, l6: 0.016356\n\nl0: 0.022631, l1: 0.022399, l2: 0.024871, l3: 0.027767, l4: 0.032940, l5: 0.038193, l6: 0.042502\n\nl0: 0.006893, l1: 0.006762, l2: 0.008660, l3: 0.010035, l4: 0.011793, l5: 0.015405, l6: 0.028686\n\nl0: 0.064300, l1: 0.062450, l2: 0.062382, l3: 0.063124, l4: 0.060276, l5: 0.067388, l6: 0.093702\n\n[epoch:  29/ 50] Validation Loss: 0.266899\nl0: 0.005522, l1: 0.005580, l2: 0.006525, l3: 0.007613, l4: 0.010391, l5: 0.015437, l6: 0.033364\n\n[epoch:  30/ 50, batch:     4/    0, ite: 117] train loss: 0.078177, tar: 0.005538 \nl0: 0.075861, l1: 0.075926, l2: 0.074836, l3: 0.075635, l4: 0.070412, l5: 0.064266, l6: 0.085479\n\nl0: 0.004362, l1: 0.004437, l2: 0.004864, l3: 0.005957, l4: 0.007772, l5: 0.010785, l6: 0.015693\n\nl0: 0.030826, l1: 0.030563, l2: 0.032314, l3: 0.034413, l4: 0.036780, l5: 0.040581, l6: 0.048064\n\nl0: 0.006941, l1: 0.006763, l2: 0.009003, l3: 0.010476, l4: 0.012598, l5: 0.015847, l6: 0.028755\n\nl0: 0.016016, l1: 0.015739, l2: 0.016297, l3: 0.019017, l4: 0.025466, l5: 0.030570, l6: 0.082421\n\n[epoch:  30/ 50] Validation Loss: 0.225147\nl0: 0.005029, l1: 0.005163, l2: 0.005675, l3: 0.007025, l4: 0.010119, l5: 0.016359, l6: 0.030175\n\n[epoch:  30/ 50, batch:     8/    0, ite: 118] train loss: 0.078348, tar: 0.005475 \nl0: 0.057543, l1: 0.057804, l2: 0.057911, l3: 0.057951, l4: 0.057929, l5: 0.067457, l6: 0.163473\n\nl0: 0.004582, l1: 0.004687, l2: 0.005038, l3: 0.005976, l4: 0.007613, l5: 0.010816, l6: 0.015770\n\nl0: 0.024872, l1: 0.024323, l2: 0.026693, l3: 0.030739, l4: 0.034907, l5: 0.040189, l6: 0.044861\n\nl0: 0.007842, l1: 0.007786, l2: 0.009499, l3: 0.010781, l4: 0.011587, l5: 0.013703, l6: 0.024606\n\nl0: 0.015509, l1: 0.015458, l2: 0.015409, l3: 0.017331, l4: 0.022877, l5: 0.044837, l6: 0.068200\n\n[epoch:  30/ 50] Validation Loss: 0.217311\nl0: 0.005906, l1: 0.005927, l2: 0.006871, l3: 0.007996, l4: 0.010540, l5: 0.015721, l6: 0.028333\n\n[epoch:  30/ 50, batch:    12/    0, ite: 119] train loss: 0.078675, tar: 0.005523 \nl0: 0.009127, l1: 0.009118, l2: 0.010229, l3: 0.012481, l4: 0.016584, l5: 0.028498, l6: 0.076670\n\nl0: 0.005151, l1: 0.005232, l2: 0.005641, l3: 0.006340, l4: 0.007974, l5: 0.011652, l6: 0.017888\n\nl0: 0.036359, l1: 0.036334, l2: 0.036828, l3: 0.039041, l4: 0.039486, l5: 0.045436, l6: 0.056683\n\nl0: 0.008551, l1: 0.008567, l2: 0.009953, l3: 0.011407, l4: 0.012620, l5: 0.016450, l6: 0.027848\n\nl0: 0.015123, l1: 0.015062, l2: 0.015228, l3: 0.017138, l4: 0.022222, l5: 0.031013, l6: 0.059313\n\n[epoch:  30/ 50] Validation Loss: 0.156650\nl0: 0.004586, l1: 0.004653, l2: 0.005619, l3: 0.006880, l4: 0.009672, l5: 0.016041, l6: 0.028712\n\n[epoch:  30/ 50, batch:    16/    0, ite: 120] train loss: 0.078424, tar: 0.005429 \nl0: 0.013143, l1: 0.012895, l2: 0.017399, l3: 0.017752, l4: 0.025752, l5: 0.039654, l6: 0.066915\n\nl0: 0.005903, l1: 0.006072, l2: 0.006465, l3: 0.007130, l4: 0.008759, l5: 0.012590, l6: 0.023762\n\nl0: 0.040838, l1: 0.040961, l2: 0.041041, l3: 0.041204, l4: 0.039507, l5: 0.041709, l6: 0.049731\n\nl0: 0.007107, l1: 0.006951, l2: 0.009143, l3: 0.010468, l4: 0.012100, l5: 0.017225, l6: 0.026673\n\nl0: 0.020578, l1: 0.020397, l2: 0.021373, l3: 0.023763, l4: 0.028893, l5: 0.037596, l6: 0.058882\n\n[epoch:  30/ 50] Validation Loss: 0.172067\nl0: 0.006413, l1: 0.006496, l2: 0.006999, l3: 0.007878, l4: 0.010340, l5: 0.016130, l6: 0.029612\n\n[epoch:  31/ 50, batch:     4/    0, ite: 121] train loss: 0.083867, tar: 0.006413 \nl0: 0.063321, l1: 0.063166, l2: 0.064756, l3: 0.065884, l4: 0.065109, l5: 0.061317, l6: 0.085703\n\nl0: 0.004932, l1: 0.005047, l2: 0.005508, l3: 0.006223, l4: 0.007849, l5: 0.011714, l6: 0.020425\n\nl0: 0.038340, l1: 0.038288, l2: 0.038434, l3: 0.040278, l4: 0.039042, l5: 0.042435, l6: 0.046282\n\nl0: 0.006709, l1: 0.006490, l2: 0.008712, l3: 0.010245, l4: 0.012381, l5: 0.017178, l6: 0.029038\n\nl0: 0.017337, l1: 0.017350, l2: 0.017118, l3: 0.018761, l4: 0.022660, l5: 0.033123, l6: 0.048194\n\n[epoch:  31/ 50] Validation Loss: 0.215870\nl0: 0.005373, l1: 0.005451, l2: 0.006285, l3: 0.007553, l4: 0.010712, l5: 0.015771, l6: 0.030996\n\n[epoch:  31/ 50, batch:     8/    0, ite: 122] train loss: 0.083004, tar: 0.005893 \nl0: 0.008396, l1: 0.008251, l2: 0.009509, l3: 0.011409, l4: 0.019252, l5: 0.044299, l6: 0.070558\n\nl0: 0.004413, l1: 0.004532, l2: 0.004808, l3: 0.005726, l4: 0.007824, l5: 0.011304, l6: 0.020307\n\nl0: 0.025582, l1: 0.025145, l2: 0.027545, l3: 0.030926, l4: 0.036029, l5: 0.040686, l6: 0.042893\n\nl0: 0.006891, l1: 0.006919, l2: 0.008639, l3: 0.009968, l4: 0.011849, l5: 0.013964, l6: 0.026975\n\nl0: 0.016173, l1: 0.015866, l2: 0.016744, l3: 0.019229, l4: 0.025466, l5: 0.048011, l6: 0.063239\n\n[epoch:  31/ 50] Validation Loss: 0.149866\nl0: 0.004109, l1: 0.004195, l2: 0.005094, l3: 0.006392, l4: 0.008677, l5: 0.012905, l6: 0.027874\n\n[epoch:  31/ 50, batch:    12/    0, ite: 123] train loss: 0.078418, tar: 0.005298 \nl0: 0.062346, l1: 0.062271, l2: 0.063306, l3: 0.063710, l4: 0.062612, l5: 0.058159, l6: 0.085938\n\nl0: 0.005687, l1: 0.005712, l2: 0.006546, l3: 0.007536, l4: 0.009991, l5: 0.013317, l6: 0.020558\n\nl0: 0.031216, l1: 0.030936, l2: 0.033248, l3: 0.036089, l4: 0.036998, l5: 0.041115, l6: 0.050453\n\nl0: 0.006800, l1: 0.006653, l2: 0.008752, l3: 0.010221, l4: 0.012647, l5: 0.015614, l6: 0.028641\n\nl0: 0.025866, l1: 0.025106, l2: 0.027058, l3: 0.031880, l4: 0.038845, l5: 0.052967, l6: 0.090028\n\n[epoch:  31/ 50] Validation Loss: 0.233765\nl0: 0.005652, l1: 0.005717, l2: 0.006333, l3: 0.007326, l4: 0.009287, l5: 0.014189, l6: 0.026664\n\n[epoch:  31/ 50, batch:    16/    0, ite: 124] train loss: 0.077606, tar: 0.005387 \nl0: 0.061487, l1: 0.061587, l2: 0.060514, l3: 0.061837, l4: 0.061414, l5: 0.053616, l6: 0.081330\n\nl0: 0.005913, l1: 0.005931, l2: 0.006471, l3: 0.007246, l4: 0.008827, l5: 0.011671, l6: 0.016872\n\nl0: 0.031570, l1: 0.031470, l2: 0.032993, l3: 0.034664, l4: 0.034000, l5: 0.040669, l6: 0.050660\n\nl0: 0.007863, l1: 0.007684, l2: 0.009547, l3: 0.011294, l4: 0.014001, l5: 0.018645, l6: 0.026701\n\nl0: 0.015195, l1: 0.014958, l2: 0.015391, l3: 0.017909, l4: 0.024724, l5: 0.030024, l6: 0.087211\n\n[epoch:  31/ 50] Validation Loss: 0.212377\nl0: 0.004190, l1: 0.004233, l2: 0.005184, l3: 0.006429, l4: 0.009315, l5: 0.013799, l6: 0.027274\n\n[epoch:  32/ 50, batch:     4/    0, ite: 125] train loss: 0.076169, tar: 0.005147 \nl0: 0.025275, l1: 0.025099, l2: 0.029479, l3: 0.031947, l4: 0.052323, l5: 0.065633, l6: 0.092682\n\nl0: 0.004769, l1: 0.004862, l2: 0.005341, l3: 0.006042, l4: 0.008575, l5: 0.011250, l6: 0.018334\n\nl0: 0.030192, l1: 0.029885, l2: 0.031696, l3: 0.033059, l4: 0.034384, l5: 0.040964, l6: 0.045121\n\nl0: 0.008193, l1: 0.008140, l2: 0.009725, l3: 0.010835, l4: 0.012162, l5: 0.014549, l6: 0.024919\n\nl0: 0.033341, l1: 0.032766, l2: 0.032660, l3: 0.035890, l4: 0.040532, l5: 0.055882, l6: 0.092281\n\n[epoch:  32/ 50] Validation Loss: 0.207757\nl0: 0.004561, l1: 0.004652, l2: 0.005385, l3: 0.006278, l4: 0.008582, l5: 0.012472, l6: 0.024451\n\n[epoch:  32/ 50, batch:     8/    0, ite: 126] train loss: 0.074538, tar: 0.005050 \nl0: 0.059502, l1: 0.059429, l2: 0.060754, l3: 0.062644, l4: 0.064882, l5: 0.072617, l6: 0.089138\n\nl0: 0.005272, l1: 0.005354, l2: 0.005636, l3: 0.006275, l4: 0.007901, l5: 0.011248, l6: 0.018206\n\nl0: 0.035883, l1: 0.035934, l2: 0.034781, l3: 0.037656, l4: 0.038638, l5: 0.040858, l6: 0.047471\n\nl0: 0.007807, l1: 0.007730, l2: 0.009837, l3: 0.011282, l4: 0.012928, l5: 0.016426, l6: 0.027450\n\nl0: 0.012637, l1: 0.012530, l2: 0.013099, l3: 0.014694, l4: 0.021239, l5: 0.035734, l6: 0.065699\n\n[epoch:  32/ 50] Validation Loss: 0.213834\nl0: 0.005166, l1: 0.005293, l2: 0.006028, l3: 0.007118, l4: 0.009421, l5: 0.015149, l6: 0.027684\n\n[epoch:  32/ 50, batch:    12/    0, ite: 127] train loss: 0.074727, tar: 0.005066 \nl0: 0.051132, l1: 0.050750, l2: 0.052469, l3: 0.055357, l4: 0.062164, l5: 0.060821, l6: 0.077387\n\nl0: 0.004793, l1: 0.004878, l2: 0.005117, l3: 0.005999, l4: 0.007662, l5: 0.012332, l6: 0.017359\n\nl0: 0.024951, l1: 0.024339, l2: 0.027532, l3: 0.030635, l4: 0.036717, l5: 0.040367, l6: 0.051914\n\nl0: 0.007112, l1: 0.006953, l2: 0.009109, l3: 0.010595, l4: 0.012511, l5: 0.016774, l6: 0.026432\n\nl0: 0.014114, l1: 0.013988, l2: 0.014068, l3: 0.015889, l4: 0.020672, l5: 0.030163, l6: 0.073463\n\n[epoch:  32/ 50] Validation Loss: 0.195304\nl0: 0.005889, l1: 0.005982, l2: 0.006320, l3: 0.007302, l4: 0.009974, l5: 0.013879, l6: 0.025280\n\n[epoch:  32/ 50, batch:    16/    0, ite: 128] train loss: 0.074714, tar: 0.005169 \nl0: 0.025019, l1: 0.023799, l2: 0.025836, l3: 0.026711, l4: 0.046192, l5: 0.052265, l6: 0.069258\n\nl0: 0.004970, l1: 0.004997, l2: 0.005313, l3: 0.006192, l4: 0.008160, l5: 0.012957, l6: 0.017567\n\nl0: 0.037040, l1: 0.036972, l2: 0.037523, l3: 0.039223, l4: 0.039032, l5: 0.041108, l6: 0.043420\n\nl0: 0.007448, l1: 0.007301, l2: 0.009255, l3: 0.010712, l4: 0.012692, l5: 0.018488, l6: 0.026858\n\nl0: 0.035108, l1: 0.033500, l2: 0.038755, l3: 0.043678, l4: 0.047288, l5: 0.068870, l6: 0.093687\n\n[epoch:  32/ 50] Validation Loss: 0.211438\nl0: 0.004855, l1: 0.004936, l2: 0.005634, l3: 0.007028, l4: 0.009603, l5: 0.014253, l6: 0.027522\n\n[epoch:  33/ 50, batch:     4/    0, ite: 129] train loss: 0.074616, tar: 0.005134 \nl0: 0.053908, l1: 0.053728, l2: 0.054424, l3: 0.056562, l4: 0.062012, l5: 0.061025, l6: 0.083289\n\nl0: 0.005185, l1: 0.005274, l2: 0.005554, l3: 0.006217, l4: 0.007621, l5: 0.012360, l6: 0.018202\n\nl0: 0.040324, l1: 0.040509, l2: 0.039567, l3: 0.039391, l4: 0.038174, l5: 0.039089, l6: 0.049380\n\nl0: 0.007993, l1: 0.007895, l2: 0.009748, l3: 0.011355, l4: 0.013187, l5: 0.016515, l6: 0.027891\n\nl0: 0.020625, l1: 0.020364, l2: 0.020965, l3: 0.022903, l4: 0.027014, l5: 0.052758, l6: 0.077112\n\n[epoch:  33/ 50] Validation Loss: 0.221624\nl0: 0.005081, l1: 0.005150, l2: 0.005819, l3: 0.006558, l4: 0.008768, l5: 0.012991, l6: 0.023712\n\n[epoch:  33/ 50, batch:     8/    0, ite: 130] train loss: 0.073962, tar: 0.005129 \nl0: 0.022392, l1: 0.022568, l2: 0.024756, l3: 0.027278, l4: 0.038211, l5: 0.055850, l6: 0.085188\n\nl0: 0.005682, l1: 0.005700, l2: 0.006405, l3: 0.007365, l4: 0.008964, l5: 0.012270, l6: 0.018880\n\nl0: 0.037148, l1: 0.037097, l2: 0.037977, l3: 0.039814, l4: 0.036687, l5: 0.042187, l6: 0.053112\n\nl0: 0.007776, l1: 0.007714, l2: 0.009354, l3: 0.010539, l4: 0.012833, l5: 0.015537, l6: 0.025550\n\nl0: 0.015482, l1: 0.015359, l2: 0.015774, l3: 0.017344, l4: 0.023099, l5: 0.048406, l6: 0.096307\n\n[epoch:  33/ 50] Validation Loss: 0.189321\nl0: 0.005273, l1: 0.005316, l2: 0.006385, l3: 0.007461, l4: 0.010529, l5: 0.015474, l6: 0.027507\n\n[epoch:  33/ 50, batch:    12/    0, ite: 131] train loss: 0.077946, tar: 0.005273 \nl0: 0.009176, l1: 0.008972, l2: 0.010167, l3: 0.012769, l4: 0.021560, l5: 0.034102, l6: 0.064495\n\nl0: 0.006856, l1: 0.006927, l2: 0.007392, l3: 0.008120, l4: 0.009995, l5: 0.013308, l6: 0.024182\n\nl0: 0.040526, l1: 0.040688, l2: 0.040342, l3: 0.040395, l4: 0.039980, l5: 0.042567, l6: 0.044921\n\nl0: 0.007121, l1: 0.006908, l2: 0.009205, l3: 0.010531, l4: 0.012961, l5: 0.014704, l6: 0.022421\n\nl0: 0.028993, l1: 0.027960, l2: 0.032812, l3: 0.038144, l4: 0.052046, l5: 0.067043, l6: 0.114673\n\n[epoch:  33/ 50] Validation Loss: 0.194592\nl0: 0.005662, l1: 0.005750, l2: 0.006405, l3: 0.007618, l4: 0.011229, l5: 0.018380, l6: 0.032456\n\n[epoch:  33/ 50, batch:    16/    0, ite: 132] train loss: 0.082722, tar: 0.005467 \nl0: 0.009199, l1: 0.009007, l2: 0.010180, l3: 0.012136, l4: 0.019352, l5: 0.030076, l6: 0.062254\n\nl0: 0.006056, l1: 0.006050, l2: 0.006996, l3: 0.007859, l4: 0.009850, l5: 0.011642, l6: 0.017322\n\nl0: 0.030942, l1: 0.030536, l2: 0.031877, l3: 0.034006, l4: 0.034891, l5: 0.039148, l6: 0.046299\n\nl0: 0.007419, l1: 0.007508, l2: 0.008976, l3: 0.010406, l4: 0.011061, l5: 0.012842, l6: 0.025451\n\nl0: 0.015609, l1: 0.015413, l2: 0.016172, l3: 0.017745, l4: 0.025089, l5: 0.054187, l6: 0.092367\n\n[epoch:  33/ 50] Validation Loss: 0.157184\nl0: 0.004543, l1: 0.004584, l2: 0.005544, l3: 0.006539, l4: 0.009157, l5: 0.013102, l6: 0.024060\n\n[epoch:  34/ 50, batch:     4/    0, ite: 133] train loss: 0.077658, tar: 0.005159 \nl0: 0.047453, l1: 0.047394, l2: 0.047861, l3: 0.049874, l4: 0.054366, l5: 0.059033, l6: 0.072941\n\nl0: 0.004372, l1: 0.004461, l2: 0.004875, l3: 0.005942, l4: 0.007993, l5: 0.011926, l6: 0.018395\n\nl0: 0.038652, l1: 0.038436, l2: 0.038873, l3: 0.040792, l4: 0.040112, l5: 0.040323, l6: 0.051232\n\nl0: 0.007146, l1: 0.006952, l2: 0.009200, l3: 0.010369, l4: 0.012212, l5: 0.015030, l6: 0.028672\n\nl0: 0.013467, l1: 0.013399, l2: 0.013061, l3: 0.014521, l4: 0.020719, l5: 0.026848, l6: 0.050743\n\n[epoch:  34/ 50] Validation Loss: 0.193529\nl0: 0.005504, l1: 0.005593, l2: 0.006057, l3: 0.006947, l4: 0.008942, l5: 0.013413, l6: 0.025577\n\n[epoch:  34/ 50, batch:     8/    0, ite: 134] train loss: 0.076252, tar: 0.005245 \nl0: 0.053594, l1: 0.053456, l2: 0.054359, l3: 0.056282, l4: 0.061969, l5: 0.065086, l6: 0.077223\n\nl0: 0.007661, l1: 0.007774, l2: 0.007533, l3: 0.008018, l4: 0.009567, l5: 0.011856, l6: 0.018986\n\nl0: 0.027662, l1: 0.027304, l2: 0.030343, l3: 0.034360, l4: 0.036503, l5: 0.042258, l6: 0.042951\n\nl0: 0.006552, l1: 0.006429, l2: 0.008796, l3: 0.010178, l4: 0.011962, l5: 0.014606, l6: 0.028054\n\nl0: 0.018183, l1: 0.017990, l2: 0.017941, l3: 0.019870, l4: 0.025038, l5: 0.033967, l6: 0.060499\n\n[epoch:  34/ 50] Validation Loss: 0.202962\nl0: 0.005153, l1: 0.005216, l2: 0.005945, l3: 0.007275, l4: 0.010279, l5: 0.015736, l6: 0.027424\n\n[epoch:  34/ 50, batch:    12/    0, ite: 135] train loss: 0.076407, tar: 0.005227 \nl0: 0.045710, l1: 0.045873, l2: 0.046294, l3: 0.047711, l4: 0.050134, l5: 0.055210, l6: 0.072871\n\nl0: 0.005267, l1: 0.005569, l2: 0.005922, l3: 0.006846, l4: 0.009670, l5: 0.012924, l6: 0.019549\n\nl0: 0.033141, l1: 0.032840, l2: 0.034336, l3: 0.036007, l4: 0.035580, l5: 0.042092, l6: 0.046650\n\nl0: 0.007068, l1: 0.006847, l2: 0.009013, l3: 0.010702, l4: 0.012785, l5: 0.016311, l6: 0.027616\n\nl0: 0.018416, l1: 0.018254, l2: 0.019426, l3: 0.021462, l4: 0.031021, l5: 0.057094, l6: 0.089506\n\n[epoch:  34/ 50] Validation Loss: 0.207143\nl0: 0.004410, l1: 0.004456, l2: 0.005360, l3: 0.006300, l4: 0.008333, l5: 0.012675, l6: 0.024968\n\n[epoch:  34/ 50, batch:    16/    0, ite: 136] train loss: 0.074756, tar: 0.005091 \nl0: 0.061233, l1: 0.061173, l2: 0.062273, l3: 0.064554, l4: 0.066485, l5: 0.080346, l6: 0.097063\n\nl0: 0.004946, l1: 0.004997, l2: 0.005585, l3: 0.006301, l4: 0.008444, l5: 0.010453, l6: 0.016522\n\nl0: 0.027951, l1: 0.027605, l2: 0.030128, l3: 0.033478, l4: 0.036744, l5: 0.042477, l6: 0.043217\n\nl0: 0.007298, l1: 0.007170, l2: 0.009179, l3: 0.010692, l4: 0.013835, l5: 0.018934, l6: 0.030491\n\nl0: 0.015720, l1: 0.015570, l2: 0.016562, l3: 0.018699, l4: 0.029810, l5: 0.046569, l6: 0.111364\n\n[epoch:  34/ 50] Validation Loss: 0.228773\nl0: 0.004982, l1: 0.005031, l2: 0.006015, l3: 0.007230, l4: 0.009973, l5: 0.014766, l6: 0.025467\n\n[epoch:  35/ 50, batch:     4/    0, ite: 137] train loss: 0.074571, tar: 0.005075 \nl0: 0.025694, l1: 0.024917, l2: 0.027618, l3: 0.029528, l4: 0.045625, l5: 0.068155, l6: 0.074258\n\nl0: 0.004824, l1: 0.004875, l2: 0.005223, l3: 0.005929, l4: 0.008029, l5: 0.013380, l6: 0.017115\n\nl0: 0.029421, l1: 0.029083, l2: 0.031368, l3: 0.035988, l4: 0.037406, l5: 0.040773, l6: 0.041902\n\nl0: 0.010183, l1: 0.010326, l2: 0.010914, l3: 0.011479, l4: 0.012973, l5: 0.016728, l6: 0.026338\n\nl0: 0.015324, l1: 0.015337, l2: 0.014375, l3: 0.015441, l4: 0.018944, l5: 0.036468, l6: 0.089754\n\n[epoch:  35/ 50] Validation Loss: 0.181139\nl0: 0.005477, l1: 0.005535, l2: 0.006228, l3: 0.006973, l4: 0.009657, l5: 0.016311, l6: 0.028393\n\n[epoch:  35/ 50, batch:     8/    0, ite: 138] train loss: 0.075072, tar: 0.005125 \nl0: 0.052923, l1: 0.052939, l2: 0.053170, l3: 0.054493, l4: 0.057156, l5: 0.057929, l6: 0.075410\n\nl0: 0.005035, l1: 0.005088, l2: 0.005307, l3: 0.006079, l4: 0.007925, l5: 0.011637, l6: 0.016528\n\nl0: 0.036399, l1: 0.036321, l2: 0.035173, l3: 0.037773, l4: 0.037175, l5: 0.040742, l6: 0.046319\n\nl0: 0.007662, l1: 0.007451, l2: 0.009610, l3: 0.010815, l4: 0.012516, l5: 0.018822, l6: 0.028216\n\nl0: 0.020827, l1: 0.020698, l2: 0.020445, l3: 0.022234, l4: 0.026110, l5: 0.035833, l6: 0.066064\n\n[epoch:  35/ 50] Validation Loss: 0.207765\nl0: 0.005956, l1: 0.006048, l2: 0.006680, l3: 0.007582, l4: 0.010251, l5: 0.014909, l6: 0.028376\n\n[epoch:  35/ 50, batch:    12/    0, ite: 139] train loss: 0.075597, tar: 0.005218 \nl0: 0.045588, l1: 0.045621, l2: 0.046044, l3: 0.047921, l4: 0.046883, l5: 0.045101, l6: 0.063550\n\nl0: 0.004937, l1: 0.005011, l2: 0.005571, l3: 0.006475, l4: 0.008824, l5: 0.011846, l6: 0.017782\n\nl0: 0.037608, l1: 0.037541, l2: 0.037420, l3: 0.038563, l4: 0.037828, l5: 0.041401, l6: 0.051270\n\nl0: 0.007756, l1: 0.007647, l2: 0.009428, l3: 0.010643, l4: 0.012194, l5: 0.017338, l6: 0.026752\n\nl0: 0.013631, l1: 0.013579, l2: 0.013662, l3: 0.015468, l4: 0.021136, l5: 0.029088, l6: 0.079313\n\n[epoch:  35/ 50] Validation Loss: 0.192084\nl0: 0.004297, l1: 0.004351, l2: 0.004881, l3: 0.006033, l4: 0.008839, l5: 0.012546, l6: 0.022515\n\n[epoch:  35/ 50, batch:    16/    0, ite: 140] train loss: 0.074384, tar: 0.005126 \nl0: 0.009785, l1: 0.009656, l2: 0.010711, l3: 0.013277, l4: 0.021749, l5: 0.051929, l6: 0.081938\n\nl0: 0.004835, l1: 0.005158, l2: 0.005351, l3: 0.006098, l4: 0.008464, l5: 0.010899, l6: 0.017971\n\nl0: 0.032725, l1: 0.032535, l2: 0.033409, l3: 0.035365, l4: 0.036644, l5: 0.041121, l6: 0.045692\n\nl0: 0.007061, l1: 0.007020, l2: 0.008990, l3: 0.010945, l4: 0.014095, l5: 0.019489, l6: 0.028916\n\nl0: 0.012988, l1: 0.012853, l2: 0.012853, l3: 0.014925, l4: 0.021099, l5: 0.029656, l6: 0.093218\n\n[epoch:  35/ 50] Validation Loss: 0.161884\nl0: 0.004683, l1: 0.004777, l2: 0.005379, l3: 0.006295, l4: 0.008563, l5: 0.013500, l6: 0.028901\n\n[epoch:  36/ 50, batch:     4/    0, ite: 141] train loss: 0.072098, tar: 0.004683 \nl0: 0.065669, l1: 0.065729, l2: 0.065847, l3: 0.067258, l4: 0.068064, l5: 0.064423, l6: 0.081283\n\nl0: 0.005826, l1: 0.005949, l2: 0.006519, l3: 0.007439, l4: 0.010693, l5: 0.013001, l6: 0.017636\n\nl0: 0.040327, l1: 0.040409, l2: 0.040706, l3: 0.040924, l4: 0.040321, l5: 0.042292, l6: 0.050127\n\nl0: 0.007441, l1: 0.007340, l2: 0.009317, l3: 0.011445, l4: 0.014297, l5: 0.019271, l6: 0.026624\n\nl0: 0.020298, l1: 0.019751, l2: 0.022096, l3: 0.028146, l4: 0.042392, l5: 0.055128, l6: 0.093774\n\n[epoch:  36/ 50] Validation Loss: 0.243553\nl0: 0.005490, l1: 0.005593, l2: 0.006224, l3: 0.007105, l4: 0.009337, l5: 0.014123, l6: 0.027277\n\n[epoch:  36/ 50, batch:     8/    0, ite: 142] train loss: 0.073624, tar: 0.005087 \nl0: 0.059150, l1: 0.059103, l2: 0.059872, l3: 0.061923, l4: 0.063474, l5: 0.071811, l6: 0.088412\n\nl0: 0.005168, l1: 0.005232, l2: 0.005833, l3: 0.006896, l4: 0.008695, l5: 0.012206, l6: 0.020736\n\nl0: 0.035100, l1: 0.035017, l2: 0.035997, l3: 0.038340, l4: 0.037295, l5: 0.040654, l6: 0.050217\n\nl0: 0.008706, l1: 0.008721, l2: 0.009979, l3: 0.010693, l4: 0.011630, l5: 0.014070, l6: 0.025086\n\nl0: 0.016663, l1: 0.016487, l2: 0.016571, l3: 0.018487, l4: 0.023799, l5: 0.029144, l6: 0.074120\n\n[epoch:  36/ 50] Validation Loss: 0.217057\nl0: 0.004631, l1: 0.004636, l2: 0.005752, l3: 0.007139, l4: 0.009582, l5: 0.013970, l6: 0.025571\n\n[epoch:  36/ 50, batch:    12/    0, ite: 143] train loss: 0.072842, tar: 0.004935 \nl0: 0.028153, l1: 0.028050, l2: 0.028702, l3: 0.030561, l4: 0.033303, l5: 0.042390, l6: 0.077529\n\nl0: 0.004718, l1: 0.004826, l2: 0.005033, l3: 0.005774, l4: 0.007657, l5: 0.011881, l6: 0.015506\n\nl0: 0.040659, l1: 0.040767, l2: 0.041018, l3: 0.041182, l4: 0.039591, l5: 0.041162, l6: 0.048687\n\nl0: 0.008164, l1: 0.008114, l2: 0.009667, l3: 0.010839, l4: 0.012510, l5: 0.016853, l6: 0.027964\n\nl0: 0.017394, l1: 0.017372, l2: 0.016967, l3: 0.018607, l4: 0.021524, l5: 0.035154, l6: 0.096207\n\n[epoch:  36/ 50] Validation Loss: 0.186897\nl0: 0.005544, l1: 0.005638, l2: 0.006294, l3: 0.007230, l4: 0.009483, l5: 0.013826, l6: 0.024955\n\n[epoch:  36/ 50, batch:    16/    0, ite: 144] train loss: 0.072875, tar: 0.005087 \nl0: 0.060612, l1: 0.061143, l2: 0.062675, l3: 0.062770, l4: 0.070403, l5: 0.062271, l6: 0.102331\n\nl0: 0.005712, l1: 0.005801, l2: 0.006242, l3: 0.006969, l4: 0.008722, l5: 0.013880, l6: 0.024862\n\nl0: 0.030216, l1: 0.029989, l2: 0.032073, l3: 0.034267, l4: 0.036419, l5: 0.039486, l6: 0.047308\n\nl0: 0.007835, l1: 0.007756, l2: 0.009616, l3: 0.010775, l4: 0.012492, l5: 0.016209, l6: 0.028631\n\nl0: 0.018012, l1: 0.017968, l2: 0.017640, l3: 0.019634, l4: 0.022600, l5: 0.031531, l6: 0.050688\n\n[epoch:  36/ 50] Validation Loss: 0.215107\nl0: 0.004775, l1: 0.004818, l2: 0.005608, l3: 0.006790, l4: 0.008964, l5: 0.013549, l6: 0.027249\n\n[epoch:  37/ 50, batch:     4/    0, ite: 145] train loss: 0.072650, tar: 0.005025 \nl0: 0.067232, l1: 0.067155, l2: 0.067443, l3: 0.067260, l4: 0.064675, l5: 0.064558, l6: 0.074290\n\nl0: 0.004999, l1: 0.005048, l2: 0.005758, l3: 0.006690, l4: 0.008987, l5: 0.011829, l6: 0.018438\n\nl0: 0.034029, l1: 0.033876, l2: 0.035044, l3: 0.036303, l4: 0.036767, l5: 0.041554, l6: 0.048053\n\nl0: 0.007757, l1: 0.007816, l2: 0.009094, l3: 0.010107, l4: 0.010977, l5: 0.014072, l6: 0.024311\n\nl0: 0.027407, l1: 0.026456, l2: 0.029075, l3: 0.034883, l4: 0.041282, l5: 0.054655, l6: 0.097012\n\n[epoch:  37/ 50] Validation Loss: 0.238979\nl0: 0.004828, l1: 0.004948, l2: 0.005693, l3: 0.006921, l4: 0.010002, l5: 0.014424, l6: 0.027665\n\n[epoch:  37/ 50, batch:     8/    0, ite: 146] train loss: 0.072955, tar: 0.004992 \nl0: 0.059901, l1: 0.059915, l2: 0.060634, l3: 0.062347, l4: 0.062472, l5: 0.070691, l6: 0.095739\n\nl0: 0.005496, l1: 0.005492, l2: 0.006191, l3: 0.007125, l4: 0.009158, l5: 0.011320, l6: 0.019565\n\nl0: 0.036685, l1: 0.036574, l2: 0.037595, l3: 0.039905, l4: 0.039582, l5: 0.043068, l6: 0.050543\n\nl0: 0.007024, l1: 0.007019, l2: 0.008529, l3: 0.009635, l4: 0.011480, l5: 0.015581, l6: 0.024423\n\nl0: 0.016752, l1: 0.016599, l2: 0.016667, l3: 0.018795, l4: 0.023730, l5: 0.030033, l6: 0.088973\n\n[epoch:  37/ 50] Validation Loss: 0.223047\nl0: 0.005728, l1: 0.005742, l2: 0.006447, l3: 0.007435, l4: 0.010311, l5: 0.014839, l6: 0.023099\n\n[epoch:  37/ 50, batch:    12/    0, ite: 147] train loss: 0.073048, tar: 0.005097 \nl0: 0.021531, l1: 0.021199, l2: 0.022829, l3: 0.025468, l4: 0.039549, l5: 0.044390, l6: 0.074883\n\nl0: 0.005543, l1: 0.005533, l2: 0.006319, l3: 0.007425, l4: 0.009516, l5: 0.012797, l6: 0.022648\n\nl0: 0.028172, l1: 0.027891, l2: 0.030563, l3: 0.035386, l4: 0.037563, l5: 0.039858, l6: 0.045303\n\nl0: 0.006597, l1: 0.006628, l2: 0.008092, l3: 0.009183, l4: 0.010383, l5: 0.012855, l6: 0.023598\n\nl0: 0.023111, l1: 0.022671, l2: 0.023389, l3: 0.026499, l4: 0.038186, l5: 0.060519, l6: 0.092011\n\n[epoch:  37/ 50] Validation Loss: 0.185618\nl0: 0.005955, l1: 0.006122, l2: 0.006190, l3: 0.007162, l4: 0.010252, l5: 0.014592, l6: 0.024945\n\n[epoch:  37/ 50, batch:    16/    0, ite: 148] train loss: 0.073319, tar: 0.005204 \nl0: 0.062937, l1: 0.062771, l2: 0.063090, l3: 0.064570, l4: 0.061769, l5: 0.076440, l6: 0.086844\n\nl0: 0.005507, l1: 0.005586, l2: 0.006157, l3: 0.006869, l4: 0.009109, l5: 0.012089, l6: 0.021235\n\nl0: 0.040437, l1: 0.040570, l2: 0.040111, l3: 0.040354, l4: 0.039438, l5: 0.041910, l6: 0.045409\n\nl0: 0.007241, l1: 0.007149, l2: 0.009023, l3: 0.010293, l4: 0.011693, l5: 0.014541, l6: 0.025496\n\nl0: 0.018183, l1: 0.018049, l2: 0.017638, l3: 0.019076, l4: 0.022829, l5: 0.034504, l6: 0.081013\n\n[epoch:  37/ 50] Validation Loss: 0.225987\nl0: 0.004097, l1: 0.004207, l2: 0.004764, l3: 0.005844, l4: 0.008543, l5: 0.014167, l6: 0.025515\n\n[epoch:  38/ 50, batch:     4/    0, ite: 149] train loss: 0.072632, tar: 0.005081 \nl0: 0.075207, l1: 0.075235, l2: 0.073930, l3: 0.074900, l4: 0.067157, l5: 0.055276, l6: 0.079081\n\nl0: 0.004369, l1: 0.004417, l2: 0.005022, l3: 0.006032, l4: 0.007989, l5: 0.012031, l6: 0.016769\n\nl0: 0.040000, l1: 0.040201, l2: 0.039001, l3: 0.039188, l4: 0.038374, l5: 0.039806, l6: 0.042980\n\nl0: 0.007727, l1: 0.007577, l2: 0.009541, l3: 0.010786, l4: 0.012255, l5: 0.016350, l6: 0.024273\n\nl0: 0.019168, l1: 0.018974, l2: 0.018706, l3: 0.019883, l4: 0.026306, l5: 0.037629, l6: 0.050758\n\n[epoch:  38/ 50] Validation Loss: 0.223380\nl0: 0.004448, l1: 0.004474, l2: 0.005673, l3: 0.007037, l4: 0.009669, l5: 0.014856, l6: 0.030413\n\n[epoch:  38/ 50, batch:     8/    0, ite: 150] train loss: 0.073026, tar: 0.005018 \nl0: 0.066935, l1: 0.066866, l2: 0.063881, l3: 0.064315, l4: 0.062027, l5: 0.057613, l6: 0.079577\n\nl0: 0.005089, l1: 0.005120, l2: 0.005586, l3: 0.006371, l4: 0.008583, l5: 0.011634, l6: 0.017346\n\nl0: 0.025733, l1: 0.025209, l2: 0.028239, l3: 0.032935, l4: 0.037163, l5: 0.040335, l6: 0.045072\n\nl0: 0.007448, l1: 0.007357, l2: 0.009168, l3: 0.010187, l4: 0.012311, l5: 0.017604, l6: 0.024159\n\nl0: 0.018194, l1: 0.018065, l2: 0.018054, l3: 0.019224, l4: 0.023708, l5: 0.054667, l6: 0.087324\n\n[epoch:  38/ 50] Validation Loss: 0.216620\nl0: 0.005318, l1: 0.005416, l2: 0.006124, l3: 0.007132, l4: 0.009197, l5: 0.013537, l6: 0.022466\n\n[epoch:  38/ 50, batch:    12/    0, ite: 151] train loss: 0.069189, tar: 0.005318 \nl0: 0.066462, l1: 0.066256, l2: 0.067427, l3: 0.067849, l4: 0.065325, l5: 0.065421, l6: 0.071948\n\nl0: 0.005472, l1: 0.005530, l2: 0.005987, l3: 0.006784, l4: 0.008280, l5: 0.013183, l6: 0.021142\n\nl0: 0.033015, l1: 0.032711, l2: 0.034425, l3: 0.036374, l4: 0.036026, l5: 0.042068, l6: 0.054984\n\nl0: 0.008782, l1: 0.008654, l2: 0.010347, l3: 0.011499, l4: 0.013530, l5: 0.017621, l6: 0.029629\n\nl0: 0.013478, l1: 0.013327, l2: 0.013810, l3: 0.015407, l4: 0.020919, l5: 0.029184, l6: 0.089167\n\n[epoch:  38/ 50] Validation Loss: 0.220405\nl0: 0.005561, l1: 0.005634, l2: 0.006062, l3: 0.006822, l4: 0.008971, l5: 0.013155, l6: 0.023418\n\n[epoch:  38/ 50, batch:    16/    0, ite: 152] train loss: 0.069406, tar: 0.005440 \nl0: 0.020321, l1: 0.020165, l2: 0.021344, l3: 0.023481, l4: 0.029366, l5: 0.042282, l6: 0.081083\n\nl0: 0.011626, l1: 0.011754, l2: 0.011751, l3: 0.012066, l4: 0.013744, l5: 0.013069, l6: 0.017733\n\nl0: 0.039910, l1: 0.040034, l2: 0.040000, l3: 0.039887, l4: 0.038431, l5: 0.040500, l6: 0.044930\n\nl0: 0.008204, l1: 0.008042, l2: 0.009651, l3: 0.010714, l4: 0.013086, l5: 0.018681, l6: 0.028029\n\nl0: 0.091056, l1: 0.089447, l2: 0.088054, l3: 0.087770, l4: 0.086626, l5: 0.089225, l6: 0.107210\n\n[epoch:  38/ 50] Validation Loss: 0.269854\nl0: 0.004058, l1: 0.004151, l2: 0.004763, l3: 0.005717, l4: 0.008499, l5: 0.013267, l6: 0.025816\n\n[epoch:  39/ 50, batch:     4/    0, ite: 153] train loss: 0.068361, tar: 0.004979 \nl0: 0.022179, l1: 0.021948, l2: 0.023212, l3: 0.025116, l4: 0.036645, l5: 0.047720, l6: 0.085213\n\nl0: 0.005393, l1: 0.005432, l2: 0.006248, l3: 0.006974, l4: 0.009397, l5: 0.012073, l6: 0.023249\n\nl0: 0.032461, l1: 0.032373, l2: 0.032403, l3: 0.037166, l4: 0.037446, l5: 0.039715, l6: 0.042153\n\nl0: 0.007608, l1: 0.007438, l2: 0.008868, l3: 0.010474, l4: 0.013040, l5: 0.016569, l6: 0.026084\n\nl0: 0.015539, l1: 0.015495, l2: 0.017511, l3: 0.019605, l4: 0.029979, l5: 0.066034, l6: 0.083402\n\n[epoch:  39/ 50] Validation Loss: 0.184432\nl0: 0.004215, l1: 0.004251, l2: 0.005070, l3: 0.006477, l4: 0.009198, l5: 0.013180, l6: 0.023412\n\n[epoch:  39/ 50, batch:     8/    0, ite: 154] train loss: 0.067721, tar: 0.004788 \nl0: 0.047264, l1: 0.047563, l2: 0.047607, l3: 0.048999, l4: 0.048739, l5: 0.053516, l6: 0.069539\n\nl0: 0.005330, l1: 0.005319, l2: 0.006290, l3: 0.007159, l4: 0.008818, l5: 0.011500, l6: 0.018848\n\nl0: 0.028493, l1: 0.028031, l2: 0.031813, l3: 0.035782, l4: 0.037021, l5: 0.041441, l6: 0.044045\n\nl0: 0.007240, l1: 0.007157, l2: 0.009045, l3: 0.010250, l4: 0.012718, l5: 0.015325, l6: 0.022990\n\nl0: 0.025405, l1: 0.025197, l2: 0.025499, l3: 0.027213, l4: 0.030226, l5: 0.043024, l6: 0.086428\n\n[epoch:  39/ 50] Validation Loss: 0.204166\nl0: 0.005860, l1: 0.005941, l2: 0.006619, l3: 0.007555, l4: 0.010154, l5: 0.015785, l6: 0.025747\n\n[epoch:  39/ 50, batch:    12/    0, ite: 155] train loss: 0.069709, tar: 0.005002 \nl0: 0.069054, l1: 0.068897, l2: 0.069499, l3: 0.071939, l4: 0.065636, l5: 0.062967, l6: 0.096261\n\nl0: 0.004851, l1: 0.004881, l2: 0.005566, l3: 0.006170, l4: 0.007864, l5: 0.010319, l6: 0.016792\n\nl0: 0.032732, l1: 0.032500, l2: 0.033123, l3: 0.034607, l4: 0.035738, l5: 0.041782, l6: 0.044389\n\nl0: 0.006939, l1: 0.006982, l2: 0.008383, l3: 0.009259, l4: 0.010806, l5: 0.015223, l6: 0.023210\n\nl0: 0.016660, l1: 0.016436, l2: 0.016425, l3: 0.018188, l4: 0.023807, l5: 0.030415, l6: 0.060581\n\n[epoch:  39/ 50] Validation Loss: 0.215776\nl0: 0.004431, l1: 0.004469, l2: 0.005103, l3: 0.005978, l4: 0.008276, l5: 0.012326, l6: 0.021991\n\n[epoch:  39/ 50, batch:    16/    0, ite: 156] train loss: 0.068520, tar: 0.004907 \nl0: 0.053426, l1: 0.053312, l2: 0.053107, l3: 0.053871, l4: 0.057899, l5: 0.059005, l6: 0.072846\n\nl0: 0.004635, l1: 0.004729, l2: 0.004832, l3: 0.005490, l4: 0.007114, l5: 0.012498, l6: 0.017373\n\nl0: 0.036076, l1: 0.035991, l2: 0.037066, l3: 0.039418, l4: 0.038074, l5: 0.040927, l6: 0.049541\n\nl0: 0.007404, l1: 0.007231, l2: 0.009187, l3: 0.010276, l4: 0.012537, l5: 0.016127, l6: 0.022883\n\nl0: 0.015564, l1: 0.015413, l2: 0.016857, l3: 0.018756, l4: 0.034355, l5: 0.058153, l6: 0.094638\n\n[epoch:  39/ 50] Validation Loss: 0.215322\nl0: 0.004157, l1: 0.004230, l2: 0.004897, l3: 0.005734, l4: 0.007743, l5: 0.012786, l6: 0.022520\n\n[epoch:  40/ 50, batch:     4/    0, ite: 157] train loss: 0.067598, tar: 0.004800 \nl0: 0.047076, l1: 0.046964, l2: 0.047683, l3: 0.049733, l4: 0.052816, l5: 0.055631, l6: 0.072467\n\nl0: 0.005274, l1: 0.005324, l2: 0.005885, l3: 0.006774, l4: 0.008746, l5: 0.011895, l6: 0.020789\n\nl0: 0.031279, l1: 0.031048, l2: 0.032066, l3: 0.033453, l4: 0.034041, l5: 0.039101, l6: 0.052571\n\nl0: 0.007375, l1: 0.007282, l2: 0.008637, l3: 0.009817, l4: 0.011274, l5: 0.015015, l6: 0.027051\n\nl0: 0.050473, l1: 0.045697, l2: 0.049201, l3: 0.048012, l4: 0.065547, l5: 0.074663, l6: 0.103474\n\n[epoch:  40/ 50] Validation Loss: 0.242827\nl0: 0.004316, l1: 0.004376, l2: 0.005112, l3: 0.006212, l4: 0.008138, l5: 0.012073, l6: 0.021994\n\n[epoch:  40/ 50, batch:     8/    0, ite: 158] train loss: 0.066926, tar: 0.004739 \nl0: 0.048462, l1: 0.049091, l2: 0.049116, l3: 0.050341, l4: 0.048758, l5: 0.049889, l6: 0.064689\n\nl0: 0.005477, l1: 0.005793, l2: 0.006876, l3: 0.008135, l4: 0.010468, l5: 0.011816, l6: 0.018894\n\nl0: 0.040362, l1: 0.040346, l2: 0.040906, l3: 0.041047, l4: 0.040218, l5: 0.043012, l6: 0.051291\n\nl0: 0.006706, l1: 0.006547, l2: 0.008506, l3: 0.009899, l4: 0.012449, l5: 0.017357, l6: 0.022000\n\nl0: 0.012770, l1: 0.012661, l2: 0.013016, l3: 0.014444, l4: 0.020144, l5: 0.028113, l6: 0.073143\n\n[epoch:  40/ 50] Validation Loss: 0.196548\nl0: 0.004507, l1: 0.004514, l2: 0.005380, l3: 0.006862, l4: 0.009641, l5: 0.013852, l6: 0.024060\n\n[epoch:  40/ 50, batch:    12/    0, ite: 159] train loss: 0.067136, tar: 0.004714 \nl0: 0.058798, l1: 0.059005, l2: 0.059992, l3: 0.061157, l4: 0.064775, l5: 0.097626, l6: 0.172378\n\nl0: 0.005247, l1: 0.005244, l2: 0.005975, l3: 0.006798, l4: 0.009332, l5: 0.011325, l6: 0.019457\n\nl0: 0.039799, l1: 0.039830, l2: 0.040340, l3: 0.040366, l4: 0.038583, l5: 0.041220, l6: 0.045362\n\nl0: 0.007178, l1: 0.007193, l2: 0.008561, l3: 0.009538, l4: 0.010603, l5: 0.012550, l6: 0.024030\n\nl0: 0.014863, l1: 0.014905, l2: 0.014555, l3: 0.016169, l4: 0.022839, l5: 0.049680, l6: 0.070136\n\n[epoch:  40/ 50] Validation Loss: 0.241082\nl0: 0.005996, l1: 0.006050, l2: 0.006740, l3: 0.007762, l4: 0.010018, l5: 0.016393, l6: 0.027933\n\n[epoch:  40/ 50, batch:    16/    0, ite: 160] train loss: 0.068511, tar: 0.004842 \nl0: 0.010041, l1: 0.010036, l2: 0.011616, l3: 0.015091, l4: 0.033675, l5: 0.043006, l6: 0.089086\n\nl0: 0.004541, l1: 0.004658, l2: 0.005131, l3: 0.006232, l4: 0.009361, l5: 0.012204, l6: 0.019805\n\nl0: 0.031829, l1: 0.031662, l2: 0.032473, l3: 0.033787, l4: 0.034101, l5: 0.038188, l6: 0.041781\n\nl0: 0.007548, l1: 0.007391, l2: 0.009070, l3: 0.010599, l4: 0.014150, l5: 0.020334, l6: 0.030039\n\nl0: 0.014638, l1: 0.014440, l2: 0.014878, l3: 0.016620, l4: 0.022528, l5: 0.039923, l6: 0.089487\n\n[epoch:  40/ 50] Validation Loss: 0.165989\nl0: 0.004951, l1: 0.004995, l2: 0.005623, l3: 0.006761, l4: 0.009604, l5: 0.015400, l6: 0.028153\n\n[epoch:  41/ 50, batch:     4/    0, ite: 161] train loss: 0.075487, tar: 0.004951 \nl0: 0.069949, l1: 0.069431, l2: 0.068420, l3: 0.071160, l4: 0.064448, l5: 0.067858, l6: 0.084496\n\nl0: 0.004744, l1: 0.004812, l2: 0.005339, l3: 0.006221, l4: 0.008305, l5: 0.011021, l6: 0.017346\n\nl0: 0.034001, l1: 0.033718, l2: 0.034939, l3: 0.036907, l4: 0.036398, l5: 0.040507, l6: 0.048935\n\nl0: 0.007996, l1: 0.007896, l2: 0.009165, l3: 0.010278, l4: 0.011818, l5: 0.016676, l6: 0.022930\n\nl0: 0.016372, l1: 0.016260, l2: 0.016079, l3: 0.017200, l4: 0.020421, l5: 0.032649, l6: 0.091255\n\n[epoch:  41/ 50] Validation Loss: 0.223190\nl0: 0.005088, l1: 0.005198, l2: 0.005715, l3: 0.006795, l4: 0.008866, l5: 0.013833, l6: 0.024968\n\n[epoch:  41/ 50, batch:     8/    0, ite: 162] train loss: 0.072975, tar: 0.005019 \nl0: 0.062809, l1: 0.062721, l2: 0.063194, l3: 0.064281, l4: 0.065434, l5: 0.075675, l6: 0.090777\n\nl0: 0.004231, l1: 0.004284, l2: 0.004718, l3: 0.005925, l4: 0.008416, l5: 0.012740, l6: 0.021417\n\nl0: 0.024198, l1: 0.023687, l2: 0.026527, l3: 0.029720, l4: 0.033226, l5: 0.037400, l6: 0.043143\n\nl0: 0.007048, l1: 0.006846, l2: 0.008910, l3: 0.010067, l4: 0.011811, l5: 0.015356, l6: 0.024373\n\nl0: 0.060596, l1: 0.057978, l2: 0.058484, l3: 0.059075, l4: 0.066716, l5: 0.071367, l6: 0.101363\n\n[epoch:  41/ 50] Validation Loss: 0.264903\nl0: 0.004154, l1: 0.004193, l2: 0.005017, l3: 0.006146, l4: 0.008559, l5: 0.013072, l6: 0.027169\n\n[epoch:  41/ 50, batch:    12/    0, ite: 163] train loss: 0.071419, tar: 0.004731 \nl0: 0.049289, l1: 0.049307, l2: 0.049560, l3: 0.050330, l4: 0.056054, l5: 0.055775, l6: 0.071827\n\nl0: 0.004869, l1: 0.005076, l2: 0.005440, l3: 0.006063, l4: 0.008224, l5: 0.011288, l6: 0.020217\n\nl0: 0.030235, l1: 0.030013, l2: 0.032395, l3: 0.034345, l4: 0.034247, l5: 0.041219, l6: 0.045954\n\nl0: 0.006645, l1: 0.006557, l2: 0.008629, l3: 0.009957, l4: 0.012112, l5: 0.016760, l6: 0.025891\n\nl0: 0.014812, l1: 0.014690, l2: 0.015060, l3: 0.017266, l4: 0.021657, l5: 0.045181, l6: 0.109450\n\n[epoch:  41/ 50] Validation Loss: 0.203279\nl0: 0.004656, l1: 0.004715, l2: 0.005319, l3: 0.006220, l4: 0.008259, l5: 0.012019, l6: 0.023923\n\n[epoch:  41/ 50, batch:    16/    0, ite: 164] train loss: 0.069842, tar: 0.004712 \nl0: 0.034772, l1: 0.034038, l2: 0.034367, l3: 0.036405, l4: 0.043195, l5: 0.049444, l6: 0.069989\n\nl0: 0.006102, l1: 0.006227, l2: 0.006719, l3: 0.007414, l4: 0.009019, l5: 0.012780, l6: 0.022772\n\nl0: 0.038596, l1: 0.038220, l2: 0.038327, l3: 0.039908, l4: 0.039686, l5: 0.040517, l6: 0.046537\n\nl0: 0.007183, l1: 0.007195, l2: 0.008579, l3: 0.009664, l4: 0.011290, l5: 0.015277, l6: 0.024398\n\nl0: 0.012269, l1: 0.012177, l2: 0.012632, l3: 0.014004, l4: 0.019732, l5: 0.026042, l6: 0.073870\n\n[epoch:  41/ 50] Validation Loss: 0.181870\nl0: 0.004704, l1: 0.004759, l2: 0.005442, l3: 0.006317, l4: 0.008820, l5: 0.013538, l6: 0.028558\n\n[epoch:  42/ 50, batch:     4/    0, ite: 165] train loss: 0.070301, tar: 0.004711 \nl0: 0.008364, l1: 0.008355, l2: 0.009246, l3: 0.010876, l4: 0.014391, l5: 0.023102, l6: 0.057693\n\nl0: 0.004883, l1: 0.004939, l2: 0.005500, l3: 0.006439, l4: 0.008780, l5: 0.011979, l6: 0.020891\n\nl0: 0.040129, l1: 0.039715, l2: 0.039422, l3: 0.040329, l4: 0.039170, l5: 0.041065, l6: 0.040543\n\nl0: 0.006740, l1: 0.006617, l2: 0.008664, l3: 0.009969, l4: 0.012628, l5: 0.017615, l6: 0.022008\n\nl0: 0.014262, l1: 0.014124, l2: 0.014509, l3: 0.016343, l4: 0.021179, l5: 0.030803, l6: 0.077008\n\n[epoch:  42/ 50] Validation Loss: 0.149656\nl0: 0.004333, l1: 0.004353, l2: 0.005286, l3: 0.006670, l4: 0.008802, l5: 0.014069, l6: 0.023329\n\n[epoch:  42/ 50, batch:     8/    0, ite: 166] train loss: 0.069725, tar: 0.004648 \nl0: 0.050387, l1: 0.050351, l2: 0.050531, l3: 0.051701, l4: 0.056817, l5: 0.056552, l6: 0.071217\n\nl0: 0.005408, l1: 0.005377, l2: 0.006206, l3: 0.006965, l4: 0.009818, l5: 0.012323, l6: 0.019589\n\nl0: 0.040612, l1: 0.040614, l2: 0.040783, l3: 0.041191, l4: 0.040334, l5: 0.043268, l6: 0.049091\n\nl0: 0.006495, l1: 0.006360, l2: 0.008527, l3: 0.009707, l4: 0.012413, l5: 0.014508, l6: 0.021927\n\nl0: 0.017425, l1: 0.017212, l2: 0.017238, l3: 0.019931, l4: 0.032937, l5: 0.040873, l6: 0.076873\n\n[epoch:  42/ 50] Validation Loss: 0.210313\nl0: 0.005251, l1: 0.005307, l2: 0.005901, l3: 0.006841, l4: 0.009392, l5: 0.013515, l6: 0.024610\n\n[epoch:  42/ 50, batch:    12/    0, ite: 167] train loss: 0.069881, tar: 0.004734 \nl0: 0.045735, l1: 0.045792, l2: 0.046238, l3: 0.047571, l4: 0.047177, l5: 0.049905, l6: 0.067812\n\nl0: 0.004933, l1: 0.004992, l2: 0.005463, l3: 0.006262, l4: 0.007908, l5: 0.012417, l6: 0.018452\n\nl0: 0.031302, l1: 0.031014, l2: 0.032853, l3: 0.034142, l4: 0.034517, l5: 0.039044, l6: 0.043573\n\nl0: 0.007720, l1: 0.007721, l2: 0.008927, l3: 0.009762, l4: 0.011469, l5: 0.014259, l6: 0.023214\n\nl0: 0.016305, l1: 0.016145, l2: 0.016123, l3: 0.017022, l4: 0.023779, l5: 0.040399, l6: 0.076005\n\n[epoch:  42/ 50] Validation Loss: 0.189191\nl0: 0.004357, l1: 0.004416, l2: 0.005041, l3: 0.005806, l4: 0.007754, l5: 0.012848, l6: 0.022751\n\n[epoch:  42/ 50, batch:    16/    0, ite: 168] train loss: 0.069017, tar: 0.004687 \nl0: 0.030175, l1: 0.030119, l2: 0.030556, l3: 0.032219, l4: 0.034343, l5: 0.044450, l6: 0.077009\n\nl0: 0.004870, l1: 0.004894, l2: 0.005302, l3: 0.006278, l4: 0.008586, l5: 0.013270, l6: 0.018534\n\nl0: 0.034588, l1: 0.034507, l2: 0.035497, l3: 0.037923, l4: 0.037111, l5: 0.040181, l6: 0.052894\n\nl0: 0.006846, l1: 0.006668, l2: 0.008525, l3: 0.009659, l4: 0.012002, l5: 0.014194, l6: 0.021199\n\nl0: 0.011978, l1: 0.011968, l2: 0.012110, l3: 0.013762, l4: 0.019097, l5: 0.029445, l6: 0.057375\n\n[epoch:  42/ 50] Validation Loss: 0.169627\nl0: 0.004991, l1: 0.005063, l2: 0.005756, l3: 0.006803, l4: 0.009349, l5: 0.014796, l6: 0.027502\n\n[epoch:  43/ 50, batch:     4/    0, ite: 169] train loss: 0.069600, tar: 0.004721 \nl0: 0.017755, l1: 0.017921, l2: 0.020136, l3: 0.022215, l4: 0.031548, l5: 0.038807, l6: 0.074382\n\nl0: 0.006087, l1: 0.006423, l2: 0.006608, l3: 0.007392, l4: 0.009679, l5: 0.012506, l6: 0.021346\n\nl0: 0.027327, l1: 0.026972, l2: 0.030217, l3: 0.034005, l4: 0.036637, l5: 0.041665, l6: 0.042160\n\nl0: 0.007626, l1: 0.007452, l2: 0.009287, l3: 0.010371, l4: 0.012898, l5: 0.015608, l6: 0.020722\n\nl0: 0.013939, l1: 0.013766, l2: 0.015507, l3: 0.017032, l4: 0.031851, l5: 0.053584, l6: 0.094355\n\n[epoch:  43/ 50] Validation Loss: 0.171157\nl0: 0.004864, l1: 0.004890, l2: 0.005799, l3: 0.007039, l4: 0.009824, l5: 0.015590, l6: 0.028036\n\n[epoch:  43/ 50, batch:     8/    0, ite: 170] train loss: 0.070244, tar: 0.004735 \nl0: 0.062410, l1: 0.062375, l2: 0.062598, l3: 0.063715, l4: 0.065681, l5: 0.064548, l6: 0.084026\n\nl0: 0.004657, l1: 0.004717, l2: 0.005074, l3: 0.006001, l4: 0.008566, l5: 0.013892, l6: 0.019355\n\nl0: 0.034574, l1: 0.034514, l2: 0.035370, l3: 0.037834, l4: 0.037195, l5: 0.039619, l6: 0.041479\n\nl0: 0.007173, l1: 0.007121, l2: 0.008855, l3: 0.010077, l4: 0.012195, l5: 0.014779, l6: 0.024769\n\nl0: 0.016301, l1: 0.016466, l2: 0.018203, l3: 0.019907, l4: 0.043711, l5: 0.059613, l6: 0.098589\n\n[epoch:  43/ 50] Validation Loss: 0.229192\nl0: 0.004075, l1: 0.004118, l2: 0.004973, l3: 0.006000, l4: 0.008940, l5: 0.013229, l6: 0.025102\n\n[epoch:  43/ 50, batch:    12/    0, ite: 171] train loss: 0.066437, tar: 0.004075 \nl0: 0.014601, l1: 0.014890, l2: 0.017380, l3: 0.021471, l4: 0.041639, l5: 0.047660, l6: 0.075626\n\nl0: 0.005920, l1: 0.006353, l2: 0.006305, l3: 0.006795, l4: 0.008816, l5: 0.012489, l6: 0.023100\n\nl0: 0.035617, l1: 0.035587, l2: 0.036425, l3: 0.039055, l4: 0.037182, l5: 0.040531, l6: 0.043249\n\nl0: 0.008537, l1: 0.008383, l2: 0.009718, l3: 0.010580, l4: 0.012514, l5: 0.017490, l6: 0.026869\n\nl0: 0.014390, l1: 0.014261, l2: 0.014740, l3: 0.016620, l4: 0.022103, l5: 0.046169, l6: 0.073157\n\n[epoch:  43/ 50] Validation Loss: 0.173244\nl0: 0.004031, l1: 0.004104, l2: 0.004797, l3: 0.005998, l4: 0.008119, l5: 0.012594, l6: 0.024307\n\n[epoch:  43/ 50, batch:    16/    0, ite: 172] train loss: 0.065193, tar: 0.004053 \nl0: 0.068165, l1: 0.068829, l2: 0.068003, l3: 0.071166, l4: 0.066382, l5: 0.064338, l6: 0.087628\n\nl0: 0.005625, l1: 0.005772, l2: 0.006037, l3: 0.006856, l4: 0.007943, l5: 0.012125, l6: 0.023402\n\nl0: 0.029408, l1: 0.028796, l2: 0.031203, l3: 0.034732, l4: 0.037317, l5: 0.041899, l6: 0.050706\n\nl0: 0.006742, l1: 0.006687, l2: 0.008303, l3: 0.009408, l4: 0.011208, l5: 0.013791, l6: 0.022698\n\nl0: 0.023188, l1: 0.023004, l2: 0.023249, l3: 0.025206, l4: 0.029327, l5: 0.057890, l6: 0.087588\n\n[epoch:  43/ 50] Validation Loss: 0.232924\nl0: 0.004628, l1: 0.004706, l2: 0.005202, l3: 0.006287, l4: 0.008732, l5: 0.012709, l6: 0.024423\n\n[epoch:  44/ 50, batch:     4/    0, ite: 173] train loss: 0.065691, tar: 0.004244 \nl0: 0.068144, l1: 0.068256, l2: 0.068635, l3: 0.069200, l4: 0.070821, l5: 0.069076, l6: 0.067975\n\nl0: 0.006720, l1: 0.006915, l2: 0.007738, l3: 0.008558, l4: 0.011544, l5: 0.013162, l6: 0.022492\n\nl0: 0.025451, l1: 0.024763, l2: 0.027658, l3: 0.031880, l4: 0.036947, l5: 0.039532, l6: 0.042508\n\nl0: 0.007797, l1: 0.007593, l2: 0.009562, l3: 0.010501, l4: 0.012267, l5: 0.015168, l6: 0.024236\n\nl0: 0.031190, l1: 0.030556, l2: 0.031652, l3: 0.035619, l4: 0.044873, l5: 0.066024, l6: 0.089539\n\n[epoch:  44/ 50] Validation Loss: 0.240910\nl0: 0.004919, l1: 0.005007, l2: 0.005469, l3: 0.006242, l4: 0.008324, l5: 0.012770, l6: 0.022696\n\n[epoch:  44/ 50, batch:     8/    0, ite: 174] train loss: 0.065625, tar: 0.004413 \nl0: 0.027044, l1: 0.026974, l2: 0.027833, l3: 0.029485, l4: 0.032904, l5: 0.042566, l6: 0.076956\n\nl0: 0.005150, l1: 0.005154, l2: 0.005925, l3: 0.007152, l4: 0.009956, l5: 0.012930, l6: 0.019992\n\nl0: 0.036335, l1: 0.036446, l2: 0.036649, l3: 0.038792, l4: 0.037242, l5: 0.040689, l6: 0.048115\n\nl0: 0.007601, l1: 0.007458, l2: 0.009050, l3: 0.010056, l4: 0.011753, l5: 0.015887, l6: 0.027304\n\nl0: 0.065969, l1: 0.064292, l2: 0.063893, l3: 0.063185, l4: 0.070886, l5: 0.076896, l6: 0.102127\n\n[epoch:  44/ 50] Validation Loss: 0.240129\nl0: 0.003821, l1: 0.003871, l2: 0.004784, l3: 0.005923, l4: 0.008160, l5: 0.012368, l6: 0.024805\n\n[epoch:  44/ 50, batch:    12/    0, ite: 175] train loss: 0.065246, tar: 0.004295 \nl0: 0.056389, l1: 0.057641, l2: 0.059897, l3: 0.059860, l4: 0.056382, l5: 0.051822, l6: 0.154969\n\nl0: 0.005325, l1: 0.005386, l2: 0.005935, l3: 0.006761, l4: 0.008658, l5: 0.011668, l6: 0.021911\n\nl0: 0.027369, l1: 0.026835, l2: 0.028864, l3: 0.031472, l4: 0.034637, l5: 0.040173, l6: 0.042541\n\nl0: 0.007411, l1: 0.007269, l2: 0.009128, l3: 0.010174, l4: 0.012540, l5: 0.015126, l6: 0.027920\n\nl0: 0.012325, l1: 0.012238, l2: 0.012101, l3: 0.013310, l4: 0.019658, l5: 0.025492, l6: 0.068257\n\n[epoch:  44/ 50] Validation Loss: 0.209488\nl0: 0.004600, l1: 0.004633, l2: 0.005247, l3: 0.006174, l4: 0.008267, l5: 0.013641, l6: 0.022671\n\n[epoch:  44/ 50, batch:    16/    0, ite: 176] train loss: 0.065244, tar: 0.004346 \nl0: 0.066566, l1: 0.066144, l2: 0.065587, l3: 0.065435, l4: 0.063239, l5: 0.070083, l6: 0.087292\n\nl0: 0.005001, l1: 0.005087, l2: 0.005570, l3: 0.006144, l4: 0.007613, l5: 0.012047, l6: 0.016551\n\nl0: 0.027277, l1: 0.026784, l2: 0.029708, l3: 0.033745, l4: 0.036842, l5: 0.040544, l6: 0.048880\n\nl0: 0.007610, l1: 0.007473, l2: 0.009000, l3: 0.009870, l4: 0.012181, l5: 0.014545, l6: 0.025330\n\nl0: 0.015398, l1: 0.015324, l2: 0.015779, l3: 0.018129, l4: 0.023643, l5: 0.033466, l6: 0.054621\n\n[epoch:  44/ 50] Validation Loss: 0.209701\nl0: 0.004850, l1: 0.004916, l2: 0.005671, l3: 0.006441, l4: 0.008759, l5: 0.013075, l6: 0.023616\n\n[epoch:  45/ 50, batch:     4/    0, ite: 177] train loss: 0.065542, tar: 0.004418 \nl0: 0.035313, l1: 0.035394, l2: 0.037836, l3: 0.040029, l4: 0.050930, l5: 0.058806, l6: 0.088427\n\nl0: 0.004759, l1: 0.004794, l2: 0.005281, l3: 0.006161, l4: 0.008260, l5: 0.012473, l6: 0.016913\n\nl0: 0.041254, l1: 0.041350, l2: 0.040932, l3: 0.041689, l4: 0.038584, l5: 0.042143, l6: 0.045085\n\nl0: 0.007628, l1: 0.007442, l2: 0.009181, l3: 0.010168, l4: 0.012524, l5: 0.015265, l6: 0.020576\n\nl0: 0.016404, l1: 0.016299, l2: 0.016665, l3: 0.018825, l4: 0.028712, l5: 0.038326, l6: 0.062670\n\n[epoch:  45/ 50] Validation Loss: 0.195420\nl0: 0.004776, l1: 0.004811, l2: 0.005313, l3: 0.006210, l4: 0.008633, l5: 0.014367, l6: 0.028355\n\n[epoch:  45/ 50, batch:     8/    0, ite: 178] train loss: 0.066407, tar: 0.004462 \nl0: 0.041154, l1: 0.040358, l2: 0.039860, l3: 0.040681, l4: 0.045290, l5: 0.048872, l6: 0.071833\n\nl0: 0.004986, l1: 0.005041, l2: 0.005507, l3: 0.006228, l4: 0.008000, l5: 0.011323, l6: 0.016794\n\nl0: 0.026904, l1: 0.026526, l2: 0.029338, l3: 0.034766, l4: 0.037611, l5: 0.041262, l6: 0.042758\n\nl0: 0.007584, l1: 0.007487, l2: 0.009426, l3: 0.010317, l4: 0.012139, l5: 0.015572, l6: 0.027772\n\nl0: 0.010872, l1: 0.010765, l2: 0.011594, l3: 0.013233, l4: 0.020363, l5: 0.046997, l6: 0.065198\n\n[epoch:  45/ 50] Validation Loss: 0.178882\nl0: 0.004892, l1: 0.004921, l2: 0.005716, l3: 0.006785, l4: 0.009017, l5: 0.013146, l6: 0.028136\n\n[epoch:  45/ 50, batch:    12/    0, ite: 179] train loss: 0.067097, tar: 0.004510 \nl0: 0.009214, l1: 0.009240, l2: 0.009966, l3: 0.012201, l4: 0.019050, l5: 0.034171, l6: 0.067742\n\nl0: 0.006015, l1: 0.006622, l2: 0.006531, l3: 0.007398, l4: 0.009426, l5: 0.012612, l6: 0.023037\n\nl0: 0.034875, l1: 0.035018, l2: 0.034740, l3: 0.036608, l4: 0.037797, l5: 0.040496, l6: 0.046374\n\nl0: 0.006958, l1: 0.006794, l2: 0.008880, l3: 0.009892, l4: 0.012188, l5: 0.017076, l6: 0.025112\n\nl0: 0.014903, l1: 0.014861, l2: 0.014264, l3: 0.015494, l4: 0.021256, l5: 0.032122, l6: 0.075162\n\n[epoch:  45/ 50] Validation Loss: 0.154819\nl0: 0.004121, l1: 0.004168, l2: 0.004924, l3: 0.006258, l4: 0.008771, l5: 0.014300, l6: 0.030735\n\n[epoch:  45/ 50, batch:    16/    0, ite: 180] train loss: 0.067715, tar: 0.004471 \nl0: 0.021116, l1: 0.022057, l2: 0.024285, l3: 0.028758, l4: 0.043823, l5: 0.051304, l6: 0.079651\n\nl0: 0.006230, l1: 0.006261, l2: 0.006491, l3: 0.007696, l4: 0.010246, l5: 0.012232, l6: 0.020335\n\nl0: 0.036209, l1: 0.036204, l2: 0.036261, l3: 0.038423, l4: 0.037224, l5: 0.041733, l6: 0.052087\n\nl0: 0.008449, l1: 0.008403, l2: 0.009805, l3: 0.010363, l4: 0.012159, l5: 0.014312, l6: 0.027017\n\nl0: 0.016094, l1: 0.016005, l2: 0.015987, l3: 0.017823, l4: 0.022931, l5: 0.045483, l6: 0.092513\n\n[epoch:  45/ 50] Validation Loss: 0.187193\nl0: 0.003679, l1: 0.003761, l2: 0.004389, l3: 0.005473, l4: 0.007358, l5: 0.011461, l6: 0.022093\n\n[epoch:  46/ 50, batch:     4/    0, ite: 181] train loss: 0.058215, tar: 0.003679 \nl0: 0.067300, l1: 0.067955, l2: 0.068384, l3: 0.071197, l4: 0.066927, l5: 0.064892, l6: 0.077624\n\nl0: 0.004873, l1: 0.004906, l2: 0.005355, l3: 0.006168, l4: 0.008396, l5: 0.011894, l6: 0.016460\n\nl0: 0.039993, l1: 0.040038, l2: 0.040200, l3: 0.040266, l4: 0.039021, l5: 0.041134, l6: 0.045328\n\nl0: 0.007494, l1: 0.007363, l2: 0.009008, l3: 0.010064, l4: 0.011782, l5: 0.014755, l6: 0.024108\n\nl0: 0.018380, l1: 0.018184, l2: 0.019651, l3: 0.022589, l4: 0.044817, l5: 0.062436, l6: 0.099983\n\n[epoch:  46/ 50] Validation Loss: 0.239785\nl0: 0.004911, l1: 0.005036, l2: 0.005373, l3: 0.005990, l4: 0.008385, l5: 0.012439, l6: 0.022160\n\n[epoch:  46/ 50, batch:     8/    0, ite: 182] train loss: 0.061254, tar: 0.004295 \nl0: 0.052805, l1: 0.052582, l2: 0.052717, l3: 0.054401, l4: 0.057890, l5: 0.058611, l6: 0.072358\n\nl0: 0.004801, l1: 0.004843, l2: 0.005389, l3: 0.006072, l4: 0.008051, l5: 0.010460, l6: 0.015385\n\nl0: 0.023617, l1: 0.023057, l2: 0.025933, l3: 0.029074, l4: 0.035408, l5: 0.036940, l6: 0.048797\n\nl0: 0.007261, l1: 0.007043, l2: 0.009009, l3: 0.010039, l4: 0.012401, l5: 0.014689, l6: 0.021880\n\nl0: 0.014588, l1: 0.014526, l2: 0.014479, l3: 0.015853, l4: 0.021635, l5: 0.030070, l6: 0.052867\n\n[epoch:  46/ 50] Validation Loss: 0.185106\nl0: 0.005397, l1: 0.005487, l2: 0.006147, l3: 0.007214, l4: 0.010003, l5: 0.014714, l6: 0.030077\n\n[epoch:  46/ 50, batch:    12/    0, ite: 183] train loss: 0.067182, tar: 0.004662 \nl0: 0.033923, l1: 0.034153, l2: 0.036717, l3: 0.038938, l4: 0.050444, l5: 0.052812, l6: 0.100892\n\nl0: 0.005207, l1: 0.005232, l2: 0.005653, l3: 0.006463, l4: 0.008924, l5: 0.011905, l6: 0.017382\n\nl0: 0.035688, l1: 0.035651, l2: 0.036744, l3: 0.038569, l4: 0.038796, l5: 0.040436, l6: 0.045384\n\nl0: 0.007901, l1: 0.007737, l2: 0.009529, l3: 0.010611, l4: 0.012496, l5: 0.016642, l6: 0.026893\n\nl0: 0.015358, l1: 0.015285, l2: 0.015744, l3: 0.017086, l4: 0.029269, l5: 0.053190, l6: 0.107375\n\n[epoch:  46/ 50] Validation Loss: 0.205006\nl0: 0.004133, l1: 0.004171, l2: 0.004808, l3: 0.005700, l4: 0.008608, l5: 0.013430, l6: 0.028242\n\n[epoch:  46/ 50, batch:    16/    0, ite: 184] train loss: 0.067660, tar: 0.004530 \nl0: 0.045604, l1: 0.045643, l2: 0.046252, l3: 0.047902, l4: 0.048302, l5: 0.049989, l6: 0.066141\n\nl0: 0.004912, l1: 0.004995, l2: 0.005240, l3: 0.005898, l4: 0.007484, l5: 0.010932, l6: 0.015039\n\nl0: 0.040780, l1: 0.040341, l2: 0.040900, l3: 0.041643, l4: 0.040271, l5: 0.040702, l6: 0.050674\n\nl0: 0.008110, l1: 0.007971, l2: 0.009453, l3: 0.010211, l4: 0.012682, l5: 0.014996, l6: 0.020346\n\nl0: 0.080378, l1: 0.079722, l2: 0.079970, l3: 0.082834, l4: 0.080657, l5: 0.086563, l6: 0.091982\n\n[epoch:  46/ 50] Validation Loss: 0.273104\nl0: 0.003986, l1: 0.004062, l2: 0.004701, l3: 0.005736, l4: 0.008546, l5: 0.013247, l6: 0.027371\n\n[epoch:  47/ 50, batch:     4/    0, ite: 185] train loss: 0.067658, tar: 0.004421 \nl0: 0.011634, l1: 0.011660, l2: 0.012507, l3: 0.014780, l4: 0.018265, l5: 0.029505, l6: 0.072802\n\nl0: 0.004644, l1: 0.004708, l2: 0.004913, l3: 0.005712, l4: 0.008133, l5: 0.011631, l6: 0.019476\n\nl0: 0.035354, l1: 0.035424, l2: 0.035779, l3: 0.037906, l4: 0.037472, l5: 0.039809, l6: 0.046033\n\nl0: 0.007382, l1: 0.007395, l2: 0.008568, l3: 0.009546, l4: 0.011307, l5: 0.012743, l6: 0.023133\n\nl0: 0.018204, l1: 0.018043, l2: 0.018164, l3: 0.019527, l4: 0.022516, l5: 0.034832, l6: 0.103502\n\n[epoch:  47/ 50] Validation Loss: 0.162602\nl0: 0.004351, l1: 0.004403, l2: 0.005180, l3: 0.006396, l4: 0.009229, l5: 0.013387, l6: 0.026870\n\n[epoch:  47/ 50, batch:     8/    0, ite: 186] train loss: 0.068017, tar: 0.004409 \nl0: 0.058163, l1: 0.058686, l2: 0.060256, l3: 0.062478, l4: 0.062416, l5: 0.058572, l6: 0.078194\n\nl0: 0.005081, l1: 0.005091, l2: 0.005656, l3: 0.006928, l4: 0.008727, l5: 0.012439, l6: 0.018773\n\nl0: 0.035109, l1: 0.035076, l2: 0.036035, l3: 0.038614, l4: 0.036055, l5: 0.040090, l6: 0.043405\n\nl0: 0.007190, l1: 0.007298, l2: 0.008277, l3: 0.009104, l4: 0.010569, l5: 0.012445, l6: 0.023111\n\nl0: 0.014312, l1: 0.014272, l2: 0.015002, l3: 0.016513, l4: 0.021762, l5: 0.033626, l6: 0.070934\n\n[epoch:  47/ 50] Validation Loss: 0.206052\nl0: 0.004793, l1: 0.004826, l2: 0.005602, l3: 0.006702, l4: 0.009002, l5: 0.014228, l6: 0.026427\n\n[epoch:  47/ 50, batch:    12/    0, ite: 187] train loss: 0.068526, tar: 0.004464 \nl0: 0.023744, l1: 0.023507, l2: 0.024953, l3: 0.027229, l4: 0.035209, l5: 0.047477, l6: 0.084576\n\nl0: 0.005242, l1: 0.005290, l2: 0.005595, l3: 0.006112, l4: 0.008702, l5: 0.011044, l6: 0.017790\n\nl0: 0.026726, l1: 0.026367, l2: 0.029317, l3: 0.034666, l4: 0.037113, l5: 0.040981, l6: 0.050432\n\nl0: 0.008001, l1: 0.007838, l2: 0.009778, l3: 0.011659, l4: 0.014343, l5: 0.017282, l6: 0.026140\n\nl0: 0.019669, l1: 0.019310, l2: 0.021176, l3: 0.023568, l4: 0.032603, l5: 0.055158, l6: 0.087792\n\n[epoch:  47/ 50] Validation Loss: 0.185278\nl0: 0.004482, l1: 0.004545, l2: 0.005071, l3: 0.005879, l4: 0.008053, l5: 0.013059, l6: 0.022994\n\n[epoch:  47/ 50, batch:    16/    0, ite: 188] train loss: 0.067971, tar: 0.004466 \nl0: 0.029102, l1: 0.029182, l2: 0.032334, l3: 0.035335, l4: 0.043796, l5: 0.056745, l6: 0.086764\n\nl0: 0.005037, l1: 0.005096, l2: 0.005451, l3: 0.006036, l4: 0.007807, l5: 0.009995, l6: 0.015282\n\nl0: 0.040261, l1: 0.040403, l2: 0.039538, l3: 0.039586, l4: 0.037839, l5: 0.038843, l6: 0.040995\n\nl0: 0.008192, l1: 0.008066, l2: 0.009702, l3: 0.010771, l4: 0.013174, l5: 0.015752, l6: 0.026087\n\nl0: 0.020159, l1: 0.019794, l2: 0.020291, l3: 0.025687, l4: 0.035317, l5: 0.043880, l6: 0.092483\n\n[epoch:  47/ 50] Validation Loss: 0.198957\nl0: 0.004223, l1: 0.004290, l2: 0.005087, l3: 0.006410, l4: 0.008866, l5: 0.012984, l6: 0.023419\n\n[epoch:  48/ 50, batch:     4/    0, ite: 189] train loss: 0.067672, tar: 0.004439 \nl0: 0.062403, l1: 0.062302, l2: 0.062724, l3: 0.062059, l4: 0.060314, l5: 0.058924, l6: 0.079494\n\nl0: 0.005318, l1: 0.005401, l2: 0.005850, l3: 0.006616, l4: 0.009261, l5: 0.012947, l6: 0.018861\n\nl0: 0.036163, l1: 0.036023, l2: 0.036339, l3: 0.038717, l4: 0.037818, l5: 0.042376, l6: 0.053642\n\nl0: 0.008002, l1: 0.007898, l2: 0.009387, l3: 0.010359, l4: 0.011916, l5: 0.013470, l6: 0.022730\n\nl0: 0.019422, l1: 0.019178, l2: 0.020817, l3: 0.026012, l4: 0.050362, l5: 0.067791, l6: 0.102289\n\n[epoch:  48/ 50] Validation Loss: 0.236637\nl0: 0.005118, l1: 0.005175, l2: 0.005589, l3: 0.006294, l4: 0.008030, l5: 0.012408, l6: 0.026003\n\n[epoch:  48/ 50, batch:     8/    0, ite: 190] train loss: 0.067766, tar: 0.004507 \nl0: 0.053430, l1: 0.053121, l2: 0.053798, l3: 0.055297, l4: 0.059218, l5: 0.060996, l6: 0.069780\n\nl0: 0.005888, l1: 0.005856, l2: 0.006444, l3: 0.007212, l4: 0.008741, l5: 0.011234, l6: 0.019730\n\nl0: 0.025706, l1: 0.025088, l2: 0.027847, l3: 0.031549, l4: 0.034403, l5: 0.037304, l6: 0.042435\n\nl0: 0.009125, l1: 0.008995, l2: 0.010401, l3: 0.011261, l4: 0.013249, l5: 0.019089, l6: 0.028862\n\nl0: 0.018523, l1: 0.018334, l2: 0.018252, l3: 0.020159, l4: 0.032399, l5: 0.044068, l6: 0.098440\n\n[epoch:  48/ 50] Validation Loss: 0.209247\nl0: 0.003678, l1: 0.003702, l2: 0.004568, l3: 0.005632, l4: 0.007915, l5: 0.011743, l6: 0.026011\n\n[epoch:  48/ 50, batch:    12/    0, ite: 191] train loss: 0.063248, tar: 0.003678 \nl0: 0.046316, l1: 0.046214, l2: 0.046688, l3: 0.048873, l4: 0.049977, l5: 0.054361, l6: 0.072457\n\nl0: 0.005537, l1: 0.005566, l2: 0.006159, l3: 0.006822, l4: 0.009190, l5: 0.010768, l6: 0.017162\n\nl0: 0.040947, l1: 0.041011, l2: 0.040930, l3: 0.041189, l4: 0.039791, l5: 0.044184, l6: 0.047184\n\nl0: 0.007687, l1: 0.007484, l2: 0.009439, l3: 0.010446, l4: 0.012825, l5: 0.015704, l6: 0.021818\n\nl0: 0.014414, l1: 0.014402, l2: 0.013964, l3: 0.015045, l4: 0.020085, l5: 0.025317, l6: 0.072183\n\n[epoch:  48/ 50] Validation Loss: 0.196428\nl0: 0.003887, l1: 0.003948, l2: 0.004611, l3: 0.005920, l4: 0.008194, l5: 0.013124, l6: 0.021439\n\n[epoch:  48/ 50, batch:    16/    0, ite: 192] train loss: 0.062185, tar: 0.003783 \nl0: 0.052332, l1: 0.052077, l2: 0.052226, l3: 0.053726, l4: 0.056471, l5: 0.057423, l6: 0.070347\n\nl0: 0.004590, l1: 0.004639, l2: 0.004960, l3: 0.005803, l4: 0.007847, l5: 0.011776, l6: 0.016544\n\nl0: 0.040545, l1: 0.040551, l2: 0.040025, l3: 0.040170, l4: 0.038840, l5: 0.040412, l6: 0.044359\n\nl0: 0.007780, l1: 0.007641, l2: 0.009468, l3: 0.010529, l4: 0.012419, l5: 0.015699, l6: 0.027259\n\nl0: 0.035371, l1: 0.034362, l2: 0.038021, l3: 0.037958, l4: 0.065426, l5: 0.076294, l6: 0.091707\n\n[epoch:  48/ 50] Validation Loss: 0.241119\nl0: 0.004970, l1: 0.004964, l2: 0.005785, l3: 0.006715, l4: 0.009706, l5: 0.015170, l6: 0.029979\n\n[epoch:  49/ 50, batch:     4/    0, ite: 193] train loss: 0.067220, tar: 0.004178 \nl0: 0.028933, l1: 0.028819, l2: 0.029305, l3: 0.030996, l4: 0.033900, l5: 0.045130, l6: 0.078315\n\nl0: 0.006038, l1: 0.006063, l2: 0.006533, l3: 0.007186, l4: 0.008586, l5: 0.011871, l6: 0.018429\n\nl0: 0.034408, l1: 0.034329, l2: 0.035029, l3: 0.037373, l4: 0.036783, l5: 0.039619, l6: 0.046479\n\nl0: 0.007327, l1: 0.007255, l2: 0.008898, l3: 0.009878, l4: 0.010834, l5: 0.012200, l6: 0.023295\n\nl0: 0.020071, l1: 0.019850, l2: 0.019874, l3: 0.021337, l4: 0.026728, l5: 0.048609, l6: 0.094239\n\n[epoch:  49/ 50] Validation Loss: 0.186904\nl0: 0.004258, l1: 0.004284, l2: 0.005090, l3: 0.006396, l4: 0.008888, l5: 0.012989, l6: 0.023234\n\n[epoch:  49/ 50, batch:     8/    0, ite: 194] train loss: 0.066700, tar: 0.004198 \nl0: 0.061658, l1: 0.061602, l2: 0.062182, l3: 0.063607, l4: 0.064808, l5: 0.069266, l6: 0.085765\n\nl0: 0.004470, l1: 0.004546, l2: 0.004941, l3: 0.005972, l4: 0.008228, l5: 0.012253, l6: 0.020045\n\nl0: 0.040809, l1: 0.040416, l2: 0.040612, l3: 0.041439, l4: 0.040240, l5: 0.043586, l6: 0.049988\n\nl0: 0.007662, l1: 0.007636, l2: 0.009112, l3: 0.009928, l4: 0.011165, l5: 0.013785, l6: 0.024024\n\nl0: 0.019806, l1: 0.019795, l2: 0.019428, l3: 0.021115, l4: 0.024158, l5: 0.047118, l6: 0.061297\n\n[epoch:  49/ 50] Validation Loss: 0.224492\nl0: 0.004712, l1: 0.004806, l2: 0.005500, l3: 0.006859, l4: 0.009201, l5: 0.013447, l6: 0.027977\n\n[epoch:  49/ 50, batch:    12/    0, ite: 195] train loss: 0.067860, tar: 0.004301 \nl0: 0.057147, l1: 0.057209, l2: 0.057667, l3: 0.059093, l4: 0.064766, l5: 0.062918, l6: 0.086566\n\nl0: 0.005345, l1: 0.005564, l2: 0.005859, l3: 0.006853, l4: 0.009311, l5: 0.012155, l6: 0.019424\n\nl0: 0.033454, l1: 0.033266, l2: 0.033463, l3: 0.035721, l4: 0.036082, l5: 0.039572, l6: 0.046029\n\nl0: 0.006874, l1: 0.006814, l2: 0.008448, l3: 0.009425, l4: 0.010557, l5: 0.012659, l6: 0.023232\n\nl0: 0.015801, l1: 0.015676, l2: 0.015836, l3: 0.016770, l4: 0.021787, l5: 0.036843, l6: 0.100342\n\n[epoch:  49/ 50] Validation Loss: 0.213705\nl0: 0.003979, l1: 0.004052, l2: 0.004763, l3: 0.005597, l4: 0.007678, l5: 0.011841, l6: 0.024908\n\n[epoch:  49/ 50, batch:    16/    0, ite: 196] train loss: 0.067020, tar: 0.004247 \nl0: 0.056637, l1: 0.055815, l2: 0.055135, l3: 0.055725, l4: 0.059336, l5: 0.062691, l6: 0.090966\n\nl0: 0.006601, l1: 0.006682, l2: 0.007187, l3: 0.007608, l4: 0.008674, l5: 0.011689, l6: 0.023225\n\nl0: 0.039714, l1: 0.039570, l2: 0.038758, l3: 0.039653, l4: 0.038171, l5: 0.040368, l6: 0.041689\n\nl0: 0.007402, l1: 0.007416, l2: 0.008616, l3: 0.009562, l4: 0.011186, l5: 0.013246, l6: 0.024591\n\nl0: 0.018834, l1: 0.018869, l2: 0.018335, l3: 0.019266, l4: 0.022322, l5: 0.028782, l6: 0.070570\n\n[epoch:  49/ 50] Validation Loss: 0.212978\nl0: 0.004178, l1: 0.004243, l2: 0.005042, l3: 0.006078, l4: 0.008565, l5: 0.013834, l6: 0.023012\n\n[epoch:  50/ 50, batch:     4/    0, ite: 197] train loss: 0.066725, tar: 0.004237 \nl0: 0.054022, l1: 0.053619, l2: 0.055148, l3: 0.057772, l4: 0.058792, l5: 0.067149, l6: 0.082410\n\nl0: 0.005400, l1: 0.005477, l2: 0.005713, l3: 0.006244, l4: 0.008601, l5: 0.010823, l6: 0.017581\n\nl0: 0.033775, l1: 0.033648, l2: 0.034793, l3: 0.035873, l4: 0.036418, l5: 0.041381, l6: 0.054142\n\nl0: 0.008572, l1: 0.008443, l2: 0.010014, l3: 0.010969, l4: 0.012819, l5: 0.019097, l6: 0.022892\n\nl0: 0.015103, l1: 0.015068, l2: 0.014849, l3: 0.015896, l4: 0.022276, l5: 0.039289, l6: 0.068106\n\n[epoch:  50/ 50] Validation Loss: 0.208435\nl0: 0.004442, l1: 0.004467, l2: 0.005508, l3: 0.006718, l4: 0.009388, l5: 0.013962, l6: 0.024028\n\n[epoch:  50/ 50, batch:     8/    0, ite: 198] train loss: 0.066948, tar: 0.004263 \nl0: 0.043768, l1: 0.043105, l2: 0.042281, l3: 0.040703, l4: 0.052156, l5: 0.055715, l6: 0.074948\n\nl0: 0.004998, l1: 0.004998, l2: 0.005478, l3: 0.006334, l4: 0.008368, l5: 0.012912, l6: 0.017675\n\nl0: 0.032343, l1: 0.032057, l2: 0.032854, l3: 0.034306, l4: 0.035127, l5: 0.040829, l6: 0.051956\n\nl0: 0.008928, l1: 0.008664, l2: 0.010593, l3: 0.011997, l4: 0.014318, l5: 0.017846, l6: 0.026650\n\nl0: 0.021113, l1: 0.021120, l2: 0.020641, l3: 0.021880, l4: 0.023710, l5: 0.032936, l6: 0.072487\n\n[epoch:  50/ 50] Validation Loss: 0.197159\nl0: 0.003873, l1: 0.003937, l2: 0.004448, l3: 0.005110, l4: 0.007299, l5: 0.011547, l6: 0.021759\n\n[epoch:  50/ 50, batch:    12/    0, ite: 199] train loss: 0.065951, tar: 0.004220 \nl0: 0.051612, l1: 0.051202, l2: 0.050719, l3: 0.052290, l4: 0.058663, l5: 0.061683, l6: 0.082688\n\nl0: 0.004617, l1: 0.004657, l2: 0.005094, l3: 0.006072, l4: 0.008608, l5: 0.011817, l6: 0.018408\n\nl0: 0.024001, l1: 0.023214, l2: 0.026508, l3: 0.030611, l4: 0.035509, l5: 0.039848, l6: 0.050598\n\nl0: 0.008888, l1: 0.008850, l2: 0.009914, l3: 0.011098, l4: 0.014193, l5: 0.019306, l6: 0.027913\n\nl0: 0.014478, l1: 0.014390, l2: 0.014105, l3: 0.015394, l4: 0.020832, l5: 0.025948, l6: 0.064235\n\n[epoch:  50/ 50] Validation Loss: 0.193592\nl0: 0.004875, l1: 0.004968, l2: 0.005574, l3: 0.006511, l4: 0.008667, l5: 0.014349, l6: 0.027184\n\n[epoch:  50/ 50, batch:    16/    0, ite: 200] train loss: 0.066569, tar: 0.004285 \nl0: 0.066919, l1: 0.066668, l2: 0.066741, l3: 0.068450, l4: 0.063295, l5: 0.069923, l6: 0.093723\n\nl0: 0.004919, l1: 0.004968, l2: 0.005375, l3: 0.006151, l4: 0.008969, l5: 0.011494, l6: 0.018654\n\nl0: 0.026711, l1: 0.026203, l2: 0.029547, l3: 0.033453, l4: 0.035029, l5: 0.039807, l6: 0.048904\n\nl0: 0.007355, l1: 0.007335, l2: 0.008621, l3: 0.009534, l4: 0.010927, l5: 0.013019, l6: 0.024448\n\nl0: 0.071212, l1: 0.070241, l2: 0.067679, l3: 0.068822, l4: 0.066079, l5: 0.071344, l6: 0.101927\n\n[epoch:  50/ 50] Validation Loss: 0.278889\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save Best model to the drive :\n# import shutil\n# shutil.copyfile('/content/U-2-Netu2netRetrain_best.pth', '/content/drive/MyDrive/BestModel/U-2-Netu2net_best_Retrain.pth')","metadata":{"id":"CDufGol7RZsc","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1694086612657,"user_tz":-330,"elapsed":696,"user":{"displayName":"ishant kukreti","userId":"05488449256694367743"}},"outputId":"56833f36-5115-4eb8-c65a-7a33da74f7c3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"yCkEkUL6YT9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model working Output Examin :","metadata":{}},{"cell_type":"markdown","source":"| Image | Output |\n|-------|--------|\n| ![Image](https://github.com/3112ik09/Images/blob/main/17.jpg?raw=true) | ![Output](https://github.com/3112ik09/Images/blob/main/retrainTest.png?raw=true) |","metadata":{}},{"cell_type":"markdown","source":"## Check in Unseen Image : \n| Input Image | Output |\n|-------------|--------|\n| ![Image](https://github.com/3112ik09/Images/blob/main/car1.jpg?raw=true) | ![Output](https://github.com/3112ik09/Images/blob/main/retrainUnseen.png?raw=true) |\n","metadata":{}},{"cell_type":"markdown","source":"### ADVT: \n##### This Model works well to dedect the vehicle on unseen data .\n\n### Dis. ADVT:\n##### can dedect other oubject tooo,..\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}